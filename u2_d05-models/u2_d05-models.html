<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Network Models</title>
    <meta charset="utf-8" />
    <meta name="author" content="Giora Simchoni" />
    <meta name="date" content="2020-01-30" />
    <head>
      <link rel="icon" href="../DSApps_logo.jpg" type="image/jpg"> 
      <link rel="shortcut icon" href="../DSApps_logo.jpg" type="image/jpg">
    </head>
    <link rel="stylesheet" href="..\slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">


class: logo-slide

---

class: title-slide

## Network Models

### Applications of Data Science - Class 10

### Giora Simchoni

#### `gsimchoni@gmail.com and add #dsapps in subject`

### Stat. and OR Department, TAU
### 2020-01-30

---



layout: true

&lt;div class="my-footer"&gt;
  &lt;span&gt;
    &lt;a href="https://dsapps-2020.github.io/Class_Slides/" target="_blank"&gt;Applications of Data Science
    &lt;/a&gt;
  &lt;/span&gt;
&lt;/div&gt;

---



class: section-slide

# Motivation

---

### *Degree histograms* of 3 real networks:

- The cast of RuPaul's Drag Race Twitterverse
- The Israeli artists musical cooperations in the 21st century
- The Sci-Fi themed correlation network

&lt;img src="images/Degree-Hists-1.png" width="80%" /&gt;

.insight[
ðŸ’¡ Which is which?
]

---

### Why model networks?

- Know your network better:
  - how it was formed
  - what class it belongs to (clustering)
  - how and why it deviates from model
- Predict behavior in network:
  - epidemic spread/resistance
  - search
  - link prediction
  - node disambiguation
- Generalization
- Simulations and the ability to estimate metrics on huge networks
  
---

class: section-slide

# The ErdÅ‘sâ€“RÃ©nyi model .font80percent[(Random Graphs)]

---

## You know the ER model

In a `\(n\)` nodes undirected network, suppose edges form identically and independently from each other.

Each possible edge is a Bernoulli trial with probability `\(p\)`.

Then the no. of edges `\(M\)` is a Binomial RV `\(\sim Binom({n\choose 2},p)\)`

.insight[
ðŸ’¡ So what is `\(P(M=m)\)`? What is `\(E(M)\)`? What is `\(Var(M)\)`?
]

The `\(G(n, p)\)` collection of all simple networks formed this way is the ErdÅ‘sâ€“RÃ©nyi model.

---

## Mean Degree

The mean number of edges is: `\(E(M)={n\choose 2}p\)`

We have defined the mean degree to be: `\(c = \frac{2m}{n}\)`, but now we treat `\(m\)` as a RV, so the mean degree would be:

`\(c=E(D)=E(\frac{2M}{n})=\frac{2E(M)}{n}=\frac{2}{n}{n\choose 2}p=(n-1)p\)`

.insight[
ðŸ’¡ Say it in words, it makes sense.
]

But this implies we could have formulated the ER model slightly different:

Let `\(D\)` be the degree of a particular node in an undirected network with `\(n\)` nodes. Suppose each node "chooses" its neighbors with probability `\(p\)`, then:  `\(D\sim Binom(n-1,p)\)`

---

Estimating `\(n\)`, `\(p\)` and `\(c\)` from our networks:


```python
def estimate_p(m, n): return m / (n * (n - 1) / 2)
def estimate_c(G): return np.mean(list(dict(G.degree()).values()))

n_isr, n_sci, n_ru = I.number_of_nodes(), Sc.number_of_nodes(), Ru.number_of_nodes()
m_isr, m_sci, m_ru = I.number_of_edges(), Sc.number_of_edges(), Ru.number_of_edges()
p_isr, p_sci, p_ru = estimate_p(m_isr, n_isr), estimate_p(m_sci, n_sci), estimate_p(m_ru, n_ru)
c_isr, c_sci, c_ru = estimate_c(I), estimate_c(Sc), estimate_c(Ru)

pd.DataFrame({
  'network': ['Israeli Artists', 'Sci-Fi Books', 'RuPaul Verse'],
  'n': [n_isr, n_sci, n_ru],
  'm': [m_isr, m_sci, m_ru],
  'p': [p_isr, p_sci, p_ru],
  'c': [c_isr, c_sci, c_ru]
})
```

```
##            network    n     m         p          c
## 0  Israeli Artists  321   381  0.007418   2.373832
## 1     Sci-Fi Books   23    51  0.201581   4.434783
## 2     RuPaul Verse  112  5119  0.823520  91.410714
```


---

## Degree Distribution

And since we're dealing with large *sparse* networks, we might mention that as `\(n\to\infty\)` and `\(p\)` is small, we expect the Binomial distribution to be approximated by the Poisson distribution, such that:

`\(D \sim Pois((n-1)p)\)` or `\(D \sim Pois(c)\)`

Which is why the ER model is sometimes referred to the Poisson random graph.

---

## ER model In NetworkX


```python
G = nx.erdos_renyi_graph(n = 100, p = 0.3)

plt.figure()
nx.draw_networkx(G)
plt.show()
```

&lt;img src="images/ER-1.png" width="50%" /&gt;

---


```python
deg = [deg for node, deg in G.degree()]
h = plt.hist(deg)
plt.show()
```

&lt;img src="images/ER-Deg_Dist-1.png" width="50%" /&gt;

.insight[
ðŸ’¡ Does this degree distribution resemble any of the distributions we've seen?
]

---

## Transitivity

We have defined the Clustering Coefficient or Transitivity to be:

`\(Transitivity(G)=\frac{\text{#closed triads}}{\text{#triads}}\)`

In other words it is the probability of two neighbors of the same node to also be connected.

.insight[
ðŸ’¡ What is that probability in the ER model?

What do you think of it after what you have seen?
]

---

## Diameter

We have defined the diameter as the maximal shortest distance between any pair of nodes.

It turns out this has a nice expression in the ER model:

- Take a random node in a ER network, how many neighbors do you expect it to have?
- Take each one of these neighbors, and take each and one's neighbors - what do you get?
- Do this `\(l\)` times, what do you get? What would you get "in the end"?

The average path length from a node to all other nodes is `\(\sim \frac{\ln(n)}{\ln(c)}\)`.

Similarly one can show the diameter is `\(\sim 2\frac{\ln(n)}{\ln(c)}\)`

---


```python
d_isr = nx.diameter(I)
d_sci = nx.diameter(Sc)
d_ru = nx.diameter(Ru)

pd.DataFrame({
  'network': ['Israeli Artists', 'Sci-Fi Books', 'RuPaul Verse'],
  'n': [n_isr, n_sci, n_ru],
  'c': [c_isr, c_sci, c_ru],
  '2ln(n)/ln(c)': [2 * np.log(n_isr)/np.log(c_isr), 2 * np.log(n_sci)/np.log(c_sci), 2 * np.log(n_ru)/np.log(c_ru)],
  'd': [d_isr, d_sci, d_ru]
})
```

```
##            network    n          c  2ln(n)/ln(c)   d
## 0  Israeli Artists  321   2.373832     13.352007  17
## 1     Sci-Fi Books   23   4.434783      4.210190   5
## 2     RuPaul Verse  112  91.410714      2.089976   3
```

.insight[
ðŸ’¡ What famous phenomenon this could help explain?
]

---



## The Giant Component

.insight[
ðŸ’¡ What is the size of the largest component with `\(p=0\)`?

What is the size of the largest component with `\(p=1\)`?
]


```python
n = 100
s = []
for p in np.linspace(0, 1, 101):
  s_p = []
  for i in range(10):
    G = nx.erdos_renyi_graph(n = n, p = p)
    Gcc = sorted(nx.connected_components(G), key=len, reverse=True)
    Gcc_n = G.subgraph(Gcc[0]).number_of_nodes()
    s_p.append(Gcc_n / n)
  s.append(np.mean(s_p))

plt.plot(s, marker='*')
locs, labels = plt.xticks(np.linspace(0, 100, 11), np.round(np.linspace(0, 1, 11), 1))
plt.xlabel('p (where n = 100)')
plt.ylabel('GCC size S')
plt.show()
```

---

&lt;img src="images/GCC-Size-1.png" width="50%" /&gt;

Surprisingly, for a given `\(n\)`, when `\(p\)` is varied from 0 to 1, the giant component size `\(S\)` suddenly "jumps"!

This is also true for a given `\(p\)` as you increase `\(n\)`: imagine sitting at the comfort of your home watching your ER network form, when suddenly...

---

&lt;img src="images/GCC-Size2-1.png" width="50%" /&gt;

Don't blink! Because there seems to be a *critical point*, a *transition point*, a *percolation point* where `\(S\)` increases from 0 and finally reaches to 1 making your network fully connected.

---

### A slight wave of hands

`\(S\)` is the probability of a node being connected to the GCC.

`\(S = P(\text{node i is connected to GCC})=\)`
`\(= 1-P(\text{node i is disconnected from GCC})=\)`
`\(= 1-P(\text{node i is disconnected from GCC via node j})^{n-1}\)`

For a given `\(i,j\)`:

`\(P(\text{node i is disconnected via node j})=\)`
`\(= P(\text{node i is disconnected with j}) +\)`
`\(P(\text{node i is connected with j but j is disconnected from GCC})=\)`
`\(= (1-p) + p(1-S) = 1-pS\)`

---

So:

`\(S = 1-(1-pS)^{n-1}\)`

And you can immediately tell finding a closed solution for `\(S\)` would be hard.

Substituting `\(p=\frac{c}{n-1}\)` where `\(c\)` is the mean degree:

`\(S = 1-(1-\frac{c}{n-1}S)^{n-1}\)`

Recall that `\(e^x=\lim_{n\to \infty}(1+\frac {x}{n})^{n}\)` so in the limit of `\(n\)`:

`\(S = 1-e^{-cS}\)`

Now there is no closed solution for `\(S\)` but we can isolate `\(c=(n-1)p\)`:

`\(c=-\frac{\ln(1-S)}{S}\)`

---

&lt;img src="images/GCC-Size3-1.png" width="30%" /&gt;

We can see that around `\(c=1\)` (or `\(p=1/n\)`) the size of the GCC starts increasing .font80percent[(makes sense)].

It can be shown that around `\(c=\ln(n)\)` (or `\(p=\frac{\ln(n)}{n}\)`) we expect the network to be fully connected.

.insight[
ðŸ’¡ So according to the ER model, for 1K nodes, a probability of 0.7% of an edge is enough to make the network connected. What do you think of that?
]

---

All of the 3 networks we've talked about are fully connected .font80percent[(though notice how the Israeli Artists and the Sci-Fi Books networks were created)].

But the model gives us a way of describing at what *stage* they are and how *robust* they are to node failures:


```python
pd.DataFrame({
  'network': ['Israeli Artists', 'Sci-Fi Books', 'RuPaul Verse'],
  'n': [n_isr, n_sci, n_ru],
  'c': [c_isr, c_sci, c_ru],
  'ln(n)': [np.log(n_isr), np.log(n_sci), np.log(n_ru)],
  'stage': ['subcritcial', 'supercritical', 'connected']
})
```

```
##            network    n          c     ln(n)          stage
## 0  Israeli Artists  321   2.373832  5.771441    subcritcial
## 1     Sci-Fi Books   23   4.434783  3.135494  supercritical
## 2     RuPaul Verse  112  91.410714  4.718499      connected
```

---

## ER Model Summary

Models well:
- Diameter, average path length
- Giant Component, Percolation, Network Robustness

Does not model well:
- Degree distribution
- Transitivity (CC)
- Communities
- Homophily

---

class: section-slide

# The Power-Law Distribution

---

Let's see some larger real-life networks:

- The clients of a high-end Brazilian escort service, connected by whether they hired the same escort
- Amazon products, scraped online, connected by whether they are recommended with each other
- Astrophysics researchers co-authorship network




```
##            network       n       m         p           c     S
## 0           Escort   10106  668183  0.013086  132.234910  0.96
## 1  Amazon Products  334863  925872  0.000017    5.529855  1.00
## 2     CoAuthorship   18772  198110  0.001124   21.106968  0.95
```

---

### Degree histograms

&lt;img src="images/Degree-Hists2-1.png" width="80%" /&gt;

One of the main shortcomings of the ER model is that none of these look even remotely Poisson.

---

## Power Law

If `\(X \sim PL(\alpha, x_\text{min})\)`, then:

`\(f(x) = Cx^{-\alpha}\space\mbox{for}\ x\ge x_\text{min}&gt;0;\alpha&gt;1\)`

where `\(C=\frac{\alpha - 1}{x^{1-\alpha}_\text{min}}\)`

and we can write: `\(f(x)=\frac{\alpha-1}{x_\text{min}}(\frac{x}{x_\text{min}})^{-\alpha}\)`

.insight[
ðŸ’¡ What do you need to check in order to verify this is a proper density function?
]

---

The CDF of `\(X\)`:

`\(F_X(x)=\int_{-\infty}^x f(x)dx=C\int_{x_{\text{min}}}^x x^{-\alpha}dx=C \cdot [\frac{x^{1-\alpha}}{1-\alpha}]^x_{x_{\text{min}}}=\)`
`\(\frac{\alpha - 1}{x^{1-\alpha}_\text{min}}[\frac{x^{1-\alpha}}{1-\alpha}-\frac{x_{\text{min}}^{1-\alpha}}{1-\alpha}]=1-(\frac{x}{x_{\text{min}}})^{1-\alpha}\)`

The Expectation of `\(X\)`:

`\(E(X)=\int_{-\infty}^{\infty} xf(x)dx=C\int_{x_{\text{min}}}^{\infty} x^{1-\alpha}dx=\frac{\alpha - 1}{x^{1-\alpha}_\text{min}}[\frac{x^{2-\alpha}}{2-\alpha}]^{\infty}_{x_\text{min}}=x_\text{min}(\frac{\alpha-1}{\alpha-2})\)`

Where `\(E(X)\)` converges only if `\(\alpha&gt;2\)`.

The MLE for `\(\alpha\)` is: `\(\hat\alpha=1+\frac{n}{\sum_i\ln(\frac{x_i}{x_\text{min}})}\)`

.insight[
ðŸ’¡ The MLE for `\(x_\text{min}\)` is...
]

---

### Fitting the PL to our networks


```python
def fit_pl(G):
  degree_seq = np.array(list(dict(G.degree()).values()))
  d_min = np.min(degree_seq)
  n = G.number_of_nodes()
  if d_min == 0:
    d_min = 1
    degree_seq = degree_seq[degree_seq &gt;= 1]
    n = len(degree_seq)
  alpha = 1 + n/np.sum(np.log(degree_seq/d_min))
  davg = np.mean(degree_seq)
  Ed = dmin * (alpha - 1)/(alpha - 2) if alpha &gt; 2 else np.inf
  dvar = np.var(degree_seq)
  Ed2 = dmin**2 * (alpha - 1)/(alpha - 3) if alpha &gt; 3 else np.inf
  Vd = Ed2 - (Ed)**2 if alpha &gt; 3 else np.inf
  return d_min, alpha, davg, Ed, dvar, Vd
```

---


```python
dmin_e, alpha_e, davg_e, Ed_e, dvar_e, Vd_e = fit_pl(E)
dmin_amz, alpha_amz, davg_amz, Ed_amz, dvar_amz, Vd_amz = fit_pl(Amz)
dmin_ca, alpha_ca, davg_ca, Ed_ca, dvar_ca, Vd_ca = fit_pl(CA)

pd.DataFrame({
  'network': ['Escort', 'Amazon', 'Co-Authorship'],
  'dmin': [dmin_e, dmin_amz, dmin_ca],
  'alpha': [alpha_e, alpha_amz, alpha_ca],
  'Avg(d)': [davg_e, davg_amz, davg_ca],
  'E(d)': [Ed_e, Ed_amz, Ed_ca],
  'Var(d)': [dvar_e, dvar_amz, dvar_ca],
  'V(d)': [Vd_e, Vd_amz, Vd_ca],
})
```

```
##          network  dmin     alpha      Avg(d)  E(d)        Var(d)  V(d)
## 0         Escort     1  1.262319  137.457930   inf  49816.740516   inf
## 1         Amazon     1  1.691223    5.529855   inf     33.196380   inf
## 2  Co-Authorship     1  1.439148   21.106968   inf    934.606926   inf
```

---

&lt;img src="images/Degree-Hists3-1.png" width="100%" /&gt;

---

### Why do we like the Power Law distribution so much?

- scale invariance
- heavy tail
- the 80/20 (Pareto) rule

---

### Scale Invariance

If we multiply `\(X\)` by constant `\(m\)` the density "scales":

`\(f(mx)=C(mx)^{-\alpha}=Cm^{-\alpha}x^{-\alpha}=C'x^{-\alpha}\propto f(x)\)`

Thus, `\(mX\)` will distribute Power-Law with the same `\(\alpha\)` parameter and a different constant.

.insight[
ðŸ’¡ How would `\(mX\)` distribute if `\(X\)` distributed Normal? Exponential?
]

---

Meaning, the relative likelihood between small and large events is the same, no matter what choice of "small" we make:

&lt;img src="images/Scale-Invariance-1.png" width="80%" /&gt;

---

Another way of seeing this:

If we take the log-log transformation of the Power-Law distribution we get:

`\(\ln(f(x))=\ln(Cx^{-\alpha})=\ln(C)-\alpha\ln(x)\)`

Thus the log-log plot of Power-Law should show a straight line with slope `\(-\alpha\)`, and multiplying by `\(m\)` only changes the intercept.

&lt;img src="images/Scale-Invariance2-1.png" width="80%" /&gt;

---

Seeing the log-log transformation with our distributions should also result in a straight line:


```
## &lt;string&gt;:6: RuntimeWarning: divide by zero encountered in log
```

&lt;img src="images/Degree-Hists4-1.png" width="100%" /&gt;

---

### Heavy Tail

The `\(k\)`-th moment of the PL distribution converges only for `\(k &lt; \lfloor{\alpha-1}\rfloor\)`:

`\(E(X^k)=\int_{x_\text{min}}^{\infty}x^kf(x)dx=\frac{\alpha-1}{x^{1-\alpha}_\text{min}}\int_{x_\text{min}}^{\infty}x^{k-\alpha}dx=x^k_\text{min}(\frac{\alpha-1}{\alpha-1-k})\)`

- For `\(1&lt;\alpha\le2\)` the mean is infinite (and all higher moments)
- For `\(2&lt;\alpha\le3\)` the mean is finite but the variance is infinite

As a result, the PL distribution is very good for modeling extreme distributions, where we *expect* extreme values, as opposed to e.g. the Normal distribution.

Also, if we sample from a typical PL distribution with `\(2&lt;\alpha&lt;3\)` the value could be "anywhere" and isn't confined by a scale parameter

.insight[
ðŸ’¡ When sampling from a Normal distribution, we can expect in 95% of cases...
]

---

For example, here is a [post](http://giorasimchoni.com/2019/03/04/2019-03-04-on-fechner-weber-whorf-zipf-and-pareto/) of mine, showing how the PL distribution is much more suitable for modeling how words are distributed in a large corpus of text such as Wikipedia, in comparison with the Exponential distribution, both fitted via MLE:

&lt;img src = "images/Wikipedia-Pareto-Exponential_Fit.png" style="width: 100%"&gt;

---

Another nice way of showing this is comparing `\(f(X)\)` under the PL distribution vs. `\(f(X)\)` under the Exponential distribution for suitable values of `\(\alpha\)` and `\(\lambda\)`:

&lt;img src = "images/PL-Exponential-Compare.png" style="width: 100%"&gt;

---

### 80/20 (Pareto) Rule

The Pareto rule says that "80% of the wealth is in the hands of the richest 20% of people".

But the Pareto distribution is just a different formulation of the Power Law distribution, and this phenomenon is indeed characteristic to the PL distribution.

If `\(X\)` is "wealth" and we want to know what is the probability of being "wealthy", i.e. the part *of population* which holds at least `\(m\)` wealth:

`\(P(X)=P(X&gt;m)=1-F_X(m)=(\frac{m}{x_\text{min}})^{1-\alpha}\)`

The part *of wealth* that those "wealthy" have is the division of expectations:

`\(W(X) = \frac{E(X &gt; m)}{E(X)}=\frac{\int_{m}^{\infty}xCx^{-\alpha}dx}{x_\text{min}(\frac{\alpha-1}{\alpha-2})}=(\frac{m}{x_\text{min}})^{2-\alpha}\)`

---

So, being wealthy isn't dependent on `\(m\)`, only on `\(\alpha\)`:

`\(W(X)=P(X)^{\frac{\alpha-2}{\alpha-1}}\)`

And this is how we get to the 80/20 rule without defining what wealthy is:

&lt;img src = "images/PL-Pareto-Rule.png" style="width: 100%"&gt;

.insight[
ðŸ’¡ The dashed line shows the empirical Lorenz curve for the wealthiest individuals in the United States (data from the Forbes 400, 2003)!
]
---

class: section-slide

# The Configuration Model

---

### Scenario 1

- We have a sequence of each node's degree: `\({k_1, ..., k_n}\)`
- Each node is given a number of "stubs" of edges equal to its required degree.
- Choose a random pair of stubs, connect.
- From the stubs left, choose another random pair, connect.
- Repeat until all stubs are left.

&lt;img src = "images/Configuration-Model-Stubs.png" style="width: 40%"&gt;

---

### Scenario 2:

- We have a degree *distribution*: `\({k_1:p_{k_1}, ..., k_m: p_{k_m}}\)` where `\(\sum_{i=1}^{m}p_{k_i}=1\)`
- We sample `\(n\)` degrees from this distribution and continue as usual.

.insight[
ðŸ’¡ What if `\(p_{k_i}\)` is Poisson?
]

Finally notice that as with the ER model, we get a *distribution* of networks from every sequence or degree-distribution specified. And this distribution of networks is Uniform!

---

### Two issues

.insight[
ðŸ’¡ Try building a network with degree sequence `\(\{1,2,2\}\)`
]

1. If `\(m\)` is no. of edges `\(\sum_i k_i = 2m\)` --&gt; the sum of degrees must be even! Even those sampled from `\(p_{k_i}\)`

2. Nothing preventing connecting two of node `\(i\)`'s stubs together
Nothing preventing commcting nodes `\(i\)` and `\(j\)` more than once
--&gt; Self-edges and multi-edges are possible! .font80percent[(But with large n chances are small)]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="../libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
