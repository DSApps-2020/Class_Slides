<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Modeling in the Tidyverse</title>
    <meta charset="utf-8" />
    <meta name="author" content="Giora Simchoni" />
    <meta name="date" content="2020-03-01" />
    <head>
      <link rel="icon" href="../DSApps_logo.jpg" type="image/jpg"> 
      <link rel="shortcut icon" href="../DSApps_logo.jpg" type="image/jpg">
    </head>
    <link rel="stylesheet" href="..\slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">


class: logo-slide

---

class: title-slide

## Modeling in the Tidyverse

### Applications of Data Science - Class 5

### Giora Simchoni

#### `gsimchoni@gmail.com and add #dsapps in subject`

### Stat. and OR Department, TAU
### 2020-03-01

---



layout: true

&lt;div class="my-footer"&gt;
  &lt;span&gt;
    &lt;a href="https://dsapps-2020.github.io/Class_Slides/" target="_blank"&gt;Applications of Data Science
    &lt;/a&gt;
  &lt;/span&gt;
&lt;/div&gt;

---



class: section-slide

# The Problem

---

### Inconsistency, Inextensibility


```r
n &lt;- 10000
x1 &lt;- runif(n)
x2 &lt;- runif(n)
t &lt;- 1 + 2 * x1 + 3 * x2
y &lt;- rbinom(n, 1, 1 / (1 + exp(-t)))
```


```r
glm(y ~ x1 + x2, family = "binomial")
```


```r
glmnet(as.matrix(cbind(x1, x2)), as.factor(y), family = "binomial")
```


```r
randomForest(as.factor(y) ~ x1 + x2)
```



```r
gbm(y ~ x1 + x2, data = data.frame(x1 = x1, x2 = x2, y = y))
```

😱

---

### Compare this with `sklearn`


```python
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier,
  GradientBoostingClassifier

LogisticRegression(penalty='none').fit(X, y)

LogisticRegression(penalty='l2', C=0.001).fit(X, y)

RandomForestClassifier(n_estimators=100).fit(X, y)

GradientBoostingClassifier(n_estimators=100).fit(X, y)
```

---

class: section-slide

# Detour: A Regression Problem

---

### IPF-Lifts: Predicting Bench Lifting

- Dataset was published as part of the [TidyTuesday](https://github.com/rfordatascience/tidytuesday) intiative
- Comes from [Open Powerlifting](https://www.openpowerlifting.org/data)
- [Wikipedia](https://en.wikipedia.org/wiki/Powerlifting): Powerlifting is a strength sport that consists of three attempts at maximal weight on three lifts: squat, bench press, and deadlift

&lt;img src="images/pl_bench.jpg" style="width: 70%" /&gt;

---

The raw data has over 40K rows: for each athlete, for each event, stats about athlete gender, age and weight, and the maximal weight lifted in the 3 types of Powerlifting.

We will be predicting `best3bench_kg` based on a few predictors, no missing values:


```r
library(lubridate)

ipf_lifts &lt;- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-08/ipf_lifts.csv")

ipf_lifts &lt;- ipf_lifts %&gt;%
  drop_na(best3bench_kg, age) %&gt;%
  filter(between(age, 18, 100), best3bench_kg &gt; 0, equipment != "Wraps") %&gt;%
  select(sex, event, equipment, age, division, bodyweight_kg, best3bench_kg, date, meet_name) %&gt;%
  drop_na() %&gt;%
  mutate(year = year(date), month = month(date),
         dayofweek = wday(date)) %&gt;%
  select(-date) %&gt;%
  mutate_if(is.character, as.factor)

dim(ipf_lifts)
```

```
## [1] 32047    11
```

---


```r
glimpse(ipf_lifts)
```

```
## Observations: 32,047
## Variables: 11
## $ sex           &lt;fct&gt; F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, ...
## $ event         &lt;fct&gt; SBD, SBD, SBD, SBD, SBD, SBD, SBD, SBD, SBD, SBD...
## $ equipment     &lt;fct&gt; Single-ply, Single-ply, Single-ply, Single-ply, ...
## $ age           &lt;dbl&gt; 33.5, 34.5, 23.5, 27.5, 37.5, 25.5, 33.5, 26.0, ...
## $ division      &lt;fct&gt; Open, Open, Open, Open, Open, Open, Open, Open, ...
## $ bodyweight_kg &lt;dbl&gt; 44, 44, 44, 44, 44, 44, 48, 48, 48, 48, 48, 48, ...
## $ best3bench_kg &lt;dbl&gt; 60.0, 62.5, 62.5, 60.0, 65.0, 45.0, 62.5, 77.5, ...
## $ meet_name     &lt;fct&gt; World Powerlifting Championships, World Powerlif...
## $ year          &lt;dbl&gt; 1989, 1989, 1989, 1989, 1989, 1989, 1989, 1989, ...
## $ month         &lt;dbl&gt; 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, ...
## $ dayofweek     &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...
```

---

See the dependent variable distribution:


```r
ggplot(ipf_lifts, aes(best3bench_kg)) +
  geom_histogram(fill = "red", alpha = 0.5) +
  theme_classic()
```

&lt;img src="images/Bench-Hist-1.png" width="100%" /&gt;

---

See it vs. say age, facetted by equipment:


```r
ggplot(ipf_lifts, aes(age, best3bench_kg)) +
  geom_point(color = "red", alpha = 0.5) +
  facet_wrap(~ equipment) +
  theme_classic()
```

&lt;img src="images/Bench-Age-Equipment-1.png" width="100%" /&gt;

---

See it vs. year, by gender:


```r
ggplot(ipf_lifts, aes(factor(year), best3bench_kg, fill = sex)) +
  geom_boxplot(outlier.alpha = 0.5) +
  labs(fill = "", x = "", y = "") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```

&lt;img src="images/Bench-Year-Gender-1.png" width="100%" /&gt;

---

Maybe add `\(age^2\)` and `\(year^2\)` to make Linear Regression's life easier?


```r
ipf_lifts &lt;- ipf_lifts %&gt;%
  mutate(age2 = age ^ 2, year2 = year ^2)
```

---

class: section-slide

# End of Detour

---

class: section-slide

# The Present Solution: `caret`

---

### Split Data


```r
library(caret)

train_idx &lt;- createDataPartition(ipf_lifts$best3bench_kg,
                                 p = 0.6, list = FALSE)

ipf_tr &lt;- ipf_lifts[train_idx, ]
ipf_te &lt;- ipf_lifts[-train_idx, ]

library(glue)
glue("train no. of rows: {nrow(ipf_tr)}
     test no. of rows: {nrow(ipf_te)}")
```

```
## train no. of rows: 19230
## test no. of rows: 12817
```

Here you might consider some preprocessing.

`caret` has some nice documentation [here](https://topepo.github.io/caret/index.html).

---

### Tuning and Modeling

Define general methodology, e.g. 10-fold Cross-Validation:


```r
fit_control &lt;- trainControl(method = "cv", number = 5)

ridge_grid &lt;- expand.grid(alpha=0, lambda = 10^seq(-3, 1, length = 50))
lasso_grid &lt;- expand.grid(alpha=1, lambda = 10^seq(-3, 1, length = 50))
rf_grid &lt;- expand.grid(splitrule = "variance",
                       min.node.size = seq(10, 30, 10),
                       mtry = seq(2, 10, 2))

mod_ridge &lt;- train(best3bench_kg ~ ., data = ipf_tr, method = "glmnet",
                trControl = fit_control, tuneGrid = ridge_grid,
                metric = "RMSE")

mod_lasso &lt;- train(best3bench_kg ~ ., data = ipf_tr, method = "glmnet",
                trControl = fit_control, tuneGrid = lasso_grid,
                metric = "RMSE")

mod_rf &lt;- train(best3bench_kg ~ ., data = ipf_tr, method = "ranger",
                trControl = fit_control, tuneGrid = rf_grid,
                num.trees = 50, metric = "RMSE")
```

---

### Evaluating Models


```r
mod_ridge
```

```
## glmnet 
## 
## 19230 samples
##    12 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 15385, 15384, 15384, 15384, 15383 
## Resampling results across tuning parameters:
## 
##   lambda        RMSE      Rsquared   MAE     
##    0.001000000  26.84827  0.8099138  20.55394
##    0.001206793  26.84827  0.8099138  20.55394
##    0.001456348  26.84827  0.8099138  20.55394
##    0.001757511  26.84827  0.8099138  20.55394
##    0.002120951  26.84827  0.8099138  20.55394
##    0.002559548  26.84827  0.8099138  20.55394
##    0.003088844  26.84827  0.8099138  20.55394
##    0.003727594  26.84827  0.8099138  20.55394
##    0.004498433  26.84827  0.8099138  20.55394
##    0.005428675  26.84827  0.8099138  20.55394
##    0.006551286  26.84827  0.8099138  20.55394
##    0.007906043  26.84827  0.8099138  20.55394
##    0.009540955  26.84827  0.8099138  20.55394
##    0.011513954  26.84827  0.8099138  20.55394
##    0.013894955  26.84827  0.8099138  20.55394
##    0.016768329  26.84827  0.8099138  20.55394
##    0.020235896  26.84827  0.8099138  20.55394
##    0.024420531  26.84827  0.8099138  20.55394
##    0.029470517  26.84827  0.8099138  20.55394
##    0.035564803  26.84827  0.8099138  20.55394
##    0.042919343  26.84827  0.8099138  20.55394
##    0.051794747  26.84827  0.8099138  20.55394
##    0.062505519  26.84827  0.8099138  20.55394
##    0.075431201  26.84827  0.8099138  20.55394
##    0.091029818  26.84827  0.8099138  20.55394
##    0.109854114  26.84827  0.8099138  20.55394
##    0.132571137  26.84827  0.8099138  20.55394
##    0.159985872  26.84827  0.8099138  20.55394
##    0.193069773  26.84827  0.8099138  20.55394
##    0.232995181  26.84827  0.8099138  20.55394
##    0.281176870  26.84827  0.8099138  20.55394
##    0.339322177  26.84827  0.8099138  20.55394
##    0.409491506  26.84827  0.8099138  20.55394
##    0.494171336  26.84827  0.8099138  20.55394
##    0.596362332  26.84827  0.8099138  20.55394
##    0.719685673  26.84827  0.8099138  20.55394
##    0.868511374  26.84827  0.8099138  20.55394
##    1.048113134  26.84827  0.8099138  20.55394
##    1.264855217  26.84827  0.8099138  20.55394
##    1.526417967  26.84827  0.8099138  20.55394
##    1.842069969  26.84827  0.8099138  20.55394
##    2.222996483  26.84827  0.8099138  20.55394
##    2.682695795  26.84827  0.8099138  20.55394
##    3.237457543  26.84827  0.8099138  20.55394
##    3.906939937  26.84827  0.8099138  20.55394
##    4.714866363  26.88392  0.8097278  20.58067
##    5.689866029  26.97878  0.8092723  20.65248
##    6.866488450  27.10141  0.8087582  20.74667
##    8.286427729  27.26126  0.8081651  20.87192
##   10.000000000  27.46912  0.8074851  21.03724
## 
## Tuning parameter 'alpha' was held constant at a value of 0
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were alpha = 0 and lambda = 3.90694.
```

---


```r
mod_lasso
```

```
## glmnet 
## 
## 19230 samples
##    12 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 15386, 15383, 15384, 15384, 15383 
## Resampling results across tuning parameters:
## 
##   lambda        RMSE      Rsquared   MAE     
##    0.001000000  26.43438  0.8137774  20.25171
##    0.001206793  26.43438  0.8137774  20.25171
##    0.001456348  26.43438  0.8137774  20.25171
##    0.001757511  26.43438  0.8137774  20.25171
##    0.002120951  26.43438  0.8137774  20.25171
##    0.002559548  26.43438  0.8137774  20.25171
##    0.003088844  26.43438  0.8137774  20.25171
##    0.003727594  26.43438  0.8137774  20.25171
##    0.004498433  26.43438  0.8137774  20.25171
##    0.005428675  26.43438  0.8137774  20.25171
##    0.006551286  26.43438  0.8137774  20.25171
##    0.007906043  26.43438  0.8137774  20.25171
##    0.009540955  26.43438  0.8137774  20.25171
##    0.011513954  26.43438  0.8137774  20.25171
##    0.013894955  26.43438  0.8137774  20.25171
##    0.016768329  26.43438  0.8137774  20.25171
##    0.020235896  26.43435  0.8137780  20.25164
##    0.024420531  26.43858  0.8137191  20.25500
##    0.029470517  26.44456  0.8136356  20.25967
##    0.035564803  26.45319  0.8135144  20.26570
##    0.042919343  26.46032  0.8134147  20.26997
##    0.051794747  26.46842  0.8133019  20.27479
##    0.062505519  26.48194  0.8131133  20.28365
##    0.075431201  26.50473  0.8127945  20.30191
##    0.091029818  26.52800  0.8124695  20.32093
##    0.109854114  26.55829  0.8120468  20.34607
##    0.132571137  26.59528  0.8115303  20.37619
##    0.159985872  26.64753  0.8107980  20.41680
##    0.193069773  26.67807  0.8103807  20.43715
##    0.232995181  26.68874  0.8102570  20.44139
##    0.281176870  26.70356  0.8100874  20.44780
##    0.339322177  26.72495  0.8098420  20.45850
##    0.409491506  26.75264  0.8095321  20.47319
##    0.494171336  26.78867  0.8091381  20.49289
##    0.596362332  26.84056  0.8085638  20.52597
##    0.719685673  26.91264  0.8077658  20.57336
##    0.868511374  26.99911  0.8068582  20.63053
##    1.048113134  27.10843  0.8057487  20.70476
##    1.264855217  27.25883  0.8042132  20.80935
##    1.526417967  27.47012  0.8020030  20.96068
##    1.842069969  27.73830  0.7992591  21.15052
##    2.222996483  28.10845  0.7953500  21.41861
##    2.682695795  28.60332  0.7899816  21.77796
##    3.237457543  29.24495  0.7828373  22.25694
##    3.906939937  30.06917  0.7732879  22.89125
##    4.714866363  31.09770  0.7608647  23.71575
##    5.689866029  32.52612  0.7412214  24.90421
##    6.866488450  34.11524  0.7183390  26.26222
##    8.286427729  35.58190  0.6992123  27.54427
##   10.000000000  37.44890  0.6714672  29.13341
## 
## Tuning parameter 'alpha' was held constant at a value of 1
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were alpha = 1 and lambda = 0.0202359.
```

---


```r
mod_rf
```

```
## Random Forest 
## 
## 19230 samples
##    12 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 15384, 15383, 15384, 15384, 15385 
## Resampling results across tuning parameters:
## 
##   min.node.size  mtry  RMSE      Rsquared   MAE     
##   10              2    42.97906  0.7374511  34.38019
##   10              4    33.58867  0.7893763  26.34715
##   10              6    28.47842  0.8179302  22.01305
##   10              8    25.65320  0.8359859  19.62846
##   10             10    24.71868  0.8424168  18.83413
##   20              2    44.40910  0.7168126  35.67302
##   20              4    34.16609  0.7813087  26.80165
##   20              6    28.29841  0.8191724  21.81372
##   20              8    25.85930  0.8350953  19.81877
##   20             10    24.62077  0.8432578  18.77625
##   30              2    43.63977  0.7206202  34.90311
##   30              4    33.66368  0.7827368  26.32678
##   30              6    28.19928  0.8202997  21.73367
##   30              8    26.05553  0.8339279  19.93278
##   30             10    24.80283  0.8419966  18.92405
## 
## Tuning parameter 'splitrule' was held constant at a value of variance
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were mtry = 10, splitrule =
##  variance and min.node.size = 20.
```

---


```r
plot(mod_ridge)
```

&lt;img src="images/Ridge-CV-1.png" width="80%" /&gt;

---


```r
plot(mod_lasso)
```

&lt;img src="images/Lasso-CV-1.png" width="80%" /&gt;

---


```r
plot(mod_rf)
```

&lt;img src="images/RF-CV-1.png" width="80%" /&gt;

---

### Comparing Models


```r
resamps &lt;- resamples(list(Ridge = mod_ridge,
                          Lasso = mod_lasso,
                          RF = mod_rf))
summary(resamps)
```

```
## 
## Call:
## summary.resamples(object = resamps)
## 
## Models: Ridge, Lasso, RF 
## Number of resamples: 5 
## 
## MAE 
##           Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's
## Ridge 20.39991 20.45566 20.53437 20.55394 20.53526 20.84448    0
## Lasso 19.88140 19.94063 20.43765 20.25164 20.48677 20.51177    0
## RF    18.40720 18.66643 18.85038 18.77625 18.94520 19.01202    0
## 
## RMSE 
##           Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's
## Ridge 26.50302 26.66880 26.73841 26.84827 26.91660 27.41453    0
## Lasso 25.87176 25.91696 26.76098 26.43435 26.79996 26.82210    0
## RF    24.04310 24.64214 24.67820 24.62077 24.82394 24.91648    0
## 
## Rsquared 
##            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's
## Ridge 0.8062753 0.8079359 0.8085034 0.8099138 0.8127057 0.8141486    0
## Lasso 0.8077883 0.8100547 0.8121120 0.8137780 0.8167442 0.8221907    0
## RF    0.8392144 0.8420675 0.8431639 0.8432578 0.8434077 0.8484356    0
```

---


```r
dotplot(resamps, metric = "RMSE")
```

&lt;img src="images/Caret-RMSE-Comp-1.png" width="100%" /&gt;

---

### Predicting


```r
pred_ridge &lt;- predict(mod_ridge, newdata = ipf_te)
pred_lasso &lt;- predict(mod_lasso, newdata = ipf_te)
pred_rf &lt;- predict(mod_rf, newdata = ipf_te)

rmse_ridge &lt;- RMSE(pred_ridge, ipf_te$best3bench_kg)
rmse_lasso &lt;- RMSE(pred_lasso, ipf_te$best3bench_kg)
rmse_rf &lt;- RMSE(pred_rf, ipf_te$best3bench_kg)

glue("Test RMSE Ridge: {format(rmse_ridge, digits = 3)}
     Test RMSE Lassoe: {format(rmse_lasso, digits = 3)}
     Test RMSE RF: {format(rmse_rf, digits = 3)}")
```

```
## Test RMSE Ridge: 26.5
## Test RMSE Lassoe: 26
## Test RMSE RF: 24.2
```

.warning[
⚠️ Is using a "regular" regression model the natural approach for these data?

Ask yourself what is this model good for, if at all 😮
]

---


```r
bind_rows(
  tibble(method = "Ridge", pred = pred_ridge, true = ipf_te$best3bench_kg),
  tibble(method = "Lasso", pred = pred_lasso, true = ipf_te$best3bench_kg),
  tibble(method = "RF", pred = pred_rf, true = ipf_te$best3bench_kg)) %&gt;%
  ggplot(aes(pred, true)) +
  geom_point(color = "red", alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0) +
  facet_wrap(~ method) +
  theme_bw()
```

&lt;img src="images/Caret-Pred-vs-True-1.png" width="100%" /&gt;

---

class: section-slide

# The Future Solution: `tidymodels`

#### Inspired by [Julia Silge](https://juliasilge.com/blog/intro-tidymodels/)

---

### Packages under tidymodels

- `paresnip`: tidy `caret`
- `dials` and `tune`: specifying and tuning model parameters
- `rsample`: sampling, data partitioning
- `recipes` and `embed`: preprocessing and creating model matrices
- `infer`: tidy statistics
- `yardstick`: measuring models performance
- `broom`: convert models output into tidy tibbles

And [more](https://www.tidyverse.org/blog/2018/08/tidymodels-0-0-1/).

.warning[
⚠️ All `tidymodels` packages are under development!
]

---

### Split Data

The `initial_split()` function is from the `rsample` package:


```r
library(tidymodels)

ipf_split_obj &lt;- ipf_lifts %&gt;%
  initial_split(prop = 0.6, strata = equipment)

ipf_tr &lt;- training(ipf_split_obj)
ipf_te &lt;- testing(ipf_split_obj)

glue("train no. of rows: {nrow(ipf_tr)}
     test no. of rows: {nrow(ipf_te)}")
```

```
## train no. of rows: 19229
## test no. of rows: 12818
```

```r
print(ipf_split_obj)
```

```
## &lt;19229/12818/32047&gt;
```

---

### Preprocess .font80percent[(but we're not gonna use it)]

The `recipe()` function is from the `recipes` package. It allows you to specify a python-like pipe you can later apply to any dataset, including all preprocessing steps:


```r
ipf_rec &lt;- recipe(best3bench_kg ~ ., data = ipf_tr)
ipf_rec
```

```
## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor         12
```

`recipes` contains more preprocessing `step_`s than you imagine:


```r
ipf_rec &lt;-  ipf_rec %&gt;%
  step_normalize(all_numeric())
```

---

After you have your `recipe` you need to `prep()` materials...


```r
ipf_rec &lt;- ipf_rec %&gt;% prep(ipf_tr)

ipf_rec
```

```
## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor         12
## 
## Training data contained 19229 data points and no missing data.
## 
## Operations:
## 
## Centering and scaling for age, bodyweight_kg, year, month, ... [trained]
```

At this point our `recipe` has all necessary `sd` and `mean`s for numeric variables.

---

And then we `bake()`:


```r
ipf_tr2 &lt;- ipf_rec %&gt;% bake(ipf_tr)
ipf_te2 &lt;- ipf_rec %&gt;% bake(ipf_te)

glue("mean of age in orig training: {format(mean(ipf_tr$age), digits = 3)}, sd: {format(sd(ipf_tr$age), digits = 3)}
     mean of age in baked training: {format(mean(ipf_tr2$age), digits = 0)}, sd: {format(sd(ipf_tr2$age), digits = 3)}")
```

```
## mean of age in orig training: 36.7, sd: 14.3
## mean of age in baked training: 0, sd: 1
```

```r
glue("mean of age in orig testing: {format(mean(ipf_te$age), digits = 3)}, sd: {format(sd(ipf_te$age), digits = 3)}
     mean of age in baked testing: {format(mean(ipf_te2$age), digits = 0)}, sd: {format(sd(ipf_te2$age), digits = 3)}")
```

```
## mean of age in orig testing: 36.5, sd: 14.3
## mean of age in baked testing: -0, sd: 0.997
```

---

Or you can do it all in a single pipe:


```r
ipf_rec &lt;- recipe(best3bench_kg ~ ., data = ipf_tr) %&gt;%
  step_normalize(all_numeric()) %&gt;%
  prep(ipf_tr)

ipf_tr2 &lt;- ipf_rec %&gt;% bake(ipf_tr)
ipf_te2 &lt;- ipf_rec %&gt;% bake(ipf_te)

glue("mean of age in orig training: {format(mean(ipf_tr$age), digits = 3)}, sd: {format(sd(ipf_tr$age), digits = 3)}
     mean of age in baked training: {format(mean(ipf_tr2$age), digits = 0)}, sd: {format(sd(ipf_tr2$age), digits = 2)}")
```

```
## mean of age in orig training: 36.7, sd: 14.3
## mean of age in baked training: 0, sd: 1
```

```r
glue("mean of age in orig testing: {format(mean(ipf_te$age), digits = 3)}, sd: {format(sd(ipf_te$age), digits = 3)}
     mean of age in baked testing: {format(mean(ipf_te2$age), digits = 0)}, sd: {format(sd(ipf_te2$age), digits = 3)}")
```

```
## mean of age in orig testing: 36.5, sd: 14.3
## mean of age in baked testing: -0, sd: 0.997
```

---

### Modeling

For now let us use the original `ipf_tr` data.

Functions `linear_reg()` and `set_engine()` are from the `parsnip` package:


```r
mod_ridge_spec &lt;- linear_reg(mixture = 0, penalty = 0.001) %&gt;%
  set_engine(engine = "glmnet")

mod_ridge_spec
```

```
## Linear Regression Model Specification (regression)
## 
## Main Arguments:
##   penalty = 0.001
##   mixture = 0
## 
## Computational engine: glmnet
```

---


```r
mod_ridge &lt;- mod_ridge_spec %&gt;%
  fit(best3bench_kg ~ ., data = ipf_tr)

mod_ridge
```

```
## parsnip model object
## 
## Fit in:  51ms
## Call:  glmnet::glmnet(x = as.matrix(x), y = y, family = "gaussian",      alpha = ~0) 
## 
##     Df    %Dev Lambda
## 1   51 0.00000  43070
## 2   51 0.00398  39250
## 3   51 0.00437  35760
## 4   51 0.00479  32580
## 5   51 0.00526  29690
## 6   51 0.00577  27050
## 7   51 0.00632  24650
## 8   51 0.00694  22460
## 9   51 0.00761  20460
## 10  51 0.00834  18640
## 11  51 0.00915  16990
## 12  51 0.01003  15480
## 13  51 0.01100  14100
## 14  51 0.01206  12850
## 15  51 0.01322  11710
## 16  51 0.01449  10670
## 17  51 0.01588   9721
## 18  51 0.01740   8858
## 19  51 0.01906   8071
## 20  51 0.02088   7354
## 21  51 0.02287   6701
## 22  51 0.02504   6105
## 23  51 0.02742   5563
## 24  51 0.03001   5069
## 25  51 0.03284   4618
## 26  51 0.03592   4208
## 27  51 0.03929   3834
## 28  51 0.04295   3494
## 29  51 0.04694   3183
## 30  51 0.05127   2901
## 31  51 0.05599   2643
## 32  51 0.06111   2408
## 33  51 0.06666   2194
## 34  51 0.07268   1999
## 35  51 0.07920   1822
## 36  51 0.08624   1660
## 37  51 0.09384   1512
## 38  51 0.10200   1378
## 39  51 0.11090   1256
## 40  51 0.12030   1144
## 41  51 0.13050   1042
## 42  51 0.14140    950
## 43  51 0.15300    865
## 44  51 0.16540    788
## 45  51 0.17860    718
## 46  51 0.19250    655
## 47  51 0.20730    596
## 48  51 0.22280    544
## 49  51 0.23920    495
## 50  51 0.25630    451
## 51  51 0.27420    411
## 52  51 0.29270    375
## 53  51 0.31200    341
## 54  51 0.33180    311
## 55  51 0.35210    283
## 56  51 0.37290    258
## 57  51 0.39410    235
## 58  51 0.41550    214
## 59  51 0.43710    195
## 60  51 0.45860    178
## 61  51 0.48020    162
## 62  51 0.50150    148
## 63  51 0.52250    135
## 64  51 0.54310    123
## 65  51 0.56320    112
## 66  51 0.58260    102
## 67  51 0.60130     93
## 68  51 0.61930     85
## 69  51 0.63630     77
## 70  51 0.65250     70
## 71  51 0.66770     64
## 72  51 0.68200     58
## 73  51 0.69520     53
## 74  51 0.70750     48
## 75  51 0.71880     44
## 76  51 0.72920     40
## 77  51 0.73870     37
## 78  51 0.74730     33
## 79  51 0.75510     30
## 80  51 0.76220     28
## 81  51 0.76850     25
## 82  51 0.77410     23
## 83  51 0.77910     21
## 84  51 0.78360     19
## 85  51 0.78760     17
## 86  51 0.79110     16
## 87  51 0.79410     14
## 88  51 0.79690     13
## 89  51 0.79920     12
## 90  51 0.80130     11
## 91  51 0.80320     10
## 92  51 0.80480      9
## 93  51 0.80620      8
## 94  51 0.80740      8
## 95  51 0.80850      7
## 96  51 0.80940      6
## 97  51 0.81030      6
## 98  51 0.81100      5
## 99  51 0.81170      5
## 100 51 0.81220      4
```

---

In a single pipe:


```r
mod_lasso &lt;- linear_reg(mixture = 1, penalty = 0.001) %&gt;%
  set_engine(engine = "glmnet") %&gt;%
  fit(best3bench_kg ~ ., data = ipf_tr)

mod_lasso
```

```
## parsnip model object
## 
## Fit in:  41ms
## Call:  glmnet::glmnet(x = as.matrix(x), y = y, family = "gaussian",      alpha = ~1) 
## 
##    Df    %Dev Lambda
## 1   0 0.00000 43.070
## 2   1 0.08401 39.250
## 3   2 0.16420 35.760
## 4   2 0.24370 32.580
## 5   2 0.30980 29.690
## 6   2 0.36460 27.050
## 7   2 0.41010 24.650
## 8   2 0.44780 22.460
## 9   2 0.47920 20.460
## 10  2 0.50520 18.640
## 11  2 0.52690 16.990
## 12  2 0.54480 15.480
## 13  2 0.55970 14.100
## 14  3 0.57220 12.850
## 15  4 0.59170 11.710
## 16  4 0.61310 10.670
## 17  6 0.63510  9.721
## 18  6 0.65400  8.858
## 19  6 0.66960  8.071
## 20  7 0.68300  7.354
## 21  7 0.69540  6.701
## 22  8 0.71120  6.105
## 23  8 0.72610  5.563
## 24  8 0.73840  5.069
## 25  8 0.74870  4.618
## 26  8 0.75720  4.208
## 27 10 0.76460  3.834
## 28 11 0.77120  3.494
## 29 12 0.77690  3.183
## 30 13 0.78220  2.901
## 31 13 0.78650  2.643
## 32 13 0.79010  2.408
## 33 14 0.79330  2.194
## 34 15 0.79620  1.999
## 35 16 0.79870  1.822
## 36 16 0.80070  1.660
## 37 17 0.80260  1.512
## 38 19 0.80430  1.378
## 39 19 0.80570  1.256
## 40 19 0.80690  1.144
## 41 20 0.80790  1.042
## 42 20 0.80870  0.950
## 43 24 0.80950  0.865
## 44 27 0.81030  0.788
## 45 27 0.81100  0.718
## 46 29 0.81160  0.655
## 47 29 0.81220  0.596
## 48 29 0.81260  0.544
## 49 29 0.81290  0.495
## 50 32 0.81330  0.451
## 51 32 0.81350  0.411
## 52 32 0.81380  0.375
## 53 32 0.81400  0.341
## 54 33 0.81410  0.311
## 55 36 0.81430  0.283
## 56 36 0.81440  0.258
## 57 37 0.81450  0.235
## 58 40 0.81460  0.214
## 59 43 0.81470  0.195
## 60 43 0.81510  0.178
## 61 43 0.81560  0.162
## 62 43 0.81610  0.148
## 63 42 0.81640  0.135
## 64 42 0.81670  0.123
## 65 44 0.81700  0.112
## 66 44 0.81720  0.102
## 67 45 0.81740  0.093
## 68 45 0.81760  0.085
## 69 46 0.81770  0.077
## 70 47 0.81790  0.070
## 71 48 0.81800  0.064
## 72 50 0.81820  0.058
## 73 49 0.81830  0.053
## 74 49 0.81840  0.048
## 75 49 0.81850  0.044
## 76 49 0.81850  0.040
## 77 50 0.81860  0.037
## 78 50 0.81870  0.033
## 79 50 0.81870  0.030
## 80 50 0.81880  0.028
## 81 49 0.81890  0.025
## 82 48 0.81890  0.023
## 83 49 0.81890  0.021
## 84 48 0.81900  0.019
## 85 48 0.81900  0.017
```

---

Can also use `fit_xy()` a-la `sklearn`:


```r
mod_rf &lt;- rand_forest(mode = "regression", mtry = 4, trees = 50, min_n = 30) %&gt;%
  set_engine("ranger") %&gt;%
  fit_xy(x = ipf_tr[, -7],
         y = ipf_tr$best3bench_kg)

mod_rf
```

```
## parsnip model object
## 
## Fit in:  641msRanger result
## 
## Call:
##  ranger::ranger(formula = formula, data = data, mtry = ~4, num.trees = ~50,      min.node.size = ~30, num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1)) 
## 
## Type:                             Regression 
## Number of trees:                  50 
## Sample size:                      19229 
## Number of independent variables:  12 
## Mtry:                             4 
## Target node size:                 30 
## Variable importance mode:         none 
## Splitrule:                        variance 
## OOB prediction error (MSE):       562.6713 
## R squared (OOB):                  0.8499346
```

---

Notice how easy it is to get the model's results in a tidy way using the `tidy()` function:


```r
tidy(mod_ridge)
```

```
## # A tibble: 5,200 x 5
##    term                 step  estimate lambda dev.ratio
##    &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;
##  1 (Intercept)             1  1.48e+ 2 43072.  2.59e-36
##  2 sexM                    1  8.51e-35 43072.  2.59e-36
##  3 eventSB                 1  3.20e-36 43072.  2.59e-36
##  4 eventSBD                1 -2.44e-35 43072.  2.59e-36
##  5 equipmentSingle-ply     1  3.04e-35 43072.  2.59e-36
##  6 age                     1 -5.54e-37 43072.  2.59e-36
##  7 divisionJuniors         1 -4.85e-36 43072.  2.59e-36
##  8 divisionLight           1 -1.81e-35 43072.  2.59e-36
##  9 divisionMasters 1       1 -1.11e-36 43072.  2.59e-36
## 10 divisionMasters 2       1 -1.54e-35 43072.  2.59e-36
## # ... with 5,190 more rows
```

---

### Predicting


```r
results_test &lt;- mod_ridge %&gt;%
  predict(new_data = ipf_te, penalty = 0.001) %&gt;%
  mutate(
    truth = ipf_te$best3bench_kg,
    method = "Ridge"
  ) %&gt;%
  bind_rows(mod_lasso %&gt;%
    predict(new_data = ipf_te) %&gt;%
    mutate(
      truth = ipf_te$best3bench_kg,
      method = "Lasso"
  )) %&gt;%
  bind_rows(mod_rf %&gt;%
    predict(new_data = ipf_te) %&gt;%
    mutate(
      truth = ipf_te$best3bench_kg,
      method = "RF"
  ))

dim(results_test)
```

```
## [1] 38454     3
```

---

### Comparing Models

The package `yardstick` has tons of performance metrics:


```r
results_test %&gt;%
  group_by(method) %&gt;%
  yardstick::rmse(truth = truth, estimate = .pred)
```

```
## # A tibble: 3 x 4
##   method .metric .estimator .estimate
##   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 Lasso  rmse    standard        26.4
## 2 RF     rmse    standard        23.7
## 3 Ridge  rmse    standard        26.8
```

---


```r
results_test %&gt;%
  ggplot(aes(.pred, truth)) +
  geom_point(color = "red", alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0) +
  facet_wrap(~ method) +
  theme_bw()
```

&lt;img src="images/Tidymodels-Pred-vs-True-1.png" width="100%" /&gt;

---

### Tuning

This isn't completely clear to me, but it seems to work:

Define your model spec, using `tune()` from the `tune` package (needs to be installed separately) for a paramter you wish to tune:


```r
library(tune)

mod_rf_spec &lt;- rand_forest(mode = "regression",
                           mtry = tune("mtry"),
                           min_n = tune("min_n")) %&gt;%
  set_engine("ranger")
```

---

Define the `grid` on which you train your params, with the `dials` package:


```r
rf_grid &lt;- grid_regular(mtry(range(2, 10)), min_n(range(10, 30)),
                        levels = c(5, 3))

rf_grid
```

```
## # A tibble: 15 x 2
##     mtry min_n
##    &lt;int&gt; &lt;int&gt;
##  1     2    10
##  2     4    10
##  3     6    10
##  4     8    10
##  5    10    10
##  6     2    20
##  7     4    20
##  8     6    20
##  9     8    20
## 10    10    20
## 11     2    30
## 12     4    30
## 13     6    30
## 14     8    30
## 15    10    30
```

---

Split your data into a few folds for Cross Validation with `vfold_cv()` from the `rsample` package:


```r
cv_splits &lt;- vfold_cv(ipf_tr, v = 5)

cv_splits
```

```
## #  5-fold cross-validation 
## # A tibble: 5 x 2
##   splits               id   
##   &lt;named list&gt;         &lt;chr&gt;
## 1 &lt;split [15.4K/3.8K]&gt; Fold1
## 2 &lt;split [15.4K/3.8K]&gt; Fold2
## 3 &lt;split [15.4K/3.8K]&gt; Fold3
## 4 &lt;split [15.4K/3.8K]&gt; Fold4
## 5 &lt;split [15.4K/3.8K]&gt; Fold5
```

---

Now perform cross validation with `tune_grid()` from the `tune` package:


```r
tune_res &lt;- tune_grid(recipe(best3bench_kg ~ ., data = ipf_tr),
                      model = mod_rf_spec,
                      resamples = cv_splits,
                      grid = rf_grid,
                      metrics = metric_set(rmse))
tune_res
```


```
## #  5-fold cross-validation 
## # A tibble: 5 x 4
##   splits               id    .metrics          .notes          
## * &lt;list&gt;               &lt;chr&gt; &lt;list&gt;            &lt;list&gt;          
## 1 &lt;split [15.4K/3.8K]&gt; Fold1 &lt;tibble [15 x 5]&gt; &lt;tibble [0 x 1]&gt;
## 2 &lt;split [15.4K/3.8K]&gt; Fold2 &lt;tibble [15 x 5]&gt; &lt;tibble [0 x 1]&gt;
## 3 &lt;split [15.4K/3.8K]&gt; Fold3 &lt;tibble [15 x 5]&gt; &lt;tibble [0 x 1]&gt;
## 4 &lt;split [15.4K/3.8K]&gt; Fold4 &lt;tibble [15 x 5]&gt; &lt;tibble [0 x 1]&gt;
## 5 &lt;split [15.4K/3.8K]&gt; Fold5 &lt;tibble [15 x 5]&gt; &lt;tibble [0 x 1]&gt;
```

---

Collect the mean metric across folds:


```r
estimates &lt;- collect_metrics(tune_res)

estimates
```

```
## # A tibble: 15 x 7
##     mtry min_n .metric .estimator  mean     n std_err
##    &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
##  1     2    10 rmse    standard    24.7     5   0.197
##  2     2    20 rmse    standard    24.7     5   0.190
##  3     2    30 rmse    standard    24.8     5   0.192
##  4     4    10 rmse    standard    23.7     5   0.186
##  5     4    20 rmse    standard    23.6     5   0.192
##  6     4    30 rmse    standard    23.5     5   0.181
##  7     6    10 rmse    standard    23.9     5   0.191
##  8     6    20 rmse    standard    23.7     5   0.186
##  9     6    30 rmse    standard    23.6     5   0.186
## 10     8    10 rmse    standard    24.0     5   0.203
## 11     8    20 rmse    standard    23.7     5   0.185
## 12     8    30 rmse    standard    23.6     5   0.185
## 13    10    10 rmse    standard    24.1     5   0.206
## 14    10    20 rmse    standard    23.8     5   0.181
## 15    10    30 rmse    standard    23.7     5   0.183
```

---

Choose best paramter:


```r
estimates %&gt;%
  mutate(min_n = factor(min_n)) %&gt;%
  ggplot(aes(x = mtry, y = mean, color = min_n)) + 
  geom_point() + 
  geom_line() + 
  labs(y = "Mean RMSE") +
  theme_classic()
```

&lt;img src="images/Tidymodels-RMSE-Comp-1.png" width="100%" /&gt;

---

class: section-slide

# `infer`: Tidy Statistics

---

### Statistical Q1

Is there a relation between men and women and the type of equipment they use in 2019? Assume observations are independent.


```r
sex_vs_equipment &lt;- ipf_lifts %&gt;%
  filter(year == 2019) %&gt;%
  select(sex, equipment) %&gt;%
  table()

sex_vs_equipment
```

```
##    equipment
## sex Raw Single-ply
##   F 678        186
##   M 854        287
```


```r
prop.table(sex_vs_equipment, margin = 1)
```

```
##    equipment
## sex       Raw Single-ply
##   F 0.7847222  0.2152778
##   M 0.7484663  0.2515337
```

---

### Statistical Q2

Is there a difference between men and women age in 2019? Assume observations are independent.


```r
ipf_lifts %&gt;%
  filter(year == 2019) %&gt;%
  group_by(sex) %&gt;% summarise(avg = mean(age), sd = sd(age), n = n())
```

```
## # A tibble: 2 x 4
##   sex     avg    sd     n
##   &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;
## 1 F      36.0  15.5   864
## 2 M      38.8  16.7  1141
```

---

### Same Problem!

Varied interface, varied output.


```r
prop.test(sex_vs_equipment[,1], rowSums(sex_vs_equipment))
```

```
## 
## 	2-sample test for equality of proportions with continuity
## 	correction
## 
## data:  sex_vs_equipment[, 1] out of rowSums(sex_vs_equipment)
## X-squared = 3.3872, df = 1, p-value = 0.0657
## alternative hypothesis: two.sided
## 95 percent confidence interval:
##  -0.001975717  0.074487646
## sample estimates:
##    prop 1    prop 2 
## 0.7847222 0.7484663
```

---


```r
t.test(age ~ sex, data = ipf_lifts %&gt;% filter(year == 2019))
```

```
## 
## 	Welch Two Sample t-test
## 
## data:  age by sex
## t = -3.8797, df = 1921.8, p-value = 0.0001081
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -4.228319 -1.388844
## sample estimates:
## mean in group F mean in group M 
##        35.97801        38.78659
```

---

### The `generics::tidy()` Approach

(Also available when you load several other packages, like `broom` and `yardstick`)


```r
tidy(prop.test(sex_vs_equipment[,1], rowSums(sex_vs_equipment)))
```

```
## # A tibble: 1 x 9
##   estimate1 estimate2 statistic p.value parameter conf.low conf.high method
##       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt; 
## 1     0.785     0.748      3.39  0.0657         1 -0.00198    0.0745 2-sam~
## # ... with 1 more variable: alternative &lt;chr&gt;
```


```r
tidy(t.test(age ~ sex, data = ipf_lifts %&gt;% filter(year == 2019)))
```

```
## # A tibble: 1 x 10
##   estimate estimate1 estimate2 statistic p.value parameter conf.low
##      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1    -2.81      36.0      38.8     -3.88 1.08e-4     1922.    -4.23
## # ... with 3 more variables: conf.high &lt;dbl&gt;, method &lt;chr&gt;,
## #   alternative &lt;chr&gt;
```

---

### The `infer` Approach

&gt; infer implements an expressive grammar to perform statistical inference that coheres with the tidyverse design framework

4 main verbs for a typical flow:

* `specify()` - dependent/independent variables, formula
* `hypothesize()` - declare the null hypothesis
* `generate()` - generate data reflecting the null hypothesis (the permutation/bootstrap approach)
* `calculate()` - calculate a distribution of statistics from the generated data, from which you can extract conclusion based on a p-value for example

---

### `infer` Diff in Proportions Test

Get the observed statistic (here manually in order to not confuse you, there *is* a way via `infer`):


```r
#    equipment
# sex Raw Single-ply
#   F 678        186
#   M 854        287
p_F &lt;- sex_vs_equipment[1, 1] / (sum(sex_vs_equipment[1, ]))
p_M &lt;- sex_vs_equipment[2, 1] / (sum(sex_vs_equipment[2, ]))
obs_diff &lt;- p_F - p_M
obs_diff
```

```
## [1] 0.03625596
```

---

Get distribution of the difference in proportions under null hypothesis


```r
diff_null_perm &lt;- ipf_lifts %&gt;%
  filter(year == 2019) %&gt;%
  specify(equipment ~ sex, success = "Raw") %&gt;%
  hypothesize(null = "independence") %&gt;%
  generate(reps = 200, type = "permute") %&gt;%
  calculate(stat = "diff in props", order = c("F", "M"))

diff_null_perm
```

```
## # A tibble: 200 x 2
##    replicate      stat
##        &lt;int&gt;     &lt;dbl&gt;
##  1         1 -0.00442 
##  2         2  0.0220  
##  3         3 -0.000353
##  4         4 -0.0126  
##  5         5  0.00371 
##  6         6 -0.0105  
##  7         7  0.00575 
##  8         8 -0.0105  
##  9         9  0.00982 
## 10        10  0.0180  
## # ... with 190 more rows
```

---

Visualize the permuted difference null distribution and the p-value


```r
visualize(diff_null_perm) +
  shade_p_value(obs_stat = obs_diff, direction = "two_sided")
```

&lt;img src="images/Diff-in-Props-Null-1.png" width="50%" /&gt;

---

Get the actual p-value:


```r
diff_null_perm %&gt;% 
  get_p_value(obs_stat = obs_diff, direction = "two_sided")
```

```
## # A tibble: 1 x 1
##   p_value
##     &lt;dbl&gt;
## 1    0.08
```

---

### `infer` t Test (independent samples)

Get the observed statistic (here via `infer`):


```r
obs_t &lt;- ipf_lifts %&gt;%
  filter(year == 2019) %&gt;%
  specify(age ~ sex) %&gt;%
  calculate(stat = "t", order = c("F", "M"))
obs_t
```

```
## # A tibble: 1 x 1
##    stat
##   &lt;dbl&gt;
## 1 -3.88
```


---

Get distribution of the t statistic under null hypothesis


```r
t_null_perm &lt;- ipf_lifts %&gt;%
  filter(year == 2019) %&gt;%
  specify(age ~ sex) %&gt;%
  hypothesize(null = "independence") %&gt;%
  generate(reps = 100, type = "permute") %&gt;%
  calculate(stat = "t", order = c("F", "M"))

t_null_perm
```

```
## # A tibble: 100 x 2
##    replicate   stat
##        &lt;int&gt;  &lt;dbl&gt;
##  1         1  0.190
##  2         2 -0.750
##  3         3 -0.338
##  4         4  1.74 
##  5         5  0.267
##  6         6  1.58 
##  7         7  0.358
##  8         8 -1.36 
##  9         9 -1.37 
## 10        10  0.162
## # ... with 90 more rows
```

---

Visualize the permuted t statistic null distribution and the two-sided p-value


```r
visualize(t_null_perm) +
  shade_p_value(obs_stat = obs_t, direction = "two_sided")
```

&lt;img src="images/T-Null-1.png" width="50%" /&gt;

---

Get the actual p-value:


```r
t_null_perm %&gt;% 
  get_p_value(obs_stat = obs_t, direction = "two_sided")
```

```
## # A tibble: 1 x 1
##   p_value
##     &lt;dbl&gt;
## 1       0
```
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="../libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
