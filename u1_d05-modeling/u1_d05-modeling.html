<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Modeling in the Tidyverse</title>
    <meta charset="utf-8" />
    <meta name="author" content="Giora Simchoni" />
    <meta name="date" content="2020-03-01" />
    <head>
      <link rel="icon" href="../DSApps_logo.jpg" type="image/jpg"> 
      <link rel="shortcut icon" href="../DSApps_logo.jpg" type="image/jpg">
    </head>
    <link rel="stylesheet" href="..\slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">


class: logo-slide

---

class: title-slide

## Modeling in the Tidyverse

### Applications of Data Science - Class 5

### Giora Simchoni

#### `gsimchoni@gmail.com and add #dsapps in subject`

### Stat. and OR Department, TAU
### 2020-03-01

---



layout: true

&lt;div class="my-footer"&gt;
  &lt;span&gt;
    &lt;a href="https://dsapps-2020.github.io/Class_Slides/" target="_blank"&gt;Applications of Data Science
    &lt;/a&gt;
  &lt;/span&gt;
&lt;/div&gt;

---



class: section-slide

# The Problem

---

### Inconsistency, Inextensibility


```r
n &lt;- 10000
x1 &lt;- runif(n)
x2 &lt;- runif(n)
t &lt;- 1 + 2 * x1 + 3 * x2
y &lt;- rbinom(n, 1, 1 / (1 + exp(-t)))
```


```r
glm(y ~ x1 + x2, family = "binomial")
```


```r
glmnet(as.matrix(cbind(x1, x2)), as.factor(y), family = "binomial")
```


```r
randomForest(as.factor(y) ~ x1 + x2)
```



```r
gbm(y ~ x1 + x2, data = data.frame(x1 = x1, x2 = x2, y = y))
```

üò±

---

### Compare this with `sklearn`

---

class: section-slide

# Detour: A Regression Problem

---

### IPF-Lifts: Predicting Bench Lifting

- Dataset was published as part of the [TidyTuesday](https://github.com/rfordatascience/tidytuesday) intiative
- Comes from [Open Powerlifting](https://www.openpowerlifting.org/data)
- [Wikipedia](https://en.wikipedia.org/wiki/Powerlifting): Powerlifting is a strength sport that consists of three attempts at maximal weight on three lifts: squat, bench press, and deadlift

&lt;img src="images/pl_bench.jpg" style="width: 70%" /&gt;

---

The raw data has over 40K rows: for each athlete, for each event, stats about athlete gender, age and weight, and the maximal weight lifted in the 3 types of Powerlifting.

We will be predicting `best3bench_kg` based on a few predictors, no missing values:


```r
library(lubridate)

ipf_lifts &lt;- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-08/ipf_lifts.csv")

ipf_lifts &lt;- ipf_lifts %&gt;%
  drop_na(best3bench_kg, age) %&gt;%
  filter(between(age, 18, 100), best3bench_kg &gt; 0, equipment != "Wraps") %&gt;%
  select(sex, event, equipment, age, division, bodyweight_kg, best3bench_kg, date, meet_name) %&gt;%
  drop_na() %&gt;%
  mutate(year = year(date), month = month(date),
         dayofweek = wday(date)) %&gt;%
  select(-date) %&gt;%
  mutate_if(is.character, as.factor)

dim(ipf_lifts)
```

```
## [1] 32047    11
```

---


```r
glimpse(ipf_lifts)
```

```
## Observations: 32,047
## Variables: 11
## $ sex           &lt;fct&gt; F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, ...
## $ event         &lt;fct&gt; SBD, SBD, SBD, SBD, SBD, SBD, SBD, SBD, SBD, SBD...
## $ equipment     &lt;fct&gt; Single-ply, Single-ply, Single-ply, Single-ply, ...
## $ age           &lt;dbl&gt; 33.5, 34.5, 23.5, 27.5, 37.5, 25.5, 33.5, 26.0, ...
## $ division      &lt;fct&gt; Open, Open, Open, Open, Open, Open, Open, Open, ...
## $ bodyweight_kg &lt;dbl&gt; 44, 44, 44, 44, 44, 44, 48, 48, 48, 48, 48, 48, ...
## $ best3bench_kg &lt;dbl&gt; 60.0, 62.5, 62.5, 60.0, 65.0, 45.0, 62.5, 77.5, ...
## $ meet_name     &lt;fct&gt; World Powerlifting Championships, World Powerlif...
## $ year          &lt;dbl&gt; 1989, 1989, 1989, 1989, 1989, 1989, 1989, 1989, ...
## $ month         &lt;dbl&gt; 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, ...
## $ dayofweek     &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...
```

---

See the dependent variable distribution:


```r
ggplot(ipf_lifts, aes(best3bench_kg)) +
  geom_histogram(fill = "red", alpha = 0.5) +
  theme_classic()
```

&lt;img src="images/Bench-Hist-1.png" width="100%" /&gt;

---

See it vs. say age, facetted by equipment:


```r
ggplot(ipf_lifts, aes(age, best3bench_kg)) +
  geom_point(color = "red", alpha = 0.5) +
  facet_wrap(~ equipment) +
  theme_classic()
```

&lt;img src="images/Bench-Age-Equipment-1.png" width="100%" /&gt;

---

See it vs. year, by gender:


```r
ggplot(ipf_lifts, aes(factor(year), best3bench_kg, fill = sex)) +
  geom_boxplot(outlier.alpha = 0.5) +
  labs(fill = "", x = "", y = "") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```

&lt;img src="images/Bench-Year-Gender-1.png" width="100%" /&gt;

---

Maybe add `\(age^2\)` and `\(year^2\)` to make Linear Regression's life easier?


```r
ipf_lifts &lt;- ipf_lifts %&gt;%
  mutate(age2 = age ^ 2, year2 = year ^2)
```

---

class: section-slide

# End of Detour

---

class: section-slide

# The Present Solution: `caret`

---

### Split Data


```r
library(caret)

train_idx &lt;- createDataPartition(ipf_lifts$best3bench_kg,
                                 p = 0.6, list = FALSE)

ipf_tr &lt;- ipf_lifts[train_idx, ]
ipf_te &lt;- ipf_lifts[-train_idx, ]

library(glue)
glue("train no. of rows: {nrow(ipf_tr)}
     test no. of rows: {nrow(ipf_te)}")
```

```
## train no. of rows: 19230
## test no. of rows: 12817
```

Here you might consider some preprocessing.

`caret` has some nice documentation [here](https://topepo.github.io/caret/index.html).

---

### Tuning and Modeling

Define general methodology, e.g. 10-fold Cross-Validation:


```r
fit_control &lt;- trainControl(method = "cv", number = 5)

ridge_grid &lt;- expand.grid(alpha=0, lambda = 10^seq(-3, 1, length = 50))
lasso_grid &lt;- expand.grid(alpha=1, lambda = 10^seq(-3, 1, length = 50))
rf_grid &lt;- expand.grid(splitrule = "variance",
                       min.node.size = seq(10, 30, 10),
                       mtry = seq(2, 10, 2))

mod_ridge &lt;- train(best3bench_kg ~ ., data = ipf_tr, method = "glmnet",
                trControl = fit_control, tuneGrid = ridge_grid,
                metric = "RMSE")

mod_lasso &lt;- train(best3bench_kg ~ ., data = ipf_tr, method = "glmnet",
                trControl = fit_control, tuneGrid = lasso_grid,
                metric = "RMSE")

mod_rf &lt;- train(best3bench_kg ~ ., data = ipf_tr, method = "ranger",
                trControl = fit_control, tuneGrid = rf_grid,
                num.trees = 50, metric = "RMSE")
```

---

### Evaluating Models


```r
mod_ridge
```

```
## glmnet 
## 
## 19230 samples
##    12 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 15384, 15383, 15384, 15384, 15385 
## Resampling results across tuning parameters:
## 
##   lambda        RMSE      Rsquared   MAE     
##    0.001000000  26.55832  0.8128887  20.39380
##    0.001206793  26.55832  0.8128887  20.39380
##    0.001456348  26.55832  0.8128887  20.39380
##    0.001757511  26.55832  0.8128887  20.39380
##    0.002120951  26.55832  0.8128887  20.39380
##    0.002559548  26.55832  0.8128887  20.39380
##    0.003088844  26.55832  0.8128887  20.39380
##    0.003727594  26.55832  0.8128887  20.39380
##    0.004498433  26.55832  0.8128887  20.39380
##    0.005428675  26.55832  0.8128887  20.39380
##    0.006551286  26.55832  0.8128887  20.39380
##    0.007906043  26.55832  0.8128887  20.39380
##    0.009540955  26.55832  0.8128887  20.39380
##    0.011513954  26.55832  0.8128887  20.39380
##    0.013894955  26.55832  0.8128887  20.39380
##    0.016768329  26.55832  0.8128887  20.39380
##    0.020235896  26.55832  0.8128887  20.39380
##    0.024420531  26.55832  0.8128887  20.39380
##    0.029470517  26.55832  0.8128887  20.39380
##    0.035564803  26.55832  0.8128887  20.39380
##    0.042919343  26.55832  0.8128887  20.39380
##    0.051794747  26.55832  0.8128887  20.39380
##    0.062505519  26.55832  0.8128887  20.39380
##    0.075431201  26.55832  0.8128887  20.39380
##    0.091029818  26.55832  0.8128887  20.39380
##    0.109854114  26.55832  0.8128887  20.39380
##    0.132571137  26.55832  0.8128887  20.39380
##    0.159985872  26.55832  0.8128887  20.39380
##    0.193069773  26.55832  0.8128887  20.39380
##    0.232995181  26.55832  0.8128887  20.39380
##    0.281176870  26.55832  0.8128887  20.39380
##    0.339322177  26.55832  0.8128887  20.39380
##    0.409491506  26.55832  0.8128887  20.39380
##    0.494171336  26.55832  0.8128887  20.39380
##    0.596362332  26.55832  0.8128887  20.39380
##    0.719685673  26.55832  0.8128887  20.39380
##    0.868511374  26.55832  0.8128887  20.39380
##    1.048113134  26.55832  0.8128887  20.39380
##    1.264855217  26.55832  0.8128887  20.39380
##    1.526417967  26.55832  0.8128887  20.39380
##    1.842069969  26.55832  0.8128887  20.39380
##    2.222996483  26.55832  0.8128887  20.39380
##    2.682695795  26.55832  0.8128887  20.39380
##    3.237457543  26.55832  0.8128887  20.39380
##    3.906939937  26.55832  0.8128887  20.39380
##    4.714866363  26.59679  0.8126786  20.42116
##    5.689866029  26.69448  0.8121938  20.49148
##    6.866488450  26.82069  0.8116427  20.58440
##    8.286427729  26.98464  0.8110094  20.70820
##   10.000000000  27.19732  0.8102827  20.87142
## 
## Tuning parameter 'alpha' was held constant at a value of 0
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were alpha = 0 and lambda = 3.90694.
```

---


```r
mod_lasso
```

```
## glmnet 
## 
## 19230 samples
##    12 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 15383, 15384, 15385, 15384, 15384 
## Resampling results across tuning parameters:
## 
##   lambda        RMSE      Rsquared   MAE     
##    0.001000000  26.11807  0.8171747  20.09521
##    0.001206793  26.11807  0.8171747  20.09521
##    0.001456348  26.11807  0.8171747  20.09521
##    0.001757511  26.11807  0.8171747  20.09521
##    0.002120951  26.11807  0.8171747  20.09521
##    0.002559548  26.11807  0.8171747  20.09521
##    0.003088844  26.11807  0.8171747  20.09521
##    0.003727594  26.11807  0.8171747  20.09521
##    0.004498433  26.11807  0.8171747  20.09521
##    0.005428675  26.11807  0.8171747  20.09521
##    0.006551286  26.11807  0.8171747  20.09521
##    0.007906043  26.11807  0.8171747  20.09521
##    0.009540955  26.11807  0.8171747  20.09521
##    0.011513954  26.11807  0.8171747  20.09521
##    0.013894955  26.11807  0.8171747  20.09521
##    0.016768329  26.11813  0.8171739  20.09523
##    0.020235896  26.12040  0.8171431  20.09723
##    0.024420531  26.12413  0.8170919  20.09996
##    0.029470517  26.13019  0.8170078  20.10443
##    0.035564803  26.13649  0.8169211  20.10898
##    0.042919343  26.14350  0.8168248  20.11374
##    0.051794747  26.15523  0.8166620  20.12195
##    0.062505519  26.17087  0.8164455  20.13227
##    0.075431201  26.18912  0.8161937  20.14449
##    0.091029818  26.21289  0.8158667  20.16269
##    0.109854114  26.24413  0.8154356  20.18617
##    0.132571137  26.28693  0.8148429  20.21776
##    0.159985872  26.34645  0.8140151  20.26083
##    0.193069773  26.38074  0.8135511  20.28421
##    0.232995181  26.39402  0.8133937  20.29044
##    0.281176870  26.40956  0.8132164  20.29700
##    0.339322177  26.43018  0.8129862  20.30688
##    0.409491506  26.45863  0.8126705  20.32109
##    0.494171336  26.49643  0.8122565  20.34168
##    0.596362332  26.54892  0.8116809  20.37302
##    0.719685673  26.62046  0.8108990  20.41761
##    0.868511374  26.71153  0.8099323  20.47310
##    1.048113134  26.82106  0.8088325  20.54186
##    1.264855217  26.97353  0.8072838  20.64197
##    1.526417967  27.19236  0.8049867  20.79134
##    1.842069969  27.47925  0.8019795  20.99131
##    2.222996483  27.85257  0.7980568  21.25184
##    2.682695795  28.34407  0.7927885  21.59854
##    3.237457543  28.99574  0.7855255  22.07529
##    3.906939937  29.82476  0.7759466  22.70624
##    4.714866363  30.85228  0.7636230  23.52677
##    5.689866029  32.27130  0.7442974  24.70147
##    6.866488450  33.88751  0.7209618  26.07528
##    8.286427729  35.37387  0.7014656  27.37328
##   10.000000000  37.24450  0.6737609  28.97115
## 
## Tuning parameter 'alpha' was held constant at a value of 1
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were alpha = 1 and lambda = 0.01389495.
```

---


```r
mod_rf
```

```
## Random Forest 
## 
## 19230 samples
##    12 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 15383, 15384, 15384, 15383, 15386 
## Resampling results across tuning parameters:
## 
##   min.node.size  mtry  RMSE      Rsquared   MAE     
##   10              2    44.96363  0.6935470  36.14186
##   10              4    33.41223  0.7921971  26.24106
##   10              6    28.19921  0.8220488  21.76780
##   10              8    25.62778  0.8369921  19.65212
##   10             10    24.37503  0.8454751  18.63220
##   20              2    44.07470  0.7191494  35.35157
##   20              4    34.26181  0.7814633  26.98655
##   20              6    28.06338  0.8227133  21.70162
##   20              8    25.68678  0.8370496  19.75861
##   20             10    24.40480  0.8457247  18.67829
##   30              2    44.05877  0.7061486  35.39419
##   30              4    33.32996  0.7882336  26.07677
##   30              6    28.24078  0.8200617  21.81228
##   30              8    25.74167  0.8360185  19.72742
##   30             10    24.39250  0.8459724  18.64230
## 
## Tuning parameter 'splitrule' was held constant at a value of variance
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were mtry = 10, splitrule =
##  variance and min.node.size = 10.
```

---


```r
plot(mod_ridge)
```

&lt;img src="images/Ridge-CV-1.png" width="80%" /&gt;

---


```r
plot(mod_lasso)
```

&lt;img src="images/Lasso-CV-1.png" width="80%" /&gt;

---


```r
plot(mod_rf)
```

&lt;img src="images/RF-CV-1.png" width="80%" /&gt;

---

### Comparing Models


```r
resamps &lt;- resamples(list(Ridge = mod_ridge,
                          Lasso = mod_lasso,
                          RF = mod_rf))
summary(resamps)
```

```
## 
## Call:
## summary.resamples(object = resamps)
## 
## Models: Ridge, Lasso, RF 
## Number of resamples: 5 
## 
## MAE 
##           Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's
## Ridge 20.16309 20.19137 20.38949 20.39380 20.51260 20.71247    0
## Lasso 19.71590 20.02545 20.12950 20.09521 20.27631 20.32890    0
## RF    18.29181 18.39233 18.67593 18.63220 18.82016 18.98076    0
## 
## RMSE 
##           Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's
## Ridge 26.32026 26.32955 26.47130 26.55832 26.68761 26.98286    0
## Lasso 25.77303 25.96537 26.19022 26.11807 26.31519 26.34653    0
## RF    23.75156 24.08704 24.46089 24.37503 24.52679 25.04886    0
## 
## Rsquared 
##            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's
## Ridge 0.8086477 0.8097555 0.8138765 0.8128887 0.8139533 0.8182104    0
## Lasso 0.8129795 0.8167978 0.8181014 0.8171747 0.8186313 0.8193636    0
## RF    0.8335137 0.8396833 0.8490873 0.8454751 0.8501729 0.8549185    0
```

---


```r
dotplot(resamps, metric = "RMSE")
```

&lt;img src="images/Caret-RMSE-Comp-1.png" width="100%" /&gt;

---

### Predicting


```r
pred_ridge &lt;- predict(mod_ridge, newdata = ipf_te)
pred_lasso &lt;- predict(mod_lasso, newdata = ipf_te)
pred_rf &lt;- predict(mod_rf, newdata = ipf_te)

rmse_ridge &lt;- RMSE(pred_ridge, ipf_te$best3bench_kg)
rmse_lasso &lt;- RMSE(pred_lasso, ipf_te$best3bench_kg)
rmse_rf &lt;- RMSE(pred_rf, ipf_te$best3bench_kg)

glue("Test RMSE Ridge: {format(rmse_ridge, digits = 3)}
     Test RMSE Lassoe: {format(rmse_lasso, digits = 3)}
     Test RMSE RF: {format(rmse_rf, digits = 3)}")
```

```
## Test RMSE Ridge: 26.9
## Test RMSE Lassoe: 26.4
## Test RMSE RF: 25.1
```

.warning[
‚ö†Ô∏è Is using a "regular" regression model the natural approach for these data?

Ask yourself what is this model good for, if at all ü§≠
]

---


```r
bind_rows(
  tibble(method = "Ridge", pred = pred_ridge, true = ipf_te$best3bench_kg),
  tibble(method = "Lasso", pred = pred_lasso, true = ipf_te$best3bench_kg),
  tibble(method = "RF", pred = pred_rf, true = ipf_te$best3bench_kg)) %&gt;%
  ggplot(aes(pred, true)) +
  geom_point(color = "red", alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0) +
  facet_wrap(~ method) +
  theme_bw()
```

&lt;img src="images/Caret-Pred-vs-True-1.png" width="100%" /&gt;

---

class: section-slide

# The Future Solution: `tidymodels`

#### Inspired by [Julia Silge](https://juliasilge.com/blog/intro-tidymodels/)

---

### Packages under tidymodels

- `paresnip`: tidy `caret`
- `dials` and `tune`: specifying and tuning model parameters
- `rsample`: sampling, data partitioning
- `recipes` and `embed`: preprocessing and creating model matrices
- `infer`: tidy statistics
- `yardstick`: measuring models performance
- `broom`: convert models output into tidy tibbles

And [more](https://www.tidyverse.org/blog/2018/08/tidymodels-0-0-1/).

.warning[
‚ö†Ô∏è All `tidymodels` packages are under development!
]

---

### Split Data

The `initial_split()` function is from the `rsample` package:


```r
library(tidymodels)

ipf_split_obj &lt;- ipf_lifts %&gt;%
  initial_split(prop = 0.6, strata = equipment)

ipf_tr &lt;- training(ipf_split_obj)
ipf_te &lt;- testing(ipf_split_obj)

glue("train no. of rows: {nrow(ipf_tr)}
     test no. of rows: {nrow(ipf_te)}")
```

```
## train no. of rows: 19229
## test no. of rows: 12818
```

```r
print(ipf_split_obj)
```

```
## &lt;19229/12818/32047&gt;
```

---

### Preprocess .font80percent[(but we're not gonna use it)]

The `recipe()` function is from the `recipes` package. It allows you to specify a python-like pipe you can later apply to any dataset, including all preprocessing steps:


```r
ipf_rec &lt;- recipe(best3bench_kg ~ ., data = ipf_tr)
ipf_rec
```

```
## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor         12
```

`recipes` contains more preprocessing `step_`s than you imagine:


```r
ipf_rec &lt;-  ipf_rec %&gt;%
  step_normalize(all_numeric())
```

---

After you have your `recipe` you need to `prep()` materials...


```r
ipf_rec &lt;- ipf_rec %&gt;% prep(ipf_tr)

ipf_rec
```

```
## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor         12
## 
## Training data contained 19229 data points and no missing data.
## 
## Operations:
## 
## Centering and scaling for age, bodyweight_kg, year, month, ... [trained]
```

At this point our `recipe` has all necessary `sd` and `mean`s for numeric variables.

---

And then we `bake()`:


```r
ipf_tr2 &lt;- ipf_rec %&gt;% bake(ipf_tr)
ipf_te2 &lt;- ipf_rec %&gt;% bake(ipf_te)

glue("mean of age in orig training: {format(mean(ipf_tr$age), digits = 3)}, sd: {format(sd(ipf_tr$age), digits = 3)}
     mean of age in baked training: {format(mean(ipf_tr2$age), digits = 0)}, sd: {format(sd(ipf_tr2$age), digits = 3)}")
```

```
## mean of age in orig training: 36.6, sd: 14.3
## mean of age in baked training: 0, sd: 1
```

```r
glue("mean of age in orig testing: {format(mean(ipf_te$age), digits = 3)}, sd: {format(sd(ipf_te$age), digits = 3)}
     mean of age in baked testing: {format(mean(ipf_te2$age), digits = 0)}, sd: {format(sd(ipf_te2$age), digits = 3)}")
```

```
## mean of age in orig testing: 36.7, sd: 14.3
## mean of age in baked testing: 0, sd: 0.998
```

---

Or you can do it all in a single pipe:


```r
ipf_rec &lt;- recipe(best3bench_kg ~ ., data = ipf_tr) %&gt;%
  step_normalize(all_numeric()) %&gt;%
  prep(ipf_tr)

ipf_tr2 &lt;- ipf_rec %&gt;% bake(ipf_tr)
ipf_te2 &lt;- ipf_rec %&gt;% bake(ipf_te)

glue("mean of age in orig training: {format(mean(ipf_tr$age), digits = 3)}, sd: {format(sd(ipf_tr$age), digits = 3)}
     mean of age in baked training: {format(mean(ipf_tr2$age), digits = 0)}, sd: {format(sd(ipf_tr2$age), digits = 2)}")
```

```
## mean of age in orig training: 36.6, sd: 14.3
## mean of age in baked training: 0, sd: 1
```

```r
glue("mean of age in orig testing: {format(mean(ipf_te$age), digits = 3)}, sd: {format(sd(ipf_te$age), digits = 3)}
     mean of age in baked testing: {format(mean(ipf_te2$age), digits = 0)}, sd: {format(sd(ipf_te2$age), digits = 3)}")
```

```
## mean of age in orig testing: 36.7, sd: 14.3
## mean of age in baked testing: 0, sd: 0.998
```

---

### Modeling

For now let us use the original `ipf_tr` data.

Functions `linear_reg()` and `set_engine()` are from the `parsnip` package:


```r
mod_ridge_spec &lt;- linear_reg(mixture = 0, penalty = 0.001) %&gt;%
  set_engine(engine = "glmnet")

mod_ridge_spec
```

```
## Linear Regression Model Specification (regression)
## 
## Main Arguments:
##   penalty = 0.001
##   mixture = 0
## 
## Computational engine: glmnet
```

---


```r
mod_ridge &lt;- mod_ridge_spec %&gt;%
  fit(best3bench_kg ~ ., data = ipf_tr)

mod_ridge
```

```
## parsnip model object
## 
## Fit in:  40ms
## Call:  glmnet::glmnet(x = as.matrix(x), y = y, family = "gaussian",      alpha = ~0) 
## 
##     Df    %Dev Lambda
## 1   51 0.00000  43050
## 2   51 0.00394  39230
## 3   51 0.00433  35740
## 4   51 0.00474  32570
## 5   51 0.00521  29670
## 6   51 0.00571  27040
## 7   51 0.00626  24640
## 8   51 0.00687  22450
## 9   51 0.00754  20450
## 10  51 0.00826  18640
## 11  51 0.00906  16980
## 12  51 0.00994  15470
## 13  51 0.01089  14100
## 14  51 0.01194  12840
## 15  51 0.01309  11700
## 16  51 0.01435  10660
## 17  51 0.01573   9717
## 18  51 0.01723   8853
## 19  51 0.01888   8067
## 20  51 0.02069   7350
## 21  51 0.02266   6697
## 22  51 0.02481   6102
## 23  51 0.02716   5560
## 24  51 0.02973   5066
## 25  51 0.03253   4616
## 26  51 0.03559   4206
## 27  51 0.03892   3832
## 28  51 0.04256   3492
## 29  51 0.04651   3182
## 30  51 0.05081   2899
## 31  51 0.05549   2642
## 32  51 0.06056   2407
## 33  51 0.06607   2193
## 34  51 0.07204   1998
## 35  51 0.07851   1821
## 36  51 0.08549   1659
## 37  51 0.09304   1512
## 38  51 0.10120   1377
## 39  51 0.10990   1255
## 40  51 0.11930   1143
## 41  51 0.12940   1042
## 42  51 0.14030    949
## 43  51 0.15180    865
## 44  51 0.16410    788
## 45  51 0.17720    718
## 46  51 0.19110    654
## 47  51 0.20580    596
## 48  51 0.22120    543
## 49  51 0.23750    495
## 50  51 0.25450    451
## 51  51 0.27230    411
## 52  51 0.29080    374
## 53  51 0.30990    341
## 54  51 0.32970    311
## 55  51 0.34990    283
## 56  51 0.37070    258
## 57  51 0.39180    235
## 58  51 0.41310    214
## 59  51 0.43460    195
## 60  51 0.45610    178
## 61  51 0.47760    162
## 62  51 0.49890    148
## 63  51 0.51980    135
## 64  51 0.54040    123
## 65  51 0.56040    112
## 66  51 0.57980    102
## 67  51 0.59850     93
## 68  51 0.61640     85
## 69  51 0.63350     77
## 70  51 0.64960     70
## 71  51 0.66480     64
## 72  51 0.67900     58
## 73  51 0.69230     53
## 74  51 0.70460     48
## 75  51 0.71590     44
## 76  51 0.72630     40
## 77  51 0.73580     37
## 78  51 0.74440     33
## 79  51 0.75220     30
## 80  51 0.75920     28
## 81  51 0.76550     25
## 82  51 0.77110     23
## 83  51 0.77620     21
## 84  51 0.78060     19
## 85  51 0.78460     17
## 86  51 0.78810     16
## 87  51 0.79110     14
## 88  51 0.79390     13
## 89  51 0.79620     12
## 90  51 0.79830     11
## 91  51 0.80020     10
## 92  51 0.80180      9
## 93  51 0.80320      8
## 94  51 0.80440      8
## 95  51 0.80550      7
## 96  51 0.80640      6
## 97  51 0.80720      6
## 98  51 0.80800      5
## 99  51 0.80860      5
## 100 51 0.80920      4
```

---

In a single pipe:


```r
mod_lasso &lt;- linear_reg(mixture = 1, penalty = 0.001) %&gt;%
  set_engine(engine = "glmnet") %&gt;%
  fit(best3bench_kg ~ ., data = ipf_tr)

mod_lasso
```

```
## parsnip model object
## 
## Fit in:  40ms
## Call:  glmnet::glmnet(x = as.matrix(x), y = y, family = "gaussian",      alpha = ~1) 
## 
##    Df    %Dev Lambda
## 1   0 0.00000 43.050
## 2   1 0.08403 39.230
## 3   2 0.16040 35.740
## 4   2 0.24020 32.570
## 5   2 0.30640 29.670
## 6   2 0.36140 27.040
## 7   2 0.40710 24.640
## 8   2 0.44500 22.450
## 9   2 0.47640 20.450
## 10  2 0.50260 18.640
## 11  2 0.52420 16.980
## 12  2 0.54220 15.470
## 13  2 0.55720 14.100
## 14  3 0.57120 12.840
## 15  4 0.59000 11.700
## 16  4 0.61160 10.660
## 17  6 0.63210  9.717
## 18  6 0.65130  8.853
## 19  6 0.66720  8.067
## 20  7 0.68140  7.350
## 21  7 0.69400  6.697
## 22  8 0.70730  6.102
## 23  8 0.72230  5.560
## 24  8 0.73470  5.066
## 25  8 0.74500  4.616
## 26  8 0.75360  4.206
## 27 10 0.76110  3.832
## 28 11 0.76780  3.492
## 29 12 0.77380  3.182
## 30 13 0.77910  2.899
## 31 13 0.78340  2.642
## 32 14 0.78710  2.407
## 33 15 0.79040  2.193
## 34 15 0.79330  1.998
## 35 16 0.79580  1.821
## 36 17 0.79790  1.659
## 37 19 0.79970  1.512
## 38 19 0.80140  1.377
## 39 19 0.80290  1.255
## 40 19 0.80400  1.143
## 41 19 0.80500  1.042
## 42 20 0.80590  0.949
## 43 23 0.80660  0.865
## 44 25 0.80730  0.788
## 45 25 0.80800  0.718
## 46 26 0.80860  0.654
## 47 27 0.80900  0.596
## 48 28 0.80950  0.543
## 49 28 0.80980  0.495
## 50 30 0.81020  0.451
## 51 31 0.81040  0.411
## 52 31 0.81070  0.374
## 53 33 0.81090  0.341
## 54 33 0.81100  0.311
## 55 34 0.81120  0.283
## 56 36 0.81130  0.258
## 57 37 0.81150  0.235
## 58 37 0.81160  0.214
## 59 38 0.81170  0.195
## 60 40 0.81180  0.178
## 61 40 0.81220  0.162
## 62 42 0.81260  0.148
## 63 42 0.81290  0.135
## 64 44 0.81320  0.123
## 65 44 0.81360  0.112
## 66 45 0.81380  0.102
## 67 47 0.81400  0.093
## 68 47 0.81430  0.085
## 69 48 0.81450  0.077
## 70 49 0.81460  0.070
## 71 47 0.81480  0.064
## 72 47 0.81490  0.058
## 73 47 0.81500  0.053
## 74 47 0.81510  0.048
## 75 48 0.81520  0.044
## 76 47 0.81520  0.040
## 77 48 0.81530  0.037
## 78 48 0.81530  0.033
```

---

Can also use `fit_xy()` a-la `sklearn`:


```r
mod_rf &lt;- rand_forest(mode = "regression", mtry = 4, trees = 50, min_n = 30) %&gt;%
  set_engine("ranger") %&gt;%
  fit_xy(x = ipf_tr[, -7],
         y = ipf_tr$best3bench_kg)

mod_rf
```

```
## parsnip model object
## 
## Fit in:  671msRanger result
## 
## Call:
##  ranger::ranger(formula = formula, data = data, mtry = ~4, num.trees = ~50,      min.node.size = ~30, num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1)) 
## 
## Type:                             Regression 
## Number of trees:                  50 
## Sample size:                      19229 
## Number of independent variables:  12 
## Mtry:                             4 
## Target node size:                 30 
## Variable importance mode:         none 
## Splitrule:                        variance 
## OOB prediction error (MSE):       572.301 
## R squared (OOB):                  0.8471802
```

---

Notice how easy it is to get the model's results in a tidy way using the `tidy()` function:


```r
tidy(mod_ridge)
```

```
## # A tibble: 5,200 x 5
##    term                 step  estimate lambda dev.ratio
##    &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;
##  1 (Intercept)             1  1.49e+ 2 43051.  2.56e-36
##  2 sexM                    1  8.47e-35 43051.  2.56e-36
##  3 eventSB                 1  2.53e-36 43051.  2.56e-36
##  4 eventSBD                1 -2.59e-35 43051.  2.56e-36
##  5 equipmentSingle-ply     1  2.85e-35 43051.  2.56e-36
##  6 age                     1 -5.30e-37 43051.  2.56e-36
##  7 divisionJuniors         1 -4.55e-36 43051.  2.56e-36
##  8 divisionLight           1 -1.68e-35 43051.  2.56e-36
##  9 divisionMasters 1       1 -2.96e-36 43051.  2.56e-36
## 10 divisionMasters 2       1 -1.39e-35 43051.  2.56e-36
## # ... with 5,190 more rows
```

---

### Predicting


```r
results_test &lt;- mod_ridge %&gt;%
  predict(new_data = ipf_te, penalty = 0.001) %&gt;%
  mutate(
    truth = ipf_te$best3bench_kg,
    method = "Ridge"
  ) %&gt;%
  bind_rows(mod_lasso %&gt;%
    predict(new_data = ipf_te) %&gt;%
    mutate(
      truth = ipf_te$best3bench_kg,
      method = "Lasso"
  )) %&gt;%
  bind_rows(mod_rf %&gt;%
    predict(new_data = ipf_te) %&gt;%
    mutate(
      truth = ipf_te$best3bench_kg,
      method = "RF"
  ))

dim(results_test)
```

```
## [1] 38454     3
```

---

### Comparing Models

The package `yardstick` has tons of performance metrics:


```r
results_test %&gt;%
  group_by(method) %&gt;%
  yardstick::rmse(truth = truth, estimate = .pred)
```

```
## # A tibble: 3 x 4
##   method .metric .estimator .estimate
##   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 Lasso  rmse    standard        26.1
## 2 RF     rmse    standard        23.3
## 3 Ridge  rmse    standard        26.5
```

---


```r
results_test %&gt;%
  ggplot(aes(.pred, truth)) +
  geom_point(color = "red", alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0) +
  facet_wrap(~ method) +
  theme_bw()
```

&lt;img src="images/Tidymodels-Pred-vs-True-1.png" width="100%" /&gt;

---

### Tuning

This isn't completely clear to me, but it seems to work:

Define your model spec, using `tune()` from the `tune` package (needs to be installed separately) for a paramter you wish to tune:


```r
library(tune)

mod_rf_spec &lt;- rand_forest(mode = "regression",
                           mtry = tune("mtry"),
                           min_n = tune("min_n")) %&gt;%
  set_engine("ranger")
```

---

Define the `grid` on which you train your params, with the `dials` package:


```r
rf_grid &lt;- grid_regular(mtry(range(2, 10)), min_n(range(10, 30)),
                        levels = c(5, 3))

rf_grid
```

```
## # A tibble: 15 x 2
##     mtry min_n
##    &lt;int&gt; &lt;int&gt;
##  1     2    10
##  2     4    10
##  3     6    10
##  4     8    10
##  5    10    10
##  6     2    20
##  7     4    20
##  8     6    20
##  9     8    20
## 10    10    20
## 11     2    30
## 12     4    30
## 13     6    30
## 14     8    30
## 15    10    30
```

---

Split your data into a few folds for Cross Validation with `vfold_cv()` from the `rsample` package:


```r
cv_splits &lt;- vfold_cv(ipf_tr, v = 5)

cv_splits
```

```
## #  5-fold cross-validation 
## # A tibble: 5 x 2
##   splits               id   
##   &lt;named list&gt;         &lt;chr&gt;
## 1 &lt;split [15.4K/3.8K]&gt; Fold1
## 2 &lt;split [15.4K/3.8K]&gt; Fold2
## 3 &lt;split [15.4K/3.8K]&gt; Fold3
## 4 &lt;split [15.4K/3.8K]&gt; Fold4
## 5 &lt;split [15.4K/3.8K]&gt; Fold5
```

---

Now perform cross validation with `tune_grid()` from the `tune` package:


```r
tune_res &lt;- tune_grid(recipe(best3bench_kg ~ ., data = ipf_tr),
                      model = mod_rf_spec,
                      resamples = cv_splits,
                      grid = rf_grid,
                      metrics = metric_set(rmse))
tune_res
```


```
## #  5-fold cross-validation 
## # A tibble: 5 x 4
##   splits               id    .metrics          .notes          
## * &lt;list&gt;               &lt;chr&gt; &lt;list&gt;            &lt;list&gt;          
## 1 &lt;split [15.4K/3.8K]&gt; Fold1 &lt;tibble [15 x 5]&gt; &lt;tibble [0 x 1]&gt;
## 2 &lt;split [15.4K/3.8K]&gt; Fold2 &lt;tibble [15 x 5]&gt; &lt;tibble [0 x 1]&gt;
## 3 &lt;split [15.4K/3.8K]&gt; Fold3 &lt;tibble [15 x 5]&gt; &lt;tibble [0 x 1]&gt;
## 4 &lt;split [15.4K/3.8K]&gt; Fold4 &lt;tibble [15 x 5]&gt; &lt;tibble [0 x 1]&gt;
## 5 &lt;split [15.4K/3.8K]&gt; Fold5 &lt;tibble [15 x 5]&gt; &lt;tibble [0 x 1]&gt;
```

---

Collect the mean metric across folds:


```r
estimates &lt;- collect_metrics(tune_res)

estimates
```

```
## # A tibble: 15 x 7
##     mtry min_n .metric .estimator  mean     n std_err
##    &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
##  1     2    10 rmse    standard    24.7     5   0.197
##  2     2    20 rmse    standard    24.7     5   0.190
##  3     2    30 rmse    standard    24.8     5   0.192
##  4     4    10 rmse    standard    23.7     5   0.186
##  5     4    20 rmse    standard    23.6     5   0.192
##  6     4    30 rmse    standard    23.5     5   0.181
##  7     6    10 rmse    standard    23.9     5   0.191
##  8     6    20 rmse    standard    23.7     5   0.186
##  9     6    30 rmse    standard    23.6     5   0.186
## 10     8    10 rmse    standard    24.0     5   0.203
## 11     8    20 rmse    standard    23.7     5   0.185
## 12     8    30 rmse    standard    23.6     5   0.185
## 13    10    10 rmse    standard    24.1     5   0.206
## 14    10    20 rmse    standard    23.8     5   0.181
## 15    10    30 rmse    standard    23.7     5   0.183
```

---

Choose best paramter:


```r
estimates %&gt;%
  mutate(min_n = factor(min_n)) %&gt;%
  ggplot(aes(x = mtry, y = mean, color = min_n)) + 
  geom_point() + 
  geom_line() + 
  labs(y = "Mean RMSE") +
  theme_classic()
```

&lt;img src="images/Tidymodels-RMSE-Comp-1.png" width="100%" /&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="../libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
