---
title: "The Trees"
subtitle: "Applications of Data Science"
author: "Giora Simchoni"
institute: "Stat. and OR Department, TAU"
date: "`r Sys.Date()`"
output_dir: "images"
output:
  xaringan::moon_reader:
    css: "../slides.css"
    seal: false
    chakra: "../libs/remark-latest.min.js"
    includes:
      in_header: "../header.html"
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

class: logo-slide

---

class: title-slide

## The Trees

### Applications of Data Science - Class 11

### Giora Simchoni

#### `gsimchoni@gmail.com and add #dsapps in subject`

### Stat. and OR Department, TAU
### `r Sys.Date()`

---
```{r child = "../setup.Rmd"}
```

```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
```

class: section-slide

# The Pros and Cons of Trees

---

## Pros

- It's just a set of if-else statements my 7 y/o can get
- Highly interpretable (when tree is not large)
- Easy to implement
- Fast? (to predict)
- Handle all types of predictors (continuous, categorical)
- Handle missing data, *predict* missing data
- Assumption free
- Feature selection built-in (but when predictors are correlated...)
- Low bias, in general

---

## Cons

- High variance, in general
- Rectangular predictor regions - not always a good thing
- Complexity of prediction limited in no. of leaves! (For a simple CART)
- Not so fast? (to train)
- Greedy
- Selection Bias of predictors with more distinct values?
- Variable Importance when predictors are correlated

---

class: section-slide

# Detour: A Regression Problem

---

### OKCupid: Predicting Annual Income

It won't be easy:

```{r, message=FALSE}
okcupid <- read_csv("../data/okcupid.csv.zip")

okcupid %>% count(income, sort = TRUE) %>% head(20)
```

---

We will stick to non-NA (income) observations, and predict $\log_{10}(income/100000)$:

```{r, warning=FALSE}
okcupid2 <- okcupid %>%
  mutate(income = ifelse(income == -1, NA, log10(income/100000))) %>%
  drop_na(income)
```

```{r, echo=FALSE}
ethnicities_keep <- c("white", "asian", "hispanic / latin", "black", "indian", "pacific islander", "middle eastern", "native american")
not_perfect <- c("a little extra", "average", "curvy", "full figured", "overweight", "rather not say")
signs <- c("leo", "libra", "cancer", "virgo", "scorpio", "gemini", "taurus", "aries", "pisces", "aquarius", "sagittarius", "capricorn")
job_underpaid <- c("administrative", "media", "other", "rather not say", "student", "unemployed")
narrow_category <- function(category, sep = " ") {
  if (is.na(category)) return(NA_character_)
  split_diet <- str_split(category, sep)
  if (length(split_diet[[1]]) == 1) return(category)
  return(split_diet[[1]][2])
}
okcupid2 <- okcupid2 %>%
  mutate(height_cm = 2.54 * height,
         religion2 = case_when(
           str_detect(religion, "agnosticism") | str_detect(religion, "atheism") ~ "atheist",
           str_detect(religion, "buddhism") ~ "buddhist",
           str_detect(religion, "christianity") | str_detect(religion, "catholicism") ~ "christian",
           str_detect(religion, "judaism") ~ "jewish",
           str_detect(religion, "hinduism") ~ "hindu",
           str_detect(religion, "islam") ~ "muslim",
           TRUE ~ NA_character_),
         education_kind = case_when(
           str_detect(education, "^dropped") ~ "dropped",
           str_detect(education, "^graduated") ~ "graduated",
           str_detect(education, "^working") ~ "working",
           TRUE ~ "other"),
         ethnicity2 = ifelse(ethnicity %in% ethnicities_keep, ethnicity, "other"),
         part_black = str_detect(ethnicity, "black"),
         part_white = str_detect(ethnicity, "white"),
         part_asian = str_detect(ethnicity, "asian"),
         part_hispanic = str_detect(ethnicity, "hispanic"),
         body_type_not_perfect = body_type %in% not_perfect,
         diet2 = map_chr(diet, narrow_category),
         location_sf = location == "san francisco, california",
         pets_has_dogs = str_detect(pets, "has dogs"),
         pets_has_cats = str_detect(pets, "has cats"),
         pets_likes_dogs = str_detect(pets, "likes dogs"),
         pets_likes_cats = str_detect(pets, "likes cats"),
         sign_fun = str_detect(sign, "fun"),
         sign_not_matter = str_detect(sign, "but it does"),
         sign_matters = str_detect(sign, "matters"),
         sign2 = str_extract(sign, str_c(signs, collapse = "|")),
         speaks_spanish = str_detect(speaks, "spanish"),
         speaks_french = str_detect(speaks, "french"),
         speaks_german = str_detect(speaks, "german"),
         speaks_chinese = str_detect(speaks, "chinese"),
         education2 = case_when(
           education == "graduated from high school" ~ "high_school",
           education == "graduated from two-year college" ~ "college",
           education == "graduated from college/university" ~ "degree1",
           education == "graduated from masters program" ~ "degree2",
           education == "graduated from ph.d program" ~ "degree3",
           education == "working on two-year college" ~ "student0",
           education == "working on college/university" ~ "student1",
           education == "working on masters program" ~ "student2",
           education == "working on ph.d program" ~ "student3",
           is.na(education) ~ NA_character_,
           TRUE ~ "other"
         ),
         education_academic = education2 %in% c("degree1", "degree2", "degree3", "student2", "student3"),
         status = ifelse(status == "unknown", NA_character_, status),
         job2 = map_chr(job, narrow_category, sep = " / "),
         job3 = case_when(
           is.na(job2) ~ "other",
           TRUE ~ job2
         ),
         job_underpaid = job2 %in% job_underpaid) %>%
  mutate_at(vars(essay0:essay9), list("len" = ~log(str_length(.x) + 0.016))) %>%
  mutate_if(is.character, as.factor) %>%
  mutate_if(is.logical, as.factor)

predictors <- c("age", "height_cm", "sex", "body_type", "body_type_not_perfect",
                "diet2", "drinks", "drugs", "religion2",
                "education2" ,"education_kind", "education_academic",
                "ethnicity2", "part_black", "part_white",
                "part_asian", "part_hispanic", "job3",
                "orientation", "pets_has_dogs", "pets_has_cats", "pets_likes_cats",
                "pets_likes_dogs", "sign_fun", "sign_not_matter", "sign_matters",
                "sign2", "speaks_spanish", "speaks_french", "speaks_german",
                "speaks_chinese", "status",
                "essay0_len", "essay1_len", "essay2_len", "essay3_len", "essay4_len",
                "essay5_len", "essay6_len", "essay7_len", "essay8_len", "essay9_len")
```

In the vector `predictors` .font80percent[(see slides Rmd source)] we have `r length(predictors)` continuous and categorical variables which may or may not be predictive to income:

```{r}
okcupid2 <- okcupid2 %>%
  select(income, predictors) %>%
  mutate(id = 1:n())

dim(okcupid2)
```

---

```{r}
glimpse(okcupid2)
```

---

We split the data into training, validation and test sets:

```{r, message=FALSE}
# test_idx <- sample(1:nrow(okcupid2), 2000, replace = FALSE)
# train_idx <- okcupid2 %>% filter(!id %in% test_idx) %>% sample_n(8000) %>% pull(id)
# valid_idx <- okcupid2 %>% filter(!id %in% test_idx, !id %in% train_idx) %>% pull(id)
okcupid2 <- okcupid2 %>% select(-id)

idx <- read_rds("../data/okcupid_idx.rda")
train_idx <- idx$train_idx
valid_idx <- idx$valid_idx
test_idx <- idx$test_idx

okcupid2_train <- okcupid2[train_idx, ]
okcupid2_valid <- okcupid2[valid_idx, ]
okcupid2_test <- okcupid2[test_idx, ]

library(glue)
glue("train no. of rows: {nrow(okcupid2_train)}
     validation no. of rows: {nrow(okcupid2_valid)}
     test no. of rows: {nrow(okcupid2_test)}")
```

---

Our transformed income dependent variable behaves "ok":

```{r Income-Hist, message=FALSE, out.width="100%", fig.asp=0.5}
ggplot(okcupid2_train, aes(income)) +
  geom_histogram(fill = "red", alpha = 0.5) +
  theme_classic()
```

---

We can quickly see percentage of missing values with [`naniar`](http://naniar.njtierney.com/):

```{r Missingness, warning=FALSE, out.width="100%", fig.asp=0.5}
library(naniar)

vis_miss(okcupid2_train %>%
           sample_frac(0.2) %>%
           select(-starts_with("essay")))
```

---

Also worth exploring some basic relations between predictors and income. You can use the work of others, e.g. [`ggpairs`](https://ggobi.github.io/ggally/):

```{r GGPairs, warning=FALSE, message=FALSE, out.width="100%", fig.asp=0.5}
library(GGally)

ggpairs(okcupid2_train %>%
          select(income, age, sex, height_cm, body_type_not_perfect))
```

---

But don't be ashamed of simply exploring on your own:

```{r}
cat_vs_income_boxplot <- function(cat) {
  ggplot(okcupid2_train, aes({{cat}}, income)) +
  geom_boxplot() +
  facet_wrap(~ sex) +
  theme_classic() +
  labs(x = "") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
}
```

---

```{r Body_Type-Income, out.width="100%", fig.asp=0.5}
cat_vs_income_boxplot(body_type)
```

---

```{r Sign-Income, out.width="100%", fig.asp=0.5}
cat_vs_income_boxplot(sign2)
```

---

```{r Job-Income, out.width="100%", fig.asp=0.5}
cat_vs_income_boxplot(job3)
```

---

```{r Edu-Income, out.width="100%", fig.asp=0.5}
cat_vs_income_boxplot(education2)
```

---

```{r Religion-Income, out.width="100%", fig.asp=0.5}
cat_vs_income_boxplot(religion2)
```

---

```{r Diet-Income, out.width="100%", fig.asp=0.5}
cat_vs_income_boxplot(diet2)
```

---

```{r Essay-Income, warning=FALSE, message=FALSE, out.width="100%", fig.asp=0.5}
ggpairs(okcupid2_train %>%
          select(essay0_len:essay2_len, income))
```

---

### Baseline: Linear Regression

R's `lm` function does not take `NA` values.

One strategy is to impute these values using a "common" value such as the median for continuous variables and mode for categorical variables. This can easily be achieved with `naniar`:

```{r}
okcupid2_imp <- naniar::impute_median_all(okcupid2)
okcupid2_imp_train <- okcupid2_imp[train_idx, ]
okcupid2_imp_valid <- okcupid2_imp[valid_idx, ]

mod_lm <- lm(income ~ ., data = okcupid2_imp_train)
pred_lm <- predict(mod_lm, okcupid2_imp_valid)

rmse <- function(obs, pred) sqrt(mean((obs - pred)^2))

report_rmse_and_cor <- function(obs, pred) {
  RMSE <- rmse(obs, pred)
  CORR <- cor(obs, pred)
  glue("RMSE: {format(RMSE, digits = 3)}
       CORR: {format(CORR, digits = 3)}")
}

```

---

```{r}
report_rmse_and_cor(okcupid2_valid$income, pred_lm)
```

```{r LM-Fit1, out.width="50%", warning=FALSE}
tibble(income = okcupid2_valid$income, pred = pred_lm) %>%
  ggplot(aes(income, pred)) + geom_point() + ylim(range(okcupid2_valid$income))
```

---

A more intelligent strategy for imputing missing values would be to *predict* them using whatever data is not missing. This can be done quite seamlessly with the [`mice`](https://stefvanbuuren.name/mice/) package:

```{r, message=FALSE}
# library(mice)
# mice_obj <- mice(okcupid2, m = 1, maxit = 10, seed = 42)
# okcupid2_imp_mice <- complete(mice_obj)

okcupid2_imp_mice <- read_rds("../data/okcupid_imp_mice.rds")
okcupid2_imp_mice_train <- okcupid2_imp_mice[train_idx, ]
okcupid2_imp_mice_valid <- okcupid2_imp_mice[valid_idx, ]

mod_lm_mice <- lm(income ~ ., data = okcupid2_imp_mice_train)
pred_lm_mice <- predict(mod_lm_mice, okcupid2_imp_mice_valid)

report_rmse_and_cor(okcupid2_valid$income, pred_lm_mice)
```

.insight[
`r emo::ji("bulb")` Can you think of other imputation strategies?
]

---

```{r LM-Fit2, out.width="60%", warning=FALSE}
tibble(income = okcupid2_valid$income, pred = pred_lm_mice) %>%
  ggplot(aes(income, pred)) +
  geom_point() +
  ylim(range(okcupid2_valid$income))
```

---

### Baseline: Ridge Regression

```{r, message=FALSE}
library(glmnet)

okcupid2_imp_mat_train <- model.matrix( ~ ., okcupid2_imp_mice_train[, predictors])
okcupid2_imp_mat_valid <- model.matrix( ~ ., okcupid2_imp_mice_valid[, predictors])

ridge_cv <- cv.glmnet(x = okcupid2_imp_mat_train,
                      y = okcupid2_train$income, alpha = 0)

best_lambda <- ridge_cv$lambda.min

mod_lm_ridge <- glmnet(x = okcupid2_imp_mat_train,
                      y = okcupid2_train$income, alpha = 0,
                      lambda = best_lambda)

pred_lm_ridge <- predict(mod_lm_ridge, okcupid2_imp_mat_valid)

report_rmse_and_cor(okcupid2_valid$income, pred_lm_ridge)
```

---

```{r LM-Ridge-Fit, out.width="60%", warning=FALSE}
tibble(income = okcupid2_valid$income, pred = pred_lm_ridge) %>%
  ggplot(aes(income, pred)) +
  geom_point() +
  ylim(range(okcupid2_valid$income))
```

---

### Baseline: Lasso Regression

```{r, message=FALSE}
lasso_cv <- cv.glmnet(x = okcupid2_imp_mat_train,
                      y = okcupid2_imp_train$income, alpha = 1)

best_lambda <- lasso_cv$lambda.min

mod_lm_lasso <- glmnet(x = okcupid2_imp_mat_train,
                      y = okcupid2_train$income, alpha = 1,
                      lambda = best_lambda)

pred_lm_lasso <- predict(mod_lm_lasso, okcupid2_imp_mat_valid)

report_rmse_and_cor(okcupid2_valid$income, pred_lm_lasso)
```

---

```{r LM-Lasso-Fit, out.width="60%", warning=FALSE}
tibble(income = okcupid2_valid$income, pred = pred_lm_lasso) %>%
  ggplot(aes(income, pred)) +
  geom_point() +
  ylim(range(okcupid2_valid$income))
```

---

class: section-slide

# End of Detour

---

class: section-slide

# The CART (Regression)

---

### The OG CART

1. Find the the predictor to (binary) split on and the value of the split, by $SSE$ criterion
2. For each resulting node if $n_{node} > 20$ go to 1
3. Once full tree has been grown, perform pruning using the *cost-complexity parameter* $c_p$ and the $SSE_{c_p}$ criterion
4. Predict the average value at each terminal node

- For each split *surrogate splits* are saved for future `NA`s
- An alternative criterion for complexity could be tree maximum depth (sklearn)
- One can also reduce all tree's unique paths to a set of rules
- Variables can be ranked by "importance"

.insight[
`r emo::ji("bulb")` What if the best split isn't binary?
]

---

### The $SSE$ criterion - continuous predictor

- $y$ is the continuous dependent variable
- a continuous predictor $v$ is nominated for splitting the current node
- with splitting value $l$
- such that $S_1$ is the set of observations for which $v_i \le l$
- and $S_2$ is the set of observations for which $v_i > l$
- $\overline y_1$ is the average of $y$ in set $S_1$

$SSE = \sum_{i \in S_1}(y_i - \overline y_1)^2 + \sum_{i \in S_2}(y_i - \overline y_2)^2$

For example if `age` is candidate in splitting `income`:

---

```{r Age-SSE0, fig.show="hide"}
library(patchwork)

sse <- function(l, df, v) {
  income_above <- df %>% filter({{v}} >= l) %>% pull(income)
  income_below <- df %>% filter({{v}} < l) %>% pull(income)
  sse_above <- sum((income_above - mean(income_above))^2)
  sse_below <- sum((income_below - mean(income_below))^2)
  return(sse_above + sse_below)
}
age <- seq(18,69, 1)
sse_age <- map_dbl(age, sse, df = okcupid2_train, v = age)

p1 <- okcupid2_train %>%
  count(age, income) %>%
  ggplot(aes(age, income)) +
  geom_point(aes(size = n)) +
  theme_classic() +
  labs(x = "") +
  theme(axis.text.x = element_blank())

p2 <- tibble(age = age, sse = sse_age) %>%
  ggplot(aes(age, sse)) +
  geom_line() +
  geom_point() +
  theme_classic()

p1 / p2
```

---

```{r Age-SSE, ref.label = "Age-SSE0", echo = FALSE, out.width = "80%"}

```

---

### The $SSE$ criterion - categorical predictor

.insight[
`r emo::ji("bulb")` What could be an issue with a categorical variable with many levels?
]

- a categorical predictor $l$ is nominated for splitting the current node
  - option 1: use dummy variables, turning each level into a 2-level 0/1 category variable
  - option 2: order levels by some criterion like $\overline{y_j}$ and treat $l$ as continuous from here on

For example if `job3` is candidate in splitting `income`:
---

```{r Job-SSE0, fig.show="hide", message=FALSE}
mean_income_vs_job <- okcupid2_train %>%
  group_by(job3) %>%
  summarise(mean_income = mean(income)) %>%
  arrange(mean_income)
jobs_levels_sorted <- as.character(mean_income_vs_job$job3)
okcupid2_train_job_sorted <- okcupid2_train %>%
  mutate(job_ordered = fct_relevel(job3, jobs_levels_sorted),
         job_ordered_n = as.numeric(job_ordered))

job_rank <- seq(1, length(jobs_levels_sorted), 1)
sse_job <- map_dbl(job_rank, sse, df = okcupid2_train_job_sorted, v = job_ordered_n)

p1 <- okcupid2_train_job_sorted %>%
  ggplot(aes(job_ordered, income)) +
  geom_boxplot() +
  theme_classic() + labs(x = "") +
  theme(axis.text.x = element_blank())

p2 <- tibble(job = factor(jobs_levels_sorted, levels = jobs_levels_sorted), sse = sse_job) %>%
  ggplot(aes(job, sse, group = 1)) +
  geom_line() +
  geom_point() +
  theme_classic() +
  labs(x = "") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))

p1 / p2
```

---

```{r Job-SSE, ref.label = "Job-SSE0", echo = FALSE, out.width = "80%"}

```

---

### Pruning: The $SSE_{c_p}$ criterion

The "right-sized" tree should not be too deep to avoid overfitting.

Once the full tree is grown, we check for each split:

$SSE_{c_p} = SSE_{tot} + c_p \cdot {\text{#terminal nodes}}$

Where $c_p$ is a penalty or regularization parameter usually chosen by cross validation.

And we choose the smallest pruned tree with the minimum $SSE_{c_p}$.

---

### CART with `rpart`

Let's tune $c_p$ for our data using a 5-fold (manual) Cross Validation. The criterion to maximize would be RMSE.

```{r Cp-CV0}
library(rpart)

n_cv <- 5; cp_seq <- seq(0, 0.02, 0.001)

okcupid2_train_val <- okcupid2_train %>%
  mutate(val = sample(1:n_cv, n(), replace = TRUE))

get_validation_set_rmse <- function(i, .cp) {
  ok_tr <- okcupid2_train_val %>% filter(val != i) %>% select(-val)
  ok_val <- okcupid2_train_val %>% filter(val == i) %>% select(-val)
  mod <- rpart(income ~ ., data = ok_tr,
               control = rpart.control(cp = .cp))
  pred <- predict(mod, ok_val)
  rmse(ok_val$income, pred)
}
get_cv_rmse <- function(.cp) {
  tibble(cp = rep(.cp, n_cv),
         rmse = map_dbl(1:n_cv, get_validation_set_rmse, .cp = .cp))
}
```

---

```{r Cp-CV, out.width = "100%", fig.asp=0.5}
# cv_table <- map_dfr(cp_seq, get_cv_rmse)
cv_table <- read_rds("../data/okcupid_CART_Reg_Cp_CV.rds")

cv_table %>%
  mutate(cp = factor(cp)) %>%
  ggplot(aes(cp, rmse)) +
  geom_boxplot() +
  theme_classic() +
  labs(x = "") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```

---

Training on the entire training set:

```{r}
mod_tree_na <- rpart(income ~ ., data = okcupid2_train,
                     control = rpart.control(cp = 0.004))
```

You can plot the tree using `rpart`, but...

```{r CART-NA1, out.width="50%"}
plot(mod_tree_na)
text(mod_tree_na, pretty = 1, use.n = TRUE)
```

---

The plotting function in the `rpart.plot` package is slightly nicer:

```{r CART-NA2, out.width="100%", message=FALSE, warning=FALSE, fig.asp=0.7}
library(rpart.plot)
prp(mod_tree_na, type = 5, extra = 1)
```

---

You can go fancy and use `partykit` and/or `ggparty`:

```{r CART-NA3a, out.width="80%", message=FALSE, warning=FALSE, fig.show="hide"}
library(ggparty)

party_obj <- as.party(mod_tree_na)

ggparty(party_obj) +
  geom_edge() +
  # geom_edge_label() +
  geom_node_label(aes(label = splitvar), ids = "inner") +
  geom_node_label(aes(label = str_c("n = ", nodesize)),
                  ids = "terminal", nudge_y = 0.02) +
  geom_node_plot(gglist = list(geom_boxplot(aes(y = income)),
                                 theme(axis.text.x=element_blank(),
                                       axis.ticks.x=element_blank())),
                 shared_axis_labels=TRUE)
```

---

```{r CART-NA3, ref.label = "CART-NA3a", echo = FALSE, out.width = "100%", fig.asp=0.8}
```

---

Or you can just print the tree and see what is going on:

```{r, eval=FALSE}
print(party_obj)
```

<img src = "images/CART_print.png" style="width: 100%">

---

### Variables Importance

Summing the reduction in $SSE$ for each split variable, we can get a measure of importance.

.font80percent[Unfortunately in `rpart` the "potential" reduction in $SSE$ for surrogate splits is also summed. You can either ignore or retrain with only the variables chosen by the model.]

```{r, CART-VarImp0, fig.show="hide"}
enframe(mod_tree_na$variable.importance) %>%
  arrange(value) %>%
  mutate(variable = as_factor(name)) %>%
  ggplot(aes(variable, value)) +
  geom_segment(aes(x = variable, xend = variable,
                   y = 0, yend = value)) +
  geom_point(size = 3) +
  theme_classic() +
  coord_flip() +
  labs(x = "", y = "")
```

---

```{r CART-VarImp, ref.label = "CART-VarImp0", echo = FALSE, out.width = "60%"}

```

.insight[
`r emo::ji("bulb")` How would this profile look for a couple of very correlated predictors?
]

---

### Prediction

```{r}
pred_tree_na <- predict(mod_tree_na, okcupid2_valid)

report_rmse_and_cor(okcupid2_valid$income, pred_tree_na)
```

As expected, far from impressive. Let's try using the imputed `NA` data:

```{r}
mod_tree_imp <- rpart(income ~ ., data = okcupid2_imp_mice_train,
                      control = rpart.control(cp = 0.004))
pred_tree_imp <- predict(mod_tree_imp, okcupid2_imp_mice_valid)

report_rmse_and_cor(okcupid2_valid$income, pred_tree_imp)
```

---

```{r Tree-Fit, out.width="60%", warning=FALSE}
tibble(income = okcupid2_valid$income, pred = pred_tree_na) %>%
  count(income, pred) %>%
  ggplot(aes(income, pred)) +
  geom_point(aes(size = n)) +
  ylim(range(okcupid2_valid$income))
```

---

class: section-slide

# Detour: A Classification Problem

---

class: section-slide

# End of Detour

---

class: section-slide

# The CART (Classification)

---

class: section-slide

# Bagged Trees

---

class: section-slide

# Random Forests

---

class: section-slide

# Boosted Trees

---

class: section-slide

# The Others

---

