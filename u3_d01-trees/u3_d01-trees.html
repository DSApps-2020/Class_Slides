<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>The Trees</title>
    <meta charset="utf-8" />
    <meta name="author" content="Giora Simchoni" />
    <meta name="date" content="2020-02-05" />
    <head>
      <link rel="icon" href="../DSApps_logo.jpg" type="image/jpg"> 
      <link rel="shortcut icon" href="../DSApps_logo.jpg" type="image/jpg">
    </head>
    <link rel="stylesheet" href="..\slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">


class: logo-slide

---

class: title-slide

## The Trees

### Applications of Data Science - Class 11

### Giora Simchoni

#### `gsimchoni@gmail.com and add #dsapps in subject`

### Stat. and OR Department, TAU
### 2020-02-05

---



layout: true

&lt;div class="my-footer"&gt;
  &lt;span&gt;
    &lt;a href="https://dsapps-2020.github.io/Class_Slides/" target="_blank"&gt;Applications of Data Science
    &lt;/a&gt;
  &lt;/span&gt;
&lt;/div&gt;

---



class: section-slide

# The Pros and Cons of Trees

---

## Pros

- It's just a set of if-else statements my 7 y/o can get
- Highly interpretable (when tree is not large)
- Easy to implement
- Fast? (to predict)
- Handle all types of predictors (continuous, categorical)
- Handle missing data, *predict* missing data
- Assumption free
- Feature selection built-in (but when predictors are correlated...)
- Low bias, in general

---

## Cons

- High variance, in general
- Rectangular predictor regions - not always a good thing
- Complexity of prediction limited in no. of leaves! (For a simple CART)
- Not so fast? (to train)
- Greedy
- Selection Bias of predictors with more distinct values?
- Variable Importance when predictors are correlated

---

class: section-slide

# Detour: A Regression Problem

---

### OKCupid: Predicting Annual Income

It won't be easy:


```r
okcupid &lt;- read_csv("../data/okcupid.csv.zip")

okcupid %&gt;% count(income, sort = TRUE) %&gt;% head(20)
```

```
## # A tibble: 13 x 2
##     income     n
##      &lt;dbl&gt; &lt;int&gt;
##  1      -1 48442
##  2   20000  2952
##  3  100000  1621
##  4   80000  1111
##  5   30000  1048
##  6   40000  1005
##  7   50000   975
##  8   60000   736
##  9   70000   707
## 10  150000   631
## 11 1000000   521
## 12  250000   149
## 13  500000    48
```

---

We will stick to non-NA (income) observations, and predict `\(\log_{10}(income/100000)\)`:


```r
okcupid2 &lt;- okcupid %&gt;%
  mutate(income = ifelse(income == -1, NA, log10(income/100000))) %&gt;%
  drop_na(income)
```



In the vector `predictors` .font80percent[(see slides Rmd source)] we have 42 continuous and categorical variables which may or may not be predictive to income:


```r
okcupid2 &lt;- okcupid2 %&gt;%
  select(income, predictors) %&gt;%
  mutate(id = 1:n())

dim(okcupid2)
```

```
## [1] 11504    44
```

---


```r
glimpse(okcupid2)
```

```
## Observations: 11,504
## Variables: 44
## $ income                &lt;dbl&gt; -0.09691001, -0.69897000, -0.39794001, -...
## $ age                   &lt;dbl&gt; 35, 23, 28, 30, 29, 40, 31, 22, 35, 31, ...
## $ height_cm             &lt;dbl&gt; 177.80, 180.34, 182.88, 167.64, 157.48, ...
## $ sex                   &lt;fct&gt; m, m, m, f, f, m, f, m, m, f, m, m, f, m...
## $ body_type             &lt;fct&gt; average, thin, average, skinny, thin, fi...
## $ body_type_not_perfect &lt;fct&gt; TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, ...
## $ diet2                 &lt;fct&gt; other, vegetarian, anything, anything, a...
## $ drinks                &lt;fct&gt; often, socially, socially, socially, soc...
## $ drugs                 &lt;fct&gt; sometimes, NA, never, never, never, NA, ...
## $ religion2             &lt;fct&gt; atheist, NA, christian, christian, chris...
## $ education2            &lt;fct&gt; other, student1, degree1, high_school, s...
## $ education_kind        &lt;fct&gt; working, working, graduated, graduated, ...
## $ education_academic    &lt;fct&gt; FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, ...
## $ ethnicity2            &lt;fct&gt; white, white, white, white, other, white...
## $ part_black            &lt;fct&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE...
## $ part_white            &lt;fct&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, NA, ...
## $ part_asian            &lt;fct&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE...
## $ part_hispanic         &lt;fct&gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,...
## $ job3                  &lt;fct&gt; travel, student, financial, marketing, o...
## $ orientation           &lt;fct&gt; straight, straight, straight, straight, ...
## $ pets_has_dogs         &lt;fct&gt; FALSE, FALSE, FALSE, TRUE, FALSE, FALSE,...
## $ pets_has_cats         &lt;fct&gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,...
## $ pets_likes_cats       &lt;fct&gt; TRUE, TRUE, TRUE, TRUE, FALSE, FALSE, NA...
## $ pets_likes_dogs       &lt;fct&gt; TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, N...
## $ sign_fun              &lt;fct&gt; FALSE, FALSE, FALSE, NA, FALSE, TRUE, NA...
## $ sign_not_matter       &lt;fct&gt; FALSE, FALSE, TRUE, NA, FALSE, FALSE, NA...
## $ sign_matters          &lt;fct&gt; FALSE, FALSE, FALSE, NA, FALSE, FALSE, N...
## $ sign2                 &lt;fct&gt; cancer, pisces, leo, NA, taurus, gemini,...
## $ speaks_spanish        &lt;fct&gt; TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, ...
## $ speaks_french         &lt;fct&gt; TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, ...
## $ speaks_german         &lt;fct&gt; FALSE, TRUE, FALSE, FALSE, FALSE, FALSE,...
## $ speaks_chinese        &lt;fct&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE...
## $ status                &lt;fct&gt; single, single, seeing someone, single, ...
## $ essay0_len            &lt;dbl&gt; 6.538163, 3.713962, 7.336296, NA, NA, 6....
## $ essay1_len            &lt;dbl&gt; 3.951551, 3.713962, 6.095861, NA, 5.8435...
## $ essay2_len            &lt;dbl&gt; 4.564515, 4.605330, 6.109283, NA, 6.4281...
## $ essay3_len            &lt;dbl&gt; NA, 3.496992, 5.733393, NA, 4.204931, 4....
## $ essay4_len            &lt;dbl&gt; 5.517517, 5.327954, 6.278551, NA, 6.9716...
## $ essay5_len            &lt;dbl&gt; 5.723637, NA, 6.424895, NA, 4.812314, 5....
## $ essay6_len            &lt;dbl&gt; NA, 3.258712, 5.459654, NA, 4.709674, 5....
## $ essay7_len            &lt;dbl&gt; NA, NA, 4.709674, NA, 4.875319, 5.459654...
## $ essay8_len            &lt;dbl&gt; 3.9123430, NA, 4.5645148, NA, 4.5434650,...
## $ essay9_len            &lt;dbl&gt; NA, 3.045284, 5.669936, NA, 3.784553, 8....
## $ id                    &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1...
```

---

We split the data into training, validation and test sets:


```r
# test_idx &lt;- sample(1:nrow(okcupid2), 2000, replace = FALSE)
# train_idx &lt;- okcupid2 %&gt;% filter(!id %in% test_idx) %&gt;% sample_n(8000) %&gt;% pull(id)
# valid_idx &lt;- okcupid2 %&gt;% filter(!id %in% test_idx, !id %in% train_idx) %&gt;% pull(id)
okcupid2 &lt;- okcupid2 %&gt;% select(-id)

idx &lt;- read_rds("../data/okcupid_idx.rda")
train_idx &lt;- idx$train_idx
valid_idx &lt;- idx$valid_idx
test_idx &lt;- idx$test_idx

okcupid2_train &lt;- okcupid2[train_idx, ]
okcupid2_valid &lt;- okcupid2[valid_idx, ]
okcupid2_test &lt;- okcupid2[test_idx, ]

library(glue)
glue("train no. of rows: {nrow(okcupid2_train)}
     validation no. of rows: {nrow(okcupid2_valid)}
     test no. of rows: {nrow(okcupid2_test)}")
```

```
## train no. of rows: 8000
## validation no. of rows: 1504
## test no. of rows: 2000
```

---

Our transformed income dependent variable behaves "ok":


```r
ggplot(okcupid2_train, aes(income)) +
  geom_histogram(fill = "red", alpha = 0.5) +
  theme_classic()
```

&lt;img src="images/Income-Hist-1.png" width="100%" /&gt;

---

We can quickly see percentage of missing values with [`naniar`](http://naniar.njtierney.com/):


```r
library(naniar)

vis_miss(okcupid2_train %&gt;%
           sample_frac(0.2) %&gt;%
           select(-starts_with("essay")))
```

&lt;img src="images/Missingness-1.png" width="100%" /&gt;

---

Also worth exploring some basic relations between predictors and income. You can use the work of others, e.g. [`ggpairs`](https://ggobi.github.io/ggally/):


```r
library(GGally)

ggpairs(okcupid2_train %&gt;%
          select(income, age, sex, height_cm, body_type_not_perfect))
```

&lt;img src="images/GGPairs-1.png" width="100%" /&gt;

---

But don't be ashamed of simply exploring on your own:


```r
cat_vs_income_boxplot &lt;- function(cat) {
  ggplot(okcupid2_train, aes({{cat}}, income)) +
  geom_boxplot() +
  facet_wrap(~ sex) +
  theme_classic() +
  labs(x = "") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
}
```

---


```r
cat_vs_income_boxplot(body_type)
```

&lt;img src="images/Body_Type-Income-1.png" width="100%" /&gt;

---


```r
cat_vs_income_boxplot(sign2)
```

&lt;img src="images/Sign-Income-1.png" width="100%" /&gt;

---


```r
cat_vs_income_boxplot(job3)
```

&lt;img src="images/Job-Income-1.png" width="100%" /&gt;

---


```r
cat_vs_income_boxplot(education2)
```

&lt;img src="images/Edu-Income-1.png" width="100%" /&gt;

---


```r
cat_vs_income_boxplot(religion2)
```

&lt;img src="images/Religion-Income-1.png" width="100%" /&gt;

---


```r
cat_vs_income_boxplot(diet2)
```

&lt;img src="images/Diet-Income-1.png" width="100%" /&gt;

---


```r
ggpairs(okcupid2_train %&gt;%
          select(essay0_len:essay2_len, income))
```

&lt;img src="images/Essay-Income-1.png" width="100%" /&gt;

---

### Baseline: Linear Regression

R's `lm` function does not take `NA` values.

One strategy is to impute these values using a "common" value such as the median for continuous variables and mode for categorical variables. This can easily be achieved with `naniar`:


```r
okcupid2_imp &lt;- naniar::impute_median_all(okcupid2)
okcupid2_imp_train &lt;- okcupid2_imp[train_idx, ]
okcupid2_imp_valid &lt;- okcupid2_imp[valid_idx, ]

mod_lm &lt;- lm(income ~ ., data = okcupid2_imp_train)
pred_lm &lt;- predict(mod_lm, okcupid2_imp_valid)

rmse &lt;- function(obs, pred) sqrt(mean((obs - pred)^2))

report_rmse_and_cor &lt;- function(obs, pred) {
  RMSE &lt;- rmse(obs, pred)
  CORR &lt;- cor(obs, pred)
  glue("RMSE: {format(RMSE, digits = 3)}
       CORR: {format(CORR, digits = 3)}")
}
```

---


```r
report_rmse_and_cor(okcupid2_valid$income, pred_lm)
```

```
## RMSE: 0.352
## CORR: 0.501
```


```r
tibble(income = okcupid2_valid$income, pred = pred_lm) %&gt;%
  ggplot(aes(income, pred)) + geom_point() + ylim(range(okcupid2_valid$income))
```

&lt;img src="images/LM-Fit1-1.png" width="50%" /&gt;

---

A more intelligent strategy for imputing missing values would be to *predict* them using whatever data is not missing. This can be done quite seamlessly with the [`mice`](https://stefvanbuuren.name/mice/) package:


```r
# library(mice)
# mice_obj &lt;- mice(okcupid2, m = 1, maxit = 10, seed = 42)
# okcupid2_imp_mice &lt;- complete(mice_obj)

okcupid2_imp_mice &lt;- read_rds("../data/okcupid_imp_mice.rds")
okcupid2_imp_mice_train &lt;- okcupid2_imp_mice[train_idx, ]
okcupid2_imp_mice_valid &lt;- okcupid2_imp_mice[valid_idx, ]

mod_lm_mice &lt;- lm(income ~ ., data = okcupid2_imp_mice_train)
pred_lm_mice &lt;- predict(mod_lm_mice, okcupid2_imp_mice_valid)

report_rmse_and_cor(okcupid2_valid$income, pred_lm_mice)
```

```
## RMSE: 0.351
## CORR: 0.504
```

.insight[
💡 Can you think of other imputation strategies?
]

---


```r
tibble(income = okcupid2_valid$income, pred = pred_lm_mice) %&gt;%
  ggplot(aes(income, pred)) +
  geom_point() +
  ylim(range(okcupid2_valid$income))
```

&lt;img src="images/LM-Fit2-1.png" width="60%" /&gt;

---

### Baseline: Ridge Regression


```r
library(glmnet)

okcupid2_imp_mat_train &lt;- model.matrix( ~ ., okcupid2_imp_mice_train[, predictors])
okcupid2_imp_mat_valid &lt;- model.matrix( ~ ., okcupid2_imp_mice_valid[, predictors])

ridge_cv &lt;- cv.glmnet(x = okcupid2_imp_mat_train,
                      y = okcupid2_train$income, alpha = 0)

best_lambda &lt;- ridge_cv$lambda.min

mod_lm_ridge &lt;- glmnet(x = okcupid2_imp_mat_train,
                      y = okcupid2_train$income, alpha = 0,
                      lambda = best_lambda)

pred_lm_ridge &lt;- predict(mod_lm_ridge, okcupid2_imp_mat_valid)

report_rmse_and_cor(okcupid2_valid$income, pred_lm_ridge)
```

```
## RMSE: 0.351
## CORR: 0.505
```

---


```r
tibble(income = okcupid2_valid$income, pred = pred_lm_ridge) %&gt;%
  ggplot(aes(income, pred)) +
  geom_point() +
  ylim(range(okcupid2_valid$income))
```

&lt;img src="images/LM-Ridge-Fit-1.png" width="60%" /&gt;

---

### Baseline: Lasso Regression


```r
lasso_cv &lt;- cv.glmnet(x = okcupid2_imp_mat_train,
                      y = okcupid2_imp_train$income, alpha = 1)

best_lambda &lt;- lasso_cv$lambda.min

mod_lm_lasso &lt;- glmnet(x = okcupid2_imp_mat_train,
                      y = okcupid2_train$income, alpha = 1,
                      lambda = best_lambda)

pred_lm_lasso &lt;- predict(mod_lm_lasso, okcupid2_imp_mat_valid)

report_rmse_and_cor(okcupid2_valid$income, pred_lm_lasso)
```

```
## RMSE: 0.351
## CORR: 0.506
```

---


```r
tibble(income = okcupid2_valid$income, pred = pred_lm_lasso) %&gt;%
  ggplot(aes(income, pred)) +
  geom_point() +
  ylim(range(okcupid2_valid$income))
```

&lt;img src="images/LM-Lasso-Fit-1.png" width="60%" /&gt;

---

class: section-slide

# End of Detour

---

class: section-slide

# The CART (Regression)

---

### The OG CART

1. Find the the predictor to (binary) split on and the value of the split, by `\(SSE\)` criterion
2. For each resulting node if `\(n_{node} &gt; 20\)` go to 1
3. Once full tree has been grown, perform pruning using the *cost-complexity parameter* `\(c_p\)` and the `\(SSE_{c_p}\)` criterion
4. Predict the average value at each terminal node

- For each split *surrogate splits* are saved for future `NA`s
- An alternative criterion for complexity could be tree maximum depth (sklearn)
- One can also reduce all tree's unique paths to a set of rules
- Variables can be ranked by "importance"

.insight[
💡 What if the best split isn't binary?
]

---

### The `\(SSE\)` criterion - continuous predictor

- `\(y\)` is the continuous dependent variable
- a continuous predictor `\(v\)` is nominated for splitting the current node
- with splitting value `\(l\)`
- such that `\(S_1\)` is the set of observations for which `\(v_i \le l\)`
- and `\(S_2\)` is the set of observations for which `\(v_i &gt; l\)`
- `\(\overline y_1\)` is the average of `\(y\)` in set `\(S_1\)`

`\(SSE = \sum_{i \in S_1}(y_i - \overline y_1)^2 + \sum_{i \in S_2}(y_i - \overline y_2)^2\)`

For example if `age` is candidate in splitting `income`:

---


```r
library(patchwork)

sse &lt;- function(l, df, v) {
  income_above &lt;- df %&gt;% filter({{v}} &gt;= l) %&gt;% pull(income)
  income_below &lt;- df %&gt;% filter({{v}} &lt; l) %&gt;% pull(income)
  sse_above &lt;- sum((income_above - mean(income_above))^2)
  sse_below &lt;- sum((income_below - mean(income_below))^2)
  return(sse_above + sse_below)
}
age &lt;- seq(18,69, 1)
sse_age &lt;- map_dbl(age, sse, df = okcupid2_train, v = age)

p1 &lt;- okcupid2_train %&gt;%
  count(age, income) %&gt;%
  ggplot(aes(age, income)) +
  geom_point(aes(size = n)) +
  theme_classic() +
  labs(x = "") +
  theme(axis.text.x = element_blank())

p2 &lt;- tibble(age = age, sse = sse_age) %&gt;%
  ggplot(aes(age, sse)) +
  geom_line() +
  geom_point() +
  theme_classic()

p1 / p2
```

---

&lt;img src="images/Age-SSE-1.png" width="80%" /&gt;

---

### The `\(SSE\)` criterion - categorical predictor

.insight[
💡 What could be an issue with a categorical variable with many levels?
]

- a categorical predictor `\(l\)` is nominated for splitting the current node
  - option 1: use dummy variables, turning each level into a 2-level 0/1 category variable
  - option 2: order levels by some criterion like `\(\overline{y_j}\)` and treat `\(l\)` as continuous from here on

For example if `job3` is candidate in splitting `income`:
---


```r
mean_income_vs_job &lt;- okcupid2_train %&gt;%
  group_by(job3) %&gt;%
  summarise(mean_income = mean(income)) %&gt;%
  arrange(mean_income)
jobs_levels_sorted &lt;- as.character(mean_income_vs_job$job3)
okcupid2_train_job_sorted &lt;- okcupid2_train %&gt;%
  mutate(job_ordered = fct_relevel(job3, jobs_levels_sorted),
         job_ordered_n = as.numeric(job_ordered))

job_rank &lt;- seq(1, length(jobs_levels_sorted), 1)
sse_job &lt;- map_dbl(job_rank, sse, df = okcupid2_train_job_sorted, v = job_ordered_n)

p1 &lt;- okcupid2_train_job_sorted %&gt;%
  ggplot(aes(job_ordered, income)) +
  geom_boxplot() +
  theme_classic() + labs(x = "") +
  theme(axis.text.x = element_blank())

p2 &lt;- tibble(job = factor(jobs_levels_sorted, levels = jobs_levels_sorted), sse = sse_job) %&gt;%
  ggplot(aes(job, sse, group = 1)) +
  geom_line() +
  geom_point() +
  theme_classic() +
  labs(x = "") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))

p1 / p2
```

---

&lt;img src="images/Job-SSE-1.png" width="80%" /&gt;

---

### Pruning: The `\(SSE_{c_p}\)` criterion

The "right-sized" tree should not be too deep to avoid overfitting.

Once the full tree is grown, we check for each split:

`\(SSE_{c_p} = SSE_{tot} + c_p \cdot {\text{#terminal nodes}}\)`

Where `\(c_p\)` is a penalty or regularization parameter usually chosen by cross validation.

And we choose the smallest pruned tree with the minimum `\(SSE_{c_p}\)`.

---

### CART with `rpart`

Let's tune `\(c_p\)` for our data using a 5-fold (manual) Cross Validation. The criterion to maximize would be RMSE.


```r
library(rpart)

n_cv &lt;- 5; cp_seq &lt;- seq(0, 0.02, 0.001)

okcupid2_train_val &lt;- okcupid2_train %&gt;%
  mutate(val = sample(1:n_cv, n(), replace = TRUE))

get_validation_set_rmse &lt;- function(i, .cp) {
  ok_tr &lt;- okcupid2_train_val %&gt;% filter(val != i) %&gt;% select(-val)
  ok_val &lt;- okcupid2_train_val %&gt;% filter(val == i) %&gt;% select(-val)
  mod &lt;- rpart(income ~ ., data = ok_tr,
               control = rpart.control(cp = .cp))
  pred &lt;- predict(mod, ok_val)
  rmse(ok_val$income, pred)
}
get_cv_rmse &lt;- function(.cp) {
  tibble(cp = rep(.cp, n_cv),
         rmse = map_dbl(1:n_cv, get_validation_set_rmse, .cp = .cp))
}
```

---


```r
# cv_table &lt;- map_dfr(cp_seq, get_cv_rmse)
cv_table &lt;- read_rds("../data/okcupid_CART_Reg_Cp_CV.rds")

cv_table %&gt;%
  mutate(cp = factor(cp)) %&gt;%
  ggplot(aes(cp, rmse)) +
  geom_boxplot() +
  theme_classic() +
  labs(x = "") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```

&lt;img src="images/Cp-CV-1.png" width="100%" /&gt;

---

Training on the entire training set:


```r
mod_tree_na &lt;- rpart(income ~ ., data = okcupid2_train,
                     control = rpart.control(cp = 0.004))
```

You can plot the tree using `rpart`, but...


```r
plot(mod_tree_na)
text(mod_tree_na, pretty = 1, use.n = TRUE)
```

&lt;img src="images/CART-NA1-1.png" width="50%" /&gt;

---

The plotting function in the `rpart.plot` package is slightly nicer:


```r
library(rpart.plot)
prp(mod_tree_na, type = 5, extra = 1)
```

&lt;img src="images/CART-NA2-1.png" width="100%" /&gt;

---

You can go fancy and use `partykit` and/or `ggparty`:


```r
library(ggparty)

party_obj &lt;- as.party(mod_tree_na)

ggparty(party_obj) +
  geom_edge() +
  # geom_edge_label() +
  geom_node_label(aes(label = splitvar), ids = "inner") +
  geom_node_label(aes(label = str_c("n = ", nodesize)),
                  ids = "terminal", nudge_y = 0.02) +
  geom_node_plot(gglist = list(geom_boxplot(aes(y = income)),
                                 theme(axis.text.x=element_blank(),
                                       axis.ticks.x=element_blank())),
                 shared_axis_labels=TRUE)
```

---

&lt;img src="images/CART-NA3-1.png" width="100%" /&gt;

---

Or you can just print the tree and see what is going on:


```r
print(party_obj)
```

&lt;img src = "images/CART_print.png" style="width: 100%"&gt;

---

### Variables Importance

Summing the reduction in `\(SSE\)` for each split variable, we can get a measure of importance.

.font80percent[Unfortunately in `rpart` the "potential" reduction in `\(SSE\)` for surrogate splits is also summed. You can either ignore or retrain with only the variables chosen by the model.]


```r
enframe(mod_tree_na$variable.importance) %&gt;%
  arrange(value) %&gt;%
  mutate(variable = as_factor(name)) %&gt;%
  ggplot(aes(variable, value)) +
  geom_segment(aes(x = variable, xend = variable,
                   y = 0, yend = value)) +
  geom_point(size = 3) +
  theme_classic() +
  coord_flip() +
  labs(x = "", y = "")
```

---

&lt;img src="images/CART-VarImp-1.png" width="60%" /&gt;

.insight[
💡 How would this profile look for a couple of very correlated predictors?
]

---

### Prediction


```r
pred_tree_na &lt;- predict(mod_tree_na, okcupid2_valid)

report_rmse_and_cor(okcupid2_valid$income, pred_tree_na)
```

```
## RMSE: 0.366
## CORR: 0.436
```

As expected, far from impressive. Let's try using the imputed `NA` data:


```r
mod_tree_imp &lt;- rpart(income ~ ., data = okcupid2_imp_mice_train,
                      control = rpart.control(cp = 0.004))
pred_tree_imp &lt;- predict(mod_tree_imp, okcupid2_imp_mice_valid)

report_rmse_and_cor(okcupid2_valid$income, pred_tree_imp)
```

```
## RMSE: 0.368
## CORR: 0.425
```

---


```r
tibble(income = okcupid2_valid$income, pred = pred_tree_na) %&gt;%
  count(income, pred) %&gt;%
  ggplot(aes(income, pred)) +
  geom_point(aes(size = n)) +
  ylim(range(okcupid2_valid$income))
```

&lt;img src="images/Tree-Fit-1.png" width="60%" /&gt;

---

class: section-slide

# Detour: A Classification Problem

---

class: section-slide

# End of Detour

---

class: section-slide

# The CART (Classification)

---

class: section-slide

# Bagged Trees

---

class: section-slide

# Random Forests

---

class: section-slide

# Boosted Trees

---

class: section-slide

# The Others

---
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="../libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
