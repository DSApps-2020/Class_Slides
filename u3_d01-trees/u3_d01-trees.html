<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>The Trees</title>
    <meta charset="utf-8" />
    <meta name="author" content="Giora Simchoni" />
    <meta name="date" content="2020-02-08" />
    <head>
      <link rel="icon" href="../DSApps_logo.jpg" type="image/jpg"> 
      <link rel="shortcut icon" href="../DSApps_logo.jpg" type="image/jpg">
    </head>
    <link rel="stylesheet" href="..\slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">


class: logo-slide

---

class: title-slide

## The Trees

### Applications of Data Science - Class 11

### Giora Simchoni

#### `gsimchoni@gmail.com and add #dsapps in subject`

### Stat. and OR Department, TAU
### 2020-02-08

---



layout: true

&lt;div class="my-footer"&gt;
  &lt;span&gt;
    &lt;a href="https://dsapps-2020.github.io/Class_Slides/" target="_blank"&gt;Applications of Data Science
    &lt;/a&gt;
  &lt;/span&gt;
&lt;/div&gt;

---



class: section-slide

# The Pros and Cons of Trees

---

## Pros

- It's just a set of if-else statements my 7 y/o can get
- Highly interpretable (when tree is not large)
- Easy to implement
- Fast? (to predict)
- Little pre-processing of predictors needed
- Handle all types of predictors (continuous, categorical)
- Handle missing data, *predict* missing data
- Assumption free
- Feature selection built-in (but when predictors are correlated...)
- Low bias, in general

---

## Cons

- High variance, in general
- Rectangular predictor regions - not always a good thing
- Complexity of prediction limited in no. of leaves! (For a simple CART)
- Not so fast? (to train)
- Greedy
- Selection Bias of predictors with more distinct values?
- Variable Importance when predictors are correlated

---

class: section-slide

# Detour: A Regression Problem

---

### OKCupid: Predicting Annual Income

It won't be easy:


```r
okcupid &lt;- read_csv("../data/okcupid.csv.zip")

okcupid %&gt;% count(income, sort = TRUE) %&gt;% head(20)
```

```
## # A tibble: 13 x 2
##     income     n
##      &lt;dbl&gt; &lt;int&gt;
##  1      -1 48442
##  2   20000  2952
##  3  100000  1621
##  4   80000  1111
##  5   30000  1048
##  6   40000  1005
##  7   50000   975
##  8   60000   736
##  9   70000   707
## 10  150000   631
## 11 1000000   521
## 12  250000   149
## 13  500000    48
```

---

We will stick to non-NA (income) observations, and predict `\(\log_{10}(income/100000)\)`:


```r
okcupid2 &lt;- okcupid %&gt;%
  mutate(income = ifelse(income == -1, NA, log10(income/100000))) %&gt;%
  drop_na(income)
```



In the vector `predictors` .font80percent[(see slides Rmd source)] we have 42 continuous and categorical variables which may or may not be predictive to income:


```r
okcupid2 &lt;- okcupid2 %&gt;%
  select(income, predictors) %&gt;%
  mutate(id = 1:n())

dim(okcupid2)
```

```
## [1] 11504    44
```

---


```r
glimpse(okcupid2)
```

```
## Observations: 11,504
## Variables: 44
## $ income                &lt;dbl&gt; -0.09691001, -0.69897000, -0.39794001, -...
## $ age                   &lt;dbl&gt; 35, 23, 28, 30, 29, 40, 31, 22, 35, 31, ...
## $ height_cm             &lt;dbl&gt; 177.80, 180.34, 182.88, 167.64, 157.48, ...
## $ sex                   &lt;fct&gt; m, m, m, f, f, m, f, m, m, f, m, m, f, m...
## $ body_type             &lt;fct&gt; average, thin, average, skinny, thin, fi...
## $ body_type_not_perfect &lt;fct&gt; TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, ...
## $ diet2                 &lt;fct&gt; other, vegetarian, anything, anything, a...
## $ drinks                &lt;fct&gt; often, socially, socially, socially, soc...
## $ drugs                 &lt;fct&gt; sometimes, NA, never, never, never, NA, ...
## $ religion2             &lt;fct&gt; atheist, NA, christian, christian, chris...
## $ education2            &lt;fct&gt; other, student1, degree1, high_school, s...
## $ education_kind        &lt;fct&gt; working, working, graduated, graduated, ...
## $ education_academic    &lt;fct&gt; FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, ...
## $ ethnicity2            &lt;fct&gt; white, white, white, white, other, white...
## $ part_black            &lt;fct&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE...
## $ part_white            &lt;fct&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, NA, ...
## $ part_asian            &lt;fct&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE...
## $ part_hispanic         &lt;fct&gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,...
## $ job3                  &lt;fct&gt; travel, student, financial, marketing, o...
## $ orientation           &lt;fct&gt; straight, straight, straight, straight, ...
## $ pets_has_dogs         &lt;fct&gt; FALSE, FALSE, FALSE, TRUE, FALSE, FALSE,...
## $ pets_has_cats         &lt;fct&gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,...
## $ pets_likes_cats       &lt;fct&gt; TRUE, TRUE, TRUE, TRUE, FALSE, FALSE, NA...
## $ pets_likes_dogs       &lt;fct&gt; TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, N...
## $ sign_fun              &lt;fct&gt; FALSE, FALSE, FALSE, NA, FALSE, TRUE, NA...
## $ sign_not_matter       &lt;fct&gt; FALSE, FALSE, TRUE, NA, FALSE, FALSE, NA...
## $ sign_matters          &lt;fct&gt; FALSE, FALSE, FALSE, NA, FALSE, FALSE, N...
## $ sign2                 &lt;fct&gt; cancer, pisces, leo, NA, taurus, gemini,...
## $ speaks_spanish        &lt;fct&gt; TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, ...
## $ speaks_french         &lt;fct&gt; TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, ...
## $ speaks_german         &lt;fct&gt; FALSE, TRUE, FALSE, FALSE, FALSE, FALSE,...
## $ speaks_chinese        &lt;fct&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE...
## $ status                &lt;fct&gt; single, single, seeing someone, single, ...
## $ essay0_len            &lt;dbl&gt; 6.538163, 3.713962, 7.336296, NA, NA, 6....
## $ essay1_len            &lt;dbl&gt; 3.951551, 3.713962, 6.095861, NA, 5.8435...
## $ essay2_len            &lt;dbl&gt; 4.564515, 4.605330, 6.109283, NA, 6.4281...
## $ essay3_len            &lt;dbl&gt; NA, 3.496992, 5.733393, NA, 4.204931, 4....
## $ essay4_len            &lt;dbl&gt; 5.517517, 5.327954, 6.278551, NA, 6.9716...
## $ essay5_len            &lt;dbl&gt; 5.723637, NA, 6.424895, NA, 4.812314, 5....
## $ essay6_len            &lt;dbl&gt; NA, 3.258712, 5.459654, NA, 4.709674, 5....
## $ essay7_len            &lt;dbl&gt; NA, NA, 4.709674, NA, 4.875319, 5.459654...
## $ essay8_len            &lt;dbl&gt; 3.9123430, NA, 4.5645148, NA, 4.5434650,...
## $ essay9_len            &lt;dbl&gt; NA, 3.045284, 5.669936, NA, 3.784553, 8....
## $ id                    &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1...
```

---

We split the data into training, validation and test sets:


```r
# test_idx &lt;- sample(1:nrow(okcupid2), 2000, replace = FALSE)
# train_idx &lt;- okcupid2 %&gt;% filter(!id %in% test_idx) %&gt;% sample_n(8000) %&gt;% pull(id)
# valid_idx &lt;- okcupid2 %&gt;% filter(!id %in% test_idx, !id %in% train_idx) %&gt;% pull(id)
okcupid2 &lt;- okcupid2 %&gt;% select(-id)

idx &lt;- read_rds("../data/okcupid2_idx.rda")
train_idx &lt;- idx$train_idx
valid_idx &lt;- idx$valid_idx
test_idx &lt;- idx$test_idx

okcupid2_train &lt;- okcupid2[train_idx, ]
okcupid2_valid &lt;- okcupid2[valid_idx, ]
okcupid2_test &lt;- okcupid2[test_idx, ]

library(glue)
glue("train no. of rows: {nrow(okcupid2_train)}
     validation no. of rows: {nrow(okcupid2_valid)}
     test no. of rows: {nrow(okcupid2_test)}")
```

```
## train no. of rows: 8000
## validation no. of rows: 1504
## test no. of rows: 2000
```

---

Our transformed income dependent variable behaves "ok":


```r
ggplot(okcupid2_train, aes(income)) +
  geom_histogram(fill = "red", alpha = 0.5) +
  theme_classic()
```

&lt;img src="images/Income-Hist-1.png" width="100%" /&gt;

---

We can quickly see percentage of missing values with [`naniar`](http://naniar.njtierney.com/):


```r
library(naniar)

vis_miss(okcupid2_train %&gt;%
           sample_frac(0.2) %&gt;%
           select(-starts_with("essay")))
```

&lt;img src="images/Missingness-1.png" width="100%" /&gt;

---

Also worth exploring some basic relations between predictors and income. You can use the work of others, e.g. [`ggpairs`](https://ggobi.github.io/ggally/):


```r
library(GGally)

ggpairs(okcupid2_train %&gt;%
          select(income, age, sex, height_cm, body_type_not_perfect))
```

&lt;img src="images/GGPairs-1.png" width="100%" /&gt;

---

But don't be ashamed of simply exploring on your own:


```r
var_vs_income_boxplot &lt;- function(var) {
  ggplot(okcupid2_train, aes({{var}}, income)) +
  geom_boxplot() +
  facet_wrap(~ sex) +
  theme_classic() +
  labs(x = "") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
}
```

---


```r
var_vs_income_boxplot(body_type)
```

&lt;img src="images/Body_Type-Income-1.png" width="100%" /&gt;

---


```r
var_vs_income_boxplot(sign2)
```

&lt;img src="images/Sign-Income-1.png" width="100%" /&gt;

---


```r
var_vs_income_boxplot(job3)
```

&lt;img src="images/Job-Income-1.png" width="100%" /&gt;

---


```r
var_vs_income_boxplot(education2)
```

&lt;img src="images/Edu-Income-1.png" width="100%" /&gt;

---


```r
var_vs_income_boxplot(religion2)
```

&lt;img src="images/Religion-Income-1.png" width="100%" /&gt;

---


```r
var_vs_income_boxplot(diet2)
```

&lt;img src="images/Diet-Income-1.png" width="100%" /&gt;

---


```r
ggpairs(okcupid2_train %&gt;%
          select(essay0_len:essay2_len, income))
```

&lt;img src="images/Essay-Income-1.png" width="100%" /&gt;

---

### Baseline: Linear Regression

R's `lm` function does not take `NA` values.

One strategy is to impute these values using a "common" value such as the median for continuous variables and mode for categorical variables. This can easily be achieved with `naniar`:


```r
okcupid2_imp &lt;- naniar::impute_median_all(okcupid2)
okcupid2_imp_train &lt;- okcupid2_imp[train_idx, ]
okcupid2_imp_valid &lt;- okcupid2_imp[valid_idx, ]

mod_lm &lt;- lm(income ~ ., data = okcupid2_imp_train)
pred_lm &lt;- predict(mod_lm, okcupid2_imp_valid)

rmse &lt;- function(obs, pred) sqrt(mean((obs - pred)^2))

report_rmse_and_cor &lt;- function(obs, pred) {
  RMSE &lt;- rmse(obs, pred)
  CORR &lt;- cor(obs, pred)
  glue("RMSE: {format(RMSE, digits = 3)}
       CORR: {format(CORR, digits = 3)}")
}
```

---


```r
report_rmse_and_cor(okcupid2_valid$income, pred_lm)
```

```
## RMSE: 0.352
## CORR: 0.501
```


```r
tibble(income = okcupid2_valid$income, pred = pred_lm) %&gt;%
  ggplot(aes(income, pred)) + geom_point() + ylim(range(okcupid2_valid$income))
```

&lt;img src="images/LM-Fit1-1.png" width="50%" /&gt;

---

A more intelligent strategy for imputing missing values would be to *predict* them using whatever data is not missing. This can be done quite seamlessly with the [`mice`](https://stefvanbuuren.name/mice/) package:


```r
# library(mice)
# mice_obj &lt;- mice(okcupid2, m = 1, maxit = 10, seed = 42)
# okcupid2_imp_mice &lt;- complete(mice_obj)

okcupid2_imp_mice &lt;- read_rds("../data/okcupid2_imp_mice.rds")
okcupid2_imp_mice_train &lt;- okcupid2_imp_mice[train_idx, ]
okcupid2_imp_mice_valid &lt;- okcupid2_imp_mice[valid_idx, ]

mod_lm_mice &lt;- lm(income ~ ., data = okcupid2_imp_mice_train)
pred_lm_mice &lt;- predict(mod_lm_mice, okcupid2_imp_mice_valid)

report_rmse_and_cor(okcupid2_valid$income, pred_lm_mice)
```

```
## RMSE: 0.351
## CORR: 0.504
```

.insight[
üí° Can you think of other imputation strategies?
]

---


```r
tibble(income = okcupid2_valid$income, pred = pred_lm_mice) %&gt;%
  ggplot(aes(income, pred)) +
  geom_point() +
  ylim(range(okcupid2_valid$income))
```

&lt;img src="images/LM-Fit2-1.png" width="60%" /&gt;

---

### Baseline: Ridge Regression


```r
library(glmnet)

okcupid2_imp_mat_train &lt;- model.matrix( ~ ., okcupid2_imp_mice_train[, predictors])
okcupid2_imp_mat_valid &lt;- model.matrix( ~ ., okcupid2_imp_mice_valid[, predictors])

ridge_cv &lt;- cv.glmnet(x = okcupid2_imp_mat_train,
                      y = okcupid2_train$income, alpha = 0)

best_lambda &lt;- ridge_cv$lambda.min

mod_lm_ridge &lt;- glmnet(x = okcupid2_imp_mat_train,
                      y = okcupid2_train$income, alpha = 0,
                      lambda = best_lambda)

pred_lm_ridge &lt;- predict(mod_lm_ridge, okcupid2_imp_mat_valid)

report_rmse_and_cor(okcupid2_valid$income, pred_lm_ridge)
```

```
## RMSE: 0.351
## CORR: 0.505
```

---


```r
tibble(income = okcupid2_valid$income, pred = pred_lm_ridge) %&gt;%
  ggplot(aes(income, pred)) +
  geom_point() +
  ylim(range(okcupid2_valid$income))
```

&lt;img src="images/LM-Ridge-Fit-1.png" width="60%" /&gt;

---

### Baseline: Lasso Regression


```r
lasso_cv &lt;- cv.glmnet(x = okcupid2_imp_mat_train,
                      y = okcupid2_imp_train$income, alpha = 1)

best_lambda &lt;- lasso_cv$lambda.min

mod_lm_lasso &lt;- glmnet(x = okcupid2_imp_mat_train,
                      y = okcupid2_train$income, alpha = 1,
                      lambda = best_lambda)

pred_lm_lasso &lt;- predict(mod_lm_lasso, okcupid2_imp_mat_valid)

report_rmse_and_cor(okcupid2_valid$income, pred_lm_lasso)
```

```
## RMSE: 0.351
## CORR: 0.505
```

---


```r
tibble(income = okcupid2_valid$income, pred = pred_lm_lasso) %&gt;%
  ggplot(aes(income, pred)) +
  geom_point() +
  ylim(range(okcupid2_valid$income))
```

&lt;img src="images/LM-Lasso-Fit-1.png" width="60%" /&gt;

---

class: section-slide

# End of Detour

---

class: section-slide

# The CART (Regression)

---

### The OG CART

1. Find the the predictor to (binary) split on and the value of the split, by `\(SSE\)` criterion
2. For each resulting node if `\(n_{node} &gt; 20\)` go to 1
3. Once full tree has been grown, perform pruning using the *cost-complexity parameter* `\(c_p\)` and the `\(SSE_{c_p}\)` criterion
4. Predict the average value at each terminal node

- For each split *surrogate splits* are saved for future `NA`s
- An alternative criterion for complexity could be tree maximum depth (sklearn)
- One can also reduce all tree's unique paths to a set of rules
- Variables can be ranked by "importance"

.insight[
üí° What if the best split isn't binary?
]

---

### The `\(SSE\)` criterion - continuous predictor

- `\(y\)` is the continuous dependent variable
- a continuous predictor `\(v\)` is nominated for splitting the current node
- with splitting value `\(l\)`
- such that `\(S_1\)` is the set of observations for which `\(v_i \le l\)`
- and `\(S_2\)` is the set of observations for which `\(v_i &gt; l\)`
- `\(\overline y_1\)` is the average of `\(y\)` in set `\(S_1\)`

`\(SSE = \sum_{i \in S_1}(y_i - \overline y_1)^2 + \sum_{i \in S_2}(y_i - \overline y_2)^2\)`

For example if `age` is candidate in splitting `income`:

---


```r
library(patchwork)

sse &lt;- function(l, df, v) {
  income_above &lt;- df %&gt;% filter({{v}} &gt;= l) %&gt;% pull(income)
  income_below &lt;- df %&gt;% filter({{v}} &lt; l) %&gt;% pull(income)
  sse_above &lt;- sum((income_above - mean(income_above))^2)
  sse_below &lt;- sum((income_below - mean(income_below))^2)
  return(sse_above + sse_below)
}
age &lt;- seq(18,69, 1)
sse_age &lt;- map_dbl(age, sse, df = okcupid2_train, v = age)

p1 &lt;- okcupid2_train %&gt;%
  count(age, income) %&gt;%
  ggplot(aes(age, income)) +
  geom_point(aes(size = n)) +
  theme_classic() +
  labs(x = "") +
  theme(axis.text.x = element_blank())

p2 &lt;- tibble(age = age, sse = sse_age) %&gt;%
  ggplot(aes(age, sse)) +
  geom_line() +
  geom_point() +
  theme_classic()

p1 / p2
```

---

&lt;img src="images/Age-SSE-1.png" width="80%" /&gt;

---

### The `\(SSE\)` criterion - categorical predictor

.insight[
üí° What could be an issue with a categorical variable with many levels?
]

- a categorical predictor `\(l\)` is nominated for splitting the current node
  - option 1: use dummy variables, turning each level into a 2-level 0/1 category variable
  - option 2: order levels by some criterion like `\(\overline{y_j}\)` and treat `\(l\)` as continuous from here on

For example if `job3` is candidate in splitting `income`:
---


```r
mean_income_vs_job &lt;- okcupid2_train %&gt;%
  group_by(job3) %&gt;%
  summarise(mean_income = mean(income)) %&gt;%
  arrange(mean_income)
jobs_levels_sorted &lt;- as.character(mean_income_vs_job$job3)
okcupid2_train_job_sorted &lt;- okcupid2_train %&gt;%
  mutate(job_ordered = fct_relevel(job3, jobs_levels_sorted),
         job_ordered_n = as.numeric(job_ordered))

job_rank &lt;- seq(1, length(jobs_levels_sorted), 1)
sse_job &lt;- map_dbl(job_rank, sse, df = okcupid2_train_job_sorted, v = job_ordered_n)

p1 &lt;- okcupid2_train_job_sorted %&gt;%
  ggplot(aes(job_ordered, income)) +
  geom_boxplot() +
  theme_classic() + labs(x = "") +
  theme(axis.text.x = element_blank())

p2 &lt;- tibble(job = factor(jobs_levels_sorted, levels = jobs_levels_sorted), sse = sse_job) %&gt;%
  ggplot(aes(job, sse, group = 1)) +
  geom_line() +
  geom_point() +
  theme_classic() +
  labs(x = "") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))

p1 / p2
```

---

&lt;img src="images/Job-SSE-1.png" width="80%" /&gt;

---

### Pruning: The `\(SSE_{c_p}\)` criterion

The "right-sized" tree should not be too deep to avoid overfitting.

Once the full tree is grown, we check for each split:

`\(SSE_{c_p} = SSE_{tot} + c_p \cdot {\text{#terminal nodes}}\)`

Where `\(c_p\)` is a penalty or regularization parameter usually chosen by cross validation.

And we choose the smallest pruned tree with the minimum `\(SSE_{c_p}\)`.

---

### CART with `rpart`

Growing the full tree .font80percent[(for a numeric `\(y\)` `rpart` will guess this is Regression)]:


```r
library(rpart)

mod_tree &lt;- rpart(income ~ ., data = okcupid2_train)
```

Pruning with some `\(c_p\)`:


```r
mod_tree &lt;- prune(mod_tree, cp = 0.05)
```

.warning[
‚ö†Ô∏è There is a `\(c_p\)` parameter you can pass to `rpart` while training like so:

`rpart(income ~ ., data = okcupid2_train, control = rpart.control(cp = 0.05))`

But this will only make `rpart` consider this `\(c_p\)` at each split as a minimum criterion *while growing the unpruned tree*. In fact the default of this parameter is 0.01!
]

---

So, in order to train a true unpruned tree you would need to pass `\(c_p = 0\)`:


```r
mod_tree &lt;- rpart(income ~ ., data = okcupid2_train,
                  control = rpart.control(cp = 0))
```

Now. `rpart` by default will perform 10-fold Cross Validation on each split, resulting in this `cptable`:


```r
head(mod_tree$cptable)
```

```
##            CP nsplit rel error    xerror       xstd
## 1 0.113597498      0 1.0000000 1.0003629 0.02238584
## 2 0.048179174      1 0.8864025 0.8899523 0.02282596
## 3 0.013140594      2 0.8382233 0.8415798 0.02365898
## 4 0.010043056      3 0.8250827 0.8379808 0.02385357
## 5 0.009147817      4 0.8150397 0.8306874 0.02405999
## 6 0.005634308      5 0.8058919 0.8207212 0.02419261
```

---

The nature of the `xerror` is unclear from the [docs](https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf) (SSE/n ?) except that it is relative to the error in the root node.

You can either use it like so:


```r
best_cp &lt;- mod_tree$cptable[which.min(mod_tree$cptable[,"xerror"]),"CP"]
mod_tree &lt;- prune(mod_tree, cp = best_cp)
```

Or you can perfrom CV on your own, passing at each stage a parameter for `rpart` to not perform CV:


```r
mod_tree &lt;- rpart(income ~ ., data = okcupid2_train,
                  control = rpart.control(cp = 0), xval = 1)
```

---

Let's tune `\(c_p\)` for our data using a 5-fold (manual) Cross Validation. The criterion to maximize would be RMSE.


```r
n_cv &lt;- 5; cp_seq &lt;- seq(0, 0.02, 0.001)

okcupid2_train_val &lt;- okcupid2_train %&gt;%
  mutate(val = sample(1:n_cv, n(), replace = TRUE))

get_validation_set_rmse &lt;- function(i, .cp) {
  ok_tr &lt;- okcupid2_train_val %&gt;% filter(val != i) %&gt;% select(-val)
  ok_val &lt;- okcupid2_train_val %&gt;% filter(val == i) %&gt;% select(-val)
  mod &lt;- rpart(income ~ ., data = ok_tr,
               control = rpart.control(cp = 0, xval = 1))
  mod &lt;- prune(mod, cp = .cp)
  pred &lt;- predict(mod, ok_val)
  rmse(ok_val$income, pred)
}

get_cv_rmse &lt;- function(.cp) {
  tibble(cp = rep(.cp, n_cv),
         rmse = map_dbl(1:n_cv, get_validation_set_rmse, .cp = .cp))
}
```

---


```r
cv_table &lt;- map_dfr(cp_seq, get_cv_rmse)

cv_table %&gt;%
  mutate(cp = factor(cp)) %&gt;%
  ggplot(aes(cp, rmse)) +
  geom_boxplot() +
  theme_classic() +
  labs(x = "") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```

&lt;img src="images/Cp-CV2-1.png" width="100%" /&gt;

---

Training on the entire training set:


```r
mod_tree_na &lt;- rpart(income ~ ., data = okcupid2_train,
                     control = rpart.control(cp = 0, xval = 1))
mod_tree_na &lt;- prune(mod_tree_na, cp = 0.003)
```

You can plot the tree using `rpart`, but...


```r
plot(mod_tree_na)
text(mod_tree_na, pretty = 1, use.n = TRUE)
```

&lt;img src="images/CART-NA1-1.png" width="50%" /&gt;

---

The plotting function in the `rpart.plot` package is slightly nicer:


```r
library(rpart.plot)
prp(mod_tree_na, type = 5, extra = 1)
```

&lt;img src="images/CART-NA2-1.png" width="100%" /&gt;

---

You can go fancy and use `partykit` and/or `ggparty`:


```r
library(ggparty)

party_obj &lt;- as.party(mod_tree_na)

ggparty(party_obj) +
  geom_edge() +
  # geom_edge_label() +
  geom_node_label(aes(label = splitvar), ids = "inner") +
  geom_node_label(aes(label = str_c("n = ", nodesize)),
                  ids = "terminal", nudge_y = 0.02) +
  geom_node_plot(gglist = list(geom_boxplot(aes(y = income)),
                                 theme(axis.text.x=element_blank(),
                                       axis.ticks.x=element_blank())),
                 shared_axis_labels=TRUE)
```

---

&lt;img src = "images/CART-ggparty.png" style="width: 100%"&gt;

---

Or you can just print the tree and see what is going on:


```r
print(party_obj)
```

&lt;img src = "images/CART_print.png" style="width: 100%"&gt;

---

### Variables Importance

Summing the reduction in `\(SSE\)` for each split variable, we can get a measure of importance.

.font80percent[Unfortunately in `rpart` the "potential" reduction in `\(SSE\)` for surrogate splits is also summed. You can either ignore or retrain with only the variables chosen by the model.]


```r
enframe(mod_tree_na$variable.importance) %&gt;%
  arrange(value) %&gt;%
  mutate(variable = as_factor(name)) %&gt;%
  ggplot(aes(variable, value)) +
  geom_segment(aes(x = variable, xend = variable,
                   y = 0, yend = value)) +
  geom_point(size = 3) +
  theme_classic() +
  coord_flip() +
  labs(x = "", y = "")
```

---

&lt;img src="images/CART-VarImp-1.png" width="60%" /&gt;

.insight[
üí° How would this profile look for a couple of very correlated predictors?
]

---

### Prediction


```r
pred_tree_na &lt;- predict(mod_tree_na, okcupid2_valid)

report_rmse_and_cor(okcupid2_valid$income, pred_tree_na)
```

```
## RMSE: 0.363
## CORR: 0.449
```

As expected, far from impressive. Let's try using the imputed `NA` data:


```r
mod_tree_imp &lt;- rpart(income ~ ., data = okcupid2_imp_mice_train,
                      control = rpart.control(cp = 0, xval = 1))
mod_tree_imp &lt;- prune(mod_tree_imp, cp = 0.003)
pred_tree_imp &lt;- predict(mod_tree_imp, okcupid2_imp_mice_valid)

report_rmse_and_cor(okcupid2_valid$income, pred_tree_imp)
```

```
## RMSE: 0.369
## CORR: 0.424
```

---


```r
tibble(income = okcupid2_valid$income, pred = pred_tree_na) %&gt;%
  count(income, pred) %&gt;%
  ggplot(aes(income, pred)) +
  geom_point(aes(size = n)) +
  ylim(range(okcupid2_valid$income))
```

&lt;img src="images/Tree-Fit-1.png" width="60%" /&gt;

---

class: section-slide

# The Others

---

&lt;img src = "images/CART-ggparty.png" style="width: 80%"&gt;

Looking at this one might wonder:
- Predicting a single value for each terminal node? Is that the best we can do?
- Since when do we just split, how about a statistical test?
- Many variables repeat in the same path, can we reduce paths to simple rules?

---

### Model Trees

[Quinlan (1992)](https://sci2s.ugr.es/keel/pdf/algorithm/congreso/1992-Quinlan-AI.pdf) and later [Wang and Witten (1997)](https://pdfs.semanticscholar.org/3324/4b59fe506331926b3a33e348209ac456c532.pdf) made a few changes to the beloved CART:

- instead of single values at terminal nodes, M5 trees have *linear models*
- sort of a piecewise linear regression only the "pieces" can come from many variables
- can extrapolate to never before seen values (not necessarily good)

---

### Conditional Inference Trees

---

### Rule-Based Trees

---

class: section-slide

# Bagged Trees

---

### Bagging: The Gist

For `\(j= 1\)` to `\(m\)` do
- Generate a bootstrap sample of the original data
- Train an unpruned tree model on this sample

End.


- Predict average through all `\(m\)` trees (or %vote for each class in classification)
- What you get: better prediction, *OOB* error estimates
- What you lose: interpretability, speed (but bagging can be paralleled)

---

Why does Bagging work?

&lt;img src="images/Bagging-Trees-1.png" width="80%" /&gt;

---

Which models are likely to benefit from bagging?


```r
train_lm &lt;- function(i, .n) {
  samp &lt;- sample(1:.n, .n, replace = TRUE)
  lm(income ~ ., data = okcupid2_imp_mice_train[samp, ])
}

train_tree &lt;- function(i, .n) {
  samp &lt;- sample(1:.n, .n, replace = TRUE)
  rpart(income ~ ., data = okcupid2_imp_mice_train[samp, ])
}

predict_mod &lt;- function(mod) {
  tibble(pred = predict(mod, okcupid2_imp_mice_valid))
}

rmse_m_models &lt;- function(m, .preds) {
  rmse(okcupid2_imp_mice_valid$income, rowMeans(.preds[, 1:m]))
}

n &lt;- nrow(okcupid2_imp_mice_train)
m_max &lt;- 50

mod_lms &lt;- map(1:m_max, train_lm, .n = n)
mod_trees &lt;- map(1:m_max, train_tree, .n = n)

preds_lm &lt;- map_dfc(mod_lms, predict_mod)
preds_tree &lt;- map_dfc(mod_trees, predict_mod)
```

---


```r
rmse_lm &lt;- map_dbl(1:m_max, rmse_m_models, .preds = preds_lm)
rmse_tree &lt;- map_dbl(1:m_max, rmse_m_models, .preds = preds_tree)
tibble(m = rep(1:m_max, 2),
       model = c(rep("lm", m_max), rep("CART", m_max)),
       RMSE = c(rmse_lm, rmse_tree)) %&gt;%
  ggplot(aes(m, RMSE, color = model)) +
  ylim(c(0.34, 0.38)) +
  geom_line() + geom_point() +
  theme_classic()
```

&lt;img src="images/Bagging-LM-CART-1.png" width="100%" /&gt;


---

### Bagging with `ipred`


```r
library(ipred)

mod_bag_na &lt;- bagging(income ~ ., data = okcupid2_train, nbagg = 50)
pred_bag_na &lt;- predict(mod_bag_na, okcupid2_valid)
report_rmse_and_cor(okcupid2_valid$income, pred_bag_na)
```

```
## RMSE: 0.368
## CORR: 0.429
```

```r
mod_bag_mice &lt;- bagging(income ~ ., data = okcupid2_imp_mice_train, nbagg = 50)
pred_bag_mice &lt;- predict(mod_bag_na, okcupid2_imp_mice_valid)
report_rmse_and_cor(okcupid2_valid$income, pred_bag_mice)
```

```
## RMSE: 0.366
## CORR: 0.435
```

---

class: section-slide

# Random Forests

---

### Random Forests: The Gist

For `\(j= 1\)` to `\(m\)` do
- Generate a bootstrap sample of the original data
- Train an unpruned tree model on this sample
- For each splot:
  - Randomly select `\(m_{try}\)` predictors
  - Select best predictor for split in this subset only

End.

---

What does RF add on top of Bagging?

&lt;img src="images/RF-Trees-1.png" width="80%" /&gt;

---

### RF with `randomForest`

For some reason this implementation of RF won't accept missing values:


```r
library(randomForest)

mod_rf_mice &lt;- randomForest(income ~ ., data = okcupid2_imp_mice_train,
                       mtry = 10, ntree = 50, importance = TRUE)
pred_rf_mice &lt;- predict(mod_rf_mice, okcupid2_imp_mice_valid)
report_rmse_and_cor(okcupid2_valid$income, pred_rf_mice)
```


```
## RMSE: 0.354
## CORR: 0.491
```

For variable importance check out the `importance` field of the RF object:

---

&lt;img src="images/RF-VarImp-1.png" width="70%" /&gt;

.insight[
üí° What is troubling about variable importance?
]
---

### Partial Dependency Plots

1. Select a subset `\(X_s\)` of variables (usually 1-2) you wish to know the relation to the dependent variable `\(y\)`
2. For each possible permutation of `\(X_s\)` run the model on the original data imputing this specific permutation instead of the values of `\(X_s\)`
3. Average predictions of `\(y\)`
4. Plot `\(y\)` vs. `\(X_s\)`

---


```r
partial(mod_rf_mice, pred.var = "age",
        plot = TRUE, plot.engine = "ggplot2") +
  labs(y = "income") +
  theme_classic()
```

&lt;img src = "images/RF-PDP-Age.png" style="width: 50%"&gt;

---

Let's see this for `age` and `height_cm` together (takes some time!):


```r
partial(mod_rf_mice, pred.var = c("age", "height_cm"), plot = TRUE)
```

&lt;img src = "images/RF-PDP-Age-Height.png" style="width: 50%"&gt;

---

class: section-slide

# Gradient Boosted Trees

---

### Boosting: The Gist

Compute the average response `\(\overline y\)`, and use this as the initial predicted value for each sample

For `\(j = 1\)` to `\(m\)` do
- Compute the residual for each observation
- Sample a fraction of the original data
- Fit a regression tree of a certain *"interaction depth"* using the residual as response
- Predict each sample using this tree
- Update the predicted value of each sample by adding the previous iteration's value to current predicted value X *learning rate* or *shrinkage* parameter

End.

---

What you end up with:

`\(\hat{y_i} = \overline{y} + \sum_{j=1}^m \lambda \cdot tree_j(\mbox{x}_i)\)`

That is, GBT (or GBM) in general is called an additive model.

.insight[
üí° Where does "Gradient" come from?
]

- The added random sampling of the data was added later, this version is called *Stochastic* Gradient Boosting, the default for the parameter often called "bagging fraction" is 0.5
- The shrinkage `\(\lambda\)` can be turned into `\(\lambda_j\)`, a unique weight for each added tree
- Note the **many** tuning parameters here. Do you know how to tune multiple params?
- Not so parallel now, eh?

---

### GBT with `gbm`


```r
library(gbm)

mod_gbm_na &lt;- gbm(income ~ ., data = okcupid2_train,
                  distribution = "gaussian",
                  n.trees = 500, shrinkage = 0.1, bag.fraction = 0.5)
pred_gbm_na &lt;- predict(mod_gbm_na, okcupid2_valid, n.trees = 500)
report_rmse_and_cor(okcupid2_valid$income, pred_gbm_na)
```

```
## RMSE: 0.351
## CORR: 0.505
```

```r
mod_gbm_mice &lt;- gbm(income ~ ., data = okcupid2_imp_mice_train,
                  distribution = "gaussian", n.trees = 500, shrinkage = 0.1, bag.fraction = 0.5)
pred_gbm_mice &lt;- predict(mod_gbm_na, okcupid2_imp_mice_valid, n.trees = 500)
report_rmse_and_cor(okcupid2_valid$income, pred_gbm_mice)
```

```
## RMSE: 0.352
## CORR: 0.498
```

For variables importance see the `summary` function:

---

&lt;img src="images/GBM-VarImp-1.png" width="70%" /&gt;


.insight[
üí° Do you see the difference in the profile of importance between RF and GBT?
]
---

### GBT with `xgboost`

`xgboost` is faster and has some additional features, e.g. the ability to train with a validation set until it shows no improvement, similar to neural networks.


```r
library(xgboost)

dtrain &lt;- xgb.DMatrix(data = data.matrix(okcupid2_train[, predictors]),
                      label = okcupid2_train$income)
dval &lt;- xgb.DMatrix(data = data.matrix(okcupid2_valid[, predictors]),
                    label=okcupid2_valid$income)
watchlist &lt;- list(train=dtrain, test=dval)

mod_xgboost_na &lt;- xgb.train(data = dtrain, watchlist=watchlist,
                            objective = "reg:squarederror", nrounds = 100,
                            eta = 0.1, subsample = 0.5,
                            early_stopping_rounds = 10, verbose = 0)
pred_xgboost_na &lt;- predict(mod_xgboost_na, dval)
report_rmse_and_cor(okcupid2_valid$income, pred_xgboost_na)
```

```
## RMSE: 0.357
## CORR: 0.477
```

.warning[
‚ö†Ô∏è Note that `xgboost` only accepts numeric input, that is why we use `data.matrix()`. Perhaps the order of the factors could be improved before they are turned into integers.
]

---

For variables importance see the `xgb.importance` function:

&lt;img src="images/XGBST-VarImp-1.png" width="70%" /&gt;

---

class: section-slide

# The CART (Classification)

---

class: section-slide

# Detour: A Classification Problem

---

### OKCupid: Predicting Dogs or Cats People

It won't be easy:


```r
okcupid %&gt;% count(pets)
```

```
## # A tibble: 16 x 2
##    pets                                n
##    &lt;chr&gt;                           &lt;int&gt;
##  1 dislikes cats                     122
##  2 dislikes dogs                      44
##  3 dislikes dogs and dislikes cats   196
##  4 dislikes dogs and has cats         81
##  5 dislikes dogs and likes cats      240
##  6 has cats                         1406
##  7 has dogs                         4134
##  8 has dogs and dislikes cats        552
##  9 has dogs and has cats            1474
## 10 has dogs and likes cats          2333
## 11 likes cats                       1063
## 12 likes dogs                       7224
## 13 likes dogs and dislikes cats     2029
## 14 likes dogs and has cats          4313
## 15 likes dogs and likes cats       14814
## 16 &lt;NA&gt;                            19921
```

---

We will define cats/dogs people in the following way and stick to non-NA observations:


```r
cats_categories &lt;- c("has cats", "likes cats", "dislikes dogs and has cats",
                     "dislikes dogs and has cats")
dogs_categories &lt;- c("has dogs", "likes dogs", "has dogs and dislikes cats",
                     "likes dogs and dislikes cats")
okcupid3 &lt;- okcupid %&gt;%
  mutate(pets = case_when(
           pets %in% cats_categories ~ "cats",
           pets %in% dogs_categories ~ "dogs",
           TRUE ~ NA_character_)) %&gt;%
  drop_na(pets)

okcupid3 %&gt;% count(pets)
```

```
## # A tibble: 2 x 2
##   pets      n
##   &lt;chr&gt; &lt;int&gt;
## 1 cats   2550
## 2 dogs  13939
```

Notice the classes are very unbalanced, with roughly 11 dogs people to every 2 cats people.


---

As before in the vector `predictors` we have 36 continuous and categorical variables which may or may not be predictive to being a cats/dogs person:



```r
okcupid3 &lt;- okcupid3 %&gt;%
  select(pets, predictors) %&gt;%
  mutate(id = 1:n())

dim(okcupid3)
```

```
## [1] 16489    38
```

And as before we split the data into training, test and validation sets, not shown here:


```
## train no. of rows: 10000
## validation no. of rows: 2489
## test no. of rows: 4000
```

---

Worth exploring relations between predictors and being a cats/dogs person:


```r
var_vs_pets_stackedbar &lt;- function(var) {
  ggplot(okcupid3_train, aes({{var}}, fill = pets)) +
  geom_bar(position = "fill") +
  theme_classic() +
  labs(x = "", y = "") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
}

var_vs_pets_stackedbar(sex)
```

&lt;img src="images/Sex-Pets-1.png" width="80%" /&gt;

---


```r
var_vs_pets_stackedbar(body_type)
```

&lt;img src="images/Body-Type-Pets-1.png" width="100%" /&gt;

---


```r
var_vs_pets_stackedbar(sign2)
```

&lt;img src="images/Sign-Pets-1.png" width="100%" /&gt;

---


```r
var_vs_pets_stackedbar(job3)
```

&lt;img src="images/Job-Pets-1.png" width="100%" /&gt;

---


```r
var_vs_pets_stackedbar(education2)
```

&lt;img src="images/Edu-Pets-1.png" width="100%" /&gt;

---


```r
var_vs_pets_stackedbar(religion2)
```

&lt;img src="images/Religion-Pets-1.png" width="100%" /&gt;

---


```r
var_vs_pets_stackedbar(diet2)
```

&lt;img src="images/Diet-Pets-1.png" width="100%" /&gt;

---


```r
ggpairs(okcupid3_train %&gt;%
          select(essay0_len:essay2_len, pets))
```

&lt;img src="images/Essay-Pets-1.png" width="100%" /&gt;

---

### Baseline: Logistic Regression

As in `lm`, it is better to impute missing values than to lose these observations:


```r
# library(mice)
# mice_obj &lt;- mice(okcupid3, m = 1, maxit = 10, seed = 42)
# okcupid3_imp_mice &lt;- complete(mice_obj)

okcupid3_imp_mice &lt;- read_rds("../data/okcupid3_imp_mice.rds")
okcupid3_imp_mice_train &lt;- okcupid3_imp_mice[train_idx, ]
okcupid3_imp_mice_valid &lt;- okcupid3_imp_mice[valid_idx, ]

mod_glm_mice &lt;- glm(pets ~ ., data = okcupid3_imp_mice_train, family = "binomial")
pred_glm_mice &lt;- predict(mod_glm_mice, okcupid3_imp_mice_valid, type = "response")
```

---

Let's plot the ROC curve:


```r
library(pROC)
roc_obj &lt;- roc(okcupid3_valid$pets, pred_glm_mice)

ggroc(roc_obj) +
  theme_classic() +
  geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), color="darkgrey", linetype="dashed")
```

&lt;img src="images/GLM-ROC-1.png" width="50%" /&gt;

---

Choosing a specific cutoff, the proportion of dogs people in the training sample (the "positive" class), 0.84, let's look at the confusion matrix:


```r
cutoff &lt;- mean(okcupid3_train$pets == "dogs")
pred_class &lt;- ifelse(pred_glm_mice &gt; cutoff, "dogs", "cats")
true_class &lt;- okcupid3_valid$pets

table(true_class, pred_class)
```

```
##           pred_class
## true_class cats dogs
##       cats  238  125
##       dogs  678 1448
```

.insight[
üí° What would be the accuracy? Recall (sensitivity, specificity)? Precision (PPV, NPV)?
]

---

We would like to know the model's AUC, accuracy for a given score cutoff, recall and precision:


```r
report_accuracy_and_auc &lt;- function(obs, pred, cutoff = mean(okcupid3_train$pets == "dogs")) {
  roc_obj &lt;- roc(obs, pred)
  AUC &lt;- as.numeric(auc(roc_obj))
  res &lt;- coords(roc_obj, x = cutoff,
                ret = c("accuracy", "recall", "precision", "specificity", "npv"),
                transpose = TRUE)
  glue("AUC: {format(AUC, digits = 3)}
  ACC: {format(res['accuracy'], digits = 3)}
  Dogs: Recall: {format(res['recall'], digits = 3)}
        Precision: {format(res['precision'], digits = 3)}
  Cats: Recall: {format(res['specificity'], digits = 3)}
        Precision: {format(res['npv'], digits = 3)}")
}

report_accuracy_and_auc(okcupid3_valid$pets, pred_glm_mice)
```

```
## AUC: 0.738
## ACC: 0.677
## Dogs: Recall: 0.681
##       Precision: 0.921
## Cats: Recall: 0.656
##       Precision: 0.26
```

---

#### Baseline: GLMNET: Penalized Logistic Regression


```r
okcupid3_imp_mat_train &lt;- model.matrix( ~ ., okcupid3_imp_mice_train[, predictors])
okcupid3_imp_mat_valid &lt;- model.matrix( ~ ., okcupid3_imp_mice_valid[, predictors])

glmnet_cv &lt;- cv.glmnet(x = okcupid3_imp_mat_train,
                       y = okcupid3_train$pets, family = "binomial")

best_lambda &lt;- glmnet_cv$lambda.min

mod_glm_glmnet &lt;- glmnet(x = okcupid3_imp_mat_train,
                         y = okcupid3_train$pets,
                         family = "binomial", lambda = best_lambda)
pred_glm_glmnet &lt;- predict(mod_glm_glmnet, okcupid3_imp_mat_valid, type = "response")

report_accuracy_and_auc(okcupid3_valid$pets, pred_glm_glmnet[, 1])
```

```
## AUC: 0.738
## ACC: 0.672
## Dogs: Recall: 0.671
##       Precision: 0.924
## Cats: Recall: 0.675
##       Precision: 0.26
```

---

class: section-slide

# End of Detour

---

### The OG CART

1. Find the the predictor to (binary) split on and the value of the split, by `\(Gini\)` (a.k.a impurity) criterion
2. For each resulting node if `\(n_{node} &gt; 20\)` go to 1
3. Once full tree has been grown, perform pruning using the *cost-complexity parameter* `\(c_p\)` and the `\(Gini_{c_p}\)` criterion
4. Predict the proportion of each class at each terminal node, or most common class

---

### The `\(Gini\)` criterion

- `\(y\)` is the discrete dependent variable with `\(J\)` classes
- The `\(Gini_{parent}\)` at a parent node is: `\(\sum_{j=1}^JP(y=j)[1-P(y=j)]=\sum_j p_j(1-p_j)\)`
- a continuous predictor `\(v\)` is nominated for splitting the current node with splitting value `\(l\)`
- such that `\(S_1\)` is the set of observations for which `\(v_i \le l\)`
- and `\(S_2\)` is the set of observations for which `\(v_i &gt; l\)`
- `\(p_{1_j}\)` is the (observed) probability of `\(y\)` equals class `\(j\)` in set `\(S_1\)`

`\(Gini_{split} = \frac{\text{#obs node1}}{\text{#obs parent}}Gini_{node1} + \frac{\text{#obs node2}}{\text{#obs parent}}Gini_{node2}=\)`
`\(\frac{\text{#obs node1}}{\text{#obs parent}}\sum_j p_{1_j}(1-p_{1_j}) + \frac{\text{#obs node2}}{\text{#obs parent}}\sum_j p_{2_j}(1-p_{2_j})\)`

And we find `\(l\)` for which `\(\Delta G = Gini_{parent} - Gini_{split}\)` is greatest.

---

Why Gini?

For `\(J=2\)` classes this means: `\(p_1(1-p_1)+p_2(1-p_2)=2p_1(1-p_1)\)`

The more "impure" the node, the higher this metric:


```r
p &lt;- seq(0, 1, 0.01)
plot(p, 2 * p * (1 - p), type = "l")
```

&lt;img src="images/Gini-2-Classes-1.png" width="45%" /&gt;

---

For example if `religion2` is candidate in splitting `pets`:


```r
gini &lt;- function(l, df, v) {
  pets_above &lt;- df %&gt;% filter({{v}} &gt;= l) %&gt;% pull(pets)
  pets_below &lt;- df %&gt;% filter({{v}} &lt; l) %&gt;% pull(pets)
  p_dogs_above &lt;- mean(pets_above == "dogs")
  gini_above &lt;- 2 * p_dogs_above * (1 - p_dogs_above)
  p_dogs_below &lt;- mean(pets_below == "dogs")
  gini_below &lt;- 2 * p_dogs_below * (1 - p_dogs_below)
  gini_weighted &lt;- (length(pets_above) / nrow(df)) * gini_above +
    (length(pets_below) / nrow(df)) * gini_below
  return(gini_weighted)
}

age &lt;- seq(18,69, 1)
gini_age &lt;- map_dbl(age, gini, df = okcupid3_train, v = age)

p1 &lt;- okcupid3_train %&gt;%
  group_by(age) %&gt;%
  summarise(p_dogs = mean(pets == "dogs"), n = n()) %&gt;%
  ggplot(aes(age, p_dogs)) +
  geom_point(aes(size = n)) +
  theme_classic() +
  labs(x = "") +
  theme(axis.text.x = element_blank())

p2 &lt;- tibble(age = age, gini = gini_age) %&gt;%
  ggplot(aes(age, gini)) +
  geom_line() +
  geom_point() +
  theme_classic()

p1 / p2
```

---

&lt;img src="images/Religion-Gini-1.png" width="80%" /&gt;

---

### CART with `rpart`

Let's tune `\(c_p\)` for our data using a 5-fold (manual) Cross Validation. The criterion to maximize would be AUC.


```r
n_cv &lt;- 5; cp_seq &lt;- seq(0, 0.01, 0.0005)

okcupid3_train_val &lt;- okcupid3_train %&gt;%
  mutate(val = sample(1:n_cv, n(), replace = TRUE))

get_validation_set_auc &lt;- function(i, .cp) {
  ok_tr &lt;- okcupid3_train_val %&gt;% filter(val != i) %&gt;% select(-val)
  ok_val &lt;- okcupid3_train_val %&gt;% filter(val == i) %&gt;% select(-val)
  mod &lt;- rpart(pets ~ ., data = ok_tr,
               control = rpart.control(cp = 0, xval = 1))
  mod &lt;- prune(mod, cp = .cp)
  pred &lt;- predict(mod, ok_val)[, 2]
  roc_obj &lt;- roc(ok_val$pets, pred)
  as.numeric(auc(roc_obj))
}

get_cv_auc &lt;- function(.cp) {
  tibble(cp = rep(.cp, n_cv),
         auc = map_dbl(1:n_cv, get_validation_set_auc, .cp = .cp))
}
```

---


```r
cv_table &lt;- map_dfr(cp_seq, get_cv_auc)

cv_table %&gt;%
  mutate(cp = factor(cp)) %&gt;%
  ggplot(aes(cp, auc)) +
  geom_boxplot() +
  theme_classic() +
  labs(x = "") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```

&lt;img src="images/Cp-CV-B2-1.png" width="100%" /&gt;

---

Training on the entire training set:


```r
mod_tree_na_class &lt;- rpart(pets ~ ., data = okcupid3_train,
                           control = rpart.control(cp = 0, xval = 1))
mod_tree_na_class &lt;- prune(mod_tree_na_class, cp = 0.0015)
```

There's no way to plot this tree nicely...


```r
plot(mod_tree_na_class)
```

&lt;img src="images/CART-NA2-Class-1.png" width="50%" /&gt;

---

Printing the tree it seems like after splitting observations by religion, those who are Christian, Jewish or Hindu have a 88% of being dogs people, and the rest are then split by Status, Body Type, the length of their 9th Essay, etc...

&lt;img src = "images/CART_class_print.png" style="width: 100%"&gt;

---

### Variables Importance

&lt;img src="images/CART-Class-VarImp-1.png" width="60%" /&gt;

---

### Prediction


```r
pred_tree_na_class &lt;- predict(mod_tree_na_class, okcupid3_valid)

report_accuracy_and_auc(okcupid3_valid$pets, pred_tree_na_class[, 2])
```

```
## AUC: 0.612
## ACC: 0.788
## Dogs: Recall: 0.87
##       Precision: 0.881
## Cats: Recall: 0.309
##       Precision: 0.289
```

As expected, far from impressive.

---

Let's try using the imputed `NA` data:


```r
mod_tree_imp_class &lt;- rpart(pets ~ ., data = okcupid3_imp_mice_train,
                            control = rpart.control(cp = 0, xval = 1))
mod_tree_imp_class &lt;- prune(mod_tree_imp_class, cp = 0.0015)

pred_tree_imp_class &lt;- predict(mod_tree_imp_class, okcupid3_imp_mice_valid)

report_accuracy_and_auc(okcupid3_valid$pets, pred_tree_imp_class[, 2])
```

```
## AUC: 0.672
## ACC: 0.748
## Dogs: Recall: 0.793
##       Precision: 0.9
## Cats: Recall: 0.485
##       Precision: 0.285
```
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="../libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
