<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Convolutional Neural Networks</title>
    <meta charset="utf-8" />
    <meta name="author" content="Giora Simchoni" />
    <meta name="date" content="2020-06-10" />
    <head>
      <link rel="icon" href="../DSApps_logo.jpg" type="image/jpg"> 
      <link rel="shortcut icon" href="../DSApps_logo.jpg" type="image/jpg">
    </head>
    <link rel="stylesheet" href="..\slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">


class: logo-slide

---

class: title-slide

### Convolutional Neural Networks

### Applications of Data Science - Class 17

### Giora Simchoni

#### `gsimchoni@gmail.com and add #dsapps in subject`

### Stat. and OR Department, TAU
### 2020-06-10

---



layout: true

&lt;div class="my-footer"&gt;
  &lt;span&gt;
    &lt;a href="https://dsapps-2020.github.io/Class_Slides/" target="_blank"&gt;Applications of Data Science
    &lt;/a&gt;
  &lt;/span&gt;
&lt;/div&gt;

---





class: section-slide

# Understanding Images

---


```python
import matplotlib.pyplot as plt
from matplotlib.image import imread

logo = imread('../DSApps_logo.jpg')

plt.imshow(logo)
plt.show()
```

&lt;img src="images/Logo-1.png" width="60%" /&gt;

---


```python
print(type(logo))
```

```
## &lt;class 'numpy.ndarray'&gt;
```

```python
print(logo.shape)
```

```
## (1400, 1400, 3)
```

```python
print(logo[:4, :4, 0])
```

```
## [[6 6 6 6]
##  [6 6 6 6]
##  [6 6 6 6]
##  [6 6 6 6]]
```

```python
print(logo.min(), logo.max())
```

```
## 0 255
```

```python
print(logo.dtype, logo.size * logo.itemsize)
```

```
## uint8 5880000
```

---

### Red channel

.pull-left[


```python
red = logo[:, :, 0]
plt.imshow(red,
  cmap='gray')
plt.show()
```

&lt;img src="images/Logo-Red-1.png" width="100%" /&gt;

]

.pull-right[

```python
fig = plt.hist(red.flatten(),
  color = 'r')
fig = plt.ylim(0, 1700000)
plt.show()
```

&lt;img src="images/Logo-Red-Hist-1.png" width="100%" /&gt;

]

---

### Green channel

.pull-left[


```python
green = logo[:, :, 1]
plt.imshow(green,
  cmap='gray')
plt.show()
```

&lt;img src="images/Logo-Green-1.png" width="100%" /&gt;

]

.pull-right[

```python
fig = plt.hist(green.flatten(),
  color = 'g')
fig = plt.ylim(0, 1700000)
plt.show()
```

&lt;img src="images/Logo-Green-Hist-1.png" width="100%" /&gt;

]

---

### Blue channel

.pull-left[


```python
blue = logo[:, :, 2]
plt.imshow(blue,
  cmap='gray')
plt.show()
```

&lt;img src="images/Logo-Blue-1.png" width="100%" /&gt;

]

.pull-right[

```python
fig = plt.hist(blue.flatten(),
  color = 'b')
fig = plt.ylim(0, 1700000)
plt.show()
```

&lt;img src="images/Logo-Blue-Hist-1.png" width="100%" /&gt;

]

---

### Until Convolutional Networks

.insight[
ðŸ’¡ So how many features is that? ðŸ˜† ðŸ˜† ðŸ˜†
]

- In 1998 LeCun et. al. published [LeNet-5](https://ieeexplore.ieee.org/abstract/document/726791) for digit recognition
- But it wasn't until 2012 when Alex Krizhevsky, Ilya Sutskever and Geoffrey Hinton published [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) when the Deep Learning mania really took off.

Until then:

1. Treat it as a regular high-dimensional ML problem
2. Image feature engineering

---

class: section-slide

# Convolutional Layer (2D)

---

### What is Convolution?

&lt;img src="images/convolution_wiki.png" style="width: 100%" /&gt;

---

### The Convolutional Layer

&lt;img src="images/conv01.png" style="width: 60%" /&gt;

.font80percent[Source: [Geron 2019](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)]

- First layer: every neuron has a "receptive field", it is focused on a specific rectangle of the image, usually 2x2, 3x3
- Second layer: every neuron has a receptive field in the first layer
- Etc.

---

### More specifically

&lt;img src="images/conv02.png" style="width: 60%" /&gt;

.font80percent[Source: [Geron 2019](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)]

- The `\([i,j]\)` neuron looks at the rectangle at rows `\(i\)` to `\(i + f_h - 1\)`, columns `\(j\)` to `\(j + f_w - 1\)`
- Zero Padding: adds 0s around the image to make the next layer the same dimensions

---

### Filters

But what does the neuron actually *do*?

All neurons in a layer learn a single *filter* the size of their receptive field, the `\(f_h, f_w\)` rectangle.

Suppose `\(f_h=f_w=3\)` and the first layer learned the `\(W_{3x3}\)` filter:


```python
W = np.array(
  [
    [0, 1, 0],
    [0, 1, 0],
    [0, 1, 0]
  ]
)
```

---

`\(X\)` is the 5x5 image, suppose it has a single color channel (i.e. grayscale), sort of a smiley:


```python
X = np.array(
  [
    [0,   1,  0,  1,  0],
    [0,   1,  0,  1,  0],
    [0,   0,  0,  0,  0],
    [1,   0,  0,  0,  1],
    [0,   1,  1,  1,  0]
  ]
)
```

Each nuron in the new layer `\(Z\)` would be the sum of elementwise multiplication of all 3x3 pixels/neurons in its receptive field with `\(W_{3x3}\)`:

`\(Z_{i,j} = b + \sum_{u=0}^{f_h-1}\sum_{v=0}^{f_w-1}X_{i+u,j+v}W_{u,v}\)`

---

### Good God, Excel?

&lt;img src="images/conv03.png" style="width: 100%" /&gt;

.insight[
ðŸ’¡ What low-level feature did this layer learn to look for? What pattern will make its neurons most positive (i.e. will "turn them on")?
]

---

### Another filter

&lt;img src="images/conv04.png" style="width: 100%" /&gt;

.insight[
ðŸ’¡ What low-level feature did this layer learn to look for? What pattern will make its neurons most positive (i.e. will "turn them on")?
]

---

### Convolving with Tensorflow


```python
import tensorflow as tf

ny = imread('images/new_york.jpg').mean(axis=2)
ny4D = np.array([ny.reshape(ny.shape[0], ny.shape[1], 1)])
W4d = W.reshape(W.shape[0], W.shape[0], 1, 1)

ny_convolved = tf.nn.conv2d(ny4D, W4d, strides=1, padding='SAME')
```

---

.pull-left[


```python
plt.imshow(
  ny,
  cmap = 'gray')
plt.show()
```

&lt;img src="images/NY-Orig-1.png" width="100%" /&gt;

]

.pull-right[


```python
plt.imshow(
  ny_convolved[0, :, :, 0],
  cmap = 'gray')
plt.show()
```

&lt;img src="images/NY-Convolved-1.png" width="100%" /&gt;

]

---

### Strides

In many CNN architechtures layers tend to get smaller and smaller using *strides*:

&lt;img src="images/conv05.png" style="width: 60%" /&gt;

- The `\([i,j]\)` neuron looks at the rectangle at rows `\(i \cdot s_h\)` to `\(i \cdot s_h + f_h - 1\)`, columns `\(j \cdot s_w\)` to `\(j \cdot s_w + f_w - 1\)`

---

### Stacking Feature Maps

A convolutional layer is actually a 3D stack of a few of the 2D layers we described (a.k.a *Feature Maps*), each learns a single filter.

&lt;img src="images/conv06.png" style="width: 60%" /&gt;

.font80percent[Source: [Geron 2019](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)]

---

- Feature map 1 learns horizontal lines
- Feature map 2 learns vertical lines
- Feature map 3 learns diagonal lines
- Etc.

And each such feature map takes as inputs all feature maps (or color channels) in the previous layer, and sums:

`\(Z_{i,j,k} = b_k + \sum_{u=0}^{f_h-1}\sum_{v=0}^{f_w-1}\sum_{k'=0}^{f_n-1}X_{i \cdot s_h+u,j \cdot s_w+v, k'} \cdot W_{u,v,k', k}\)`

Where `\(f_n\)` is the number of feature maps (or color channels) in the previous layer.

- Feature map 1 with 3x3 filter x 3 color channels + 1 bias term = 28 params
- Feature map 2 with 3x3 filter x 3 color channels+ 1 bias term = 28 params
- Etc.

---

### And how does the network *learn* these filters?

---

### Pooling Layers

---

### Why does this work?

- No longer a long 1D column of neurons, but 2D, taking into accout spatial realtions between pixels/neurons
- First layer learns very low-level features, second layer learns higher level features, etc.
- Shared weights - learn feature in one area of the image, generalize it to the entire image
- Less weights --&gt; smaller size, more feasible model, less prone to overfitting

---

### Too early to rejoice

That's still quite a lot.

- 100 x 100 RGB image
- 100 feature maps in first layer of filter 3x3
- (3 x 3 x 3 + 1) x 100 = 2800 params, not too bad
- But 100 x 100 numbers in each feature map (x 100) = 1M numbers, each say takes 4B, that's 4MB for 1 image for 1 layer
- Each number is a weighted sum of 3 x 3 x 3 = 27 numbers, so that's 1M x 27 = 27M multiplications for 1 layer...

---

class: section-slide

# Back to Malaria!

---

class: section-slide

# CNN Architectures

---

class: section-slide

# Using Pre-trained Models
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="../libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
