<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Topics in Regression</title>
    <meta charset="utf-8" />
    <meta name="author" content="Giora Simchoni" />
    <meta name="date" content="2020-04-06" />
    <head>
      <link rel="icon" href="../DSApps_logo.jpg" type="image/jpg"> 
      <link rel="shortcut icon" href="../DSApps_logo.jpg" type="image/jpg">
    </head>
    <link rel="stylesheet" href="..\slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">


class: logo-slide

---

class: title-slide

## Topics in Regression

### Applications of Data Science - Class 14

### Giora Simchoni

#### `gsimchoni@gmail.com and add #dsapps in subject`

### Stat. and OR Department, TAU
### 2020-04-06

---



layout: true

&lt;div class="my-footer"&gt;
  &lt;span&gt;
    &lt;a href="https://dsapps-2020.github.io/Class_Slides/" target="_blank"&gt;Applications of Data Science
    &lt;/a&gt;
  &lt;/span&gt;
&lt;/div&gt;

---



class: section-slide

# MARS
#### (Multivariate Adaptive Regression Splines)
---

### What's the No. 1 complaint about LR?

&lt;img src="images/Bad-LR-1.png" width="50%" /&gt;

`\(\hat{y} = 12 + 1.5x\)`

---

### It would have been much nicer if we could...

&lt;img src="images/LR-Good-1.png" width="50%" /&gt;

`$$\hat{y} = [9x]I(x &lt; 3) + [33 - 2x]I(x \gt 3) = \\
= 27 - 9\max(0, 3 - x) - 2\max(0, x - 3) = \\
27 - 9[-(x-3)]_+ - 2[+(x-3)]_+$$`

---

### The Hinge Function

`\(h(x - c) = [(x - c)]_+ = max(0, x - c)\)`

Where `\(c\)` would be called a knot.

&lt;img src="images/Hinge-1.png" width="40%" /&gt;

.insight[
💡 If you know about Deep Learning this would be called...
]

---

### MARS via `earth`




```r
library(earth)

mod_mars &lt;- earth(y ~ x, degree = 1)

summary(mod_mars)
```

```
## Call: earth(formula=y~x, degree=1)
## 
##             coefficients
## (Intercept)    26.902230
## h(3.1-x)       -8.403566
## h(x-3.1)       -1.995450
## 
## Selected 3 of 3 terms, and 1 of 1 predictors
## Termination condition: RSq changed by less than 0.001 at 3 terms
## Importance: x
## Number of terms at each degree of interaction: 1 2 (additive model)
## GCV 1.319067    RSS 95.37498    GRSq 0.9715939    RSq 0.9743302
```

.insight[
💡 Why on earth is the package for MARS called... 🤦
]

---

### This is exactly like


```r
X &lt;- data.frame(y, h1 = pmax(0, x - 3.1), h2 = pmax(0, 3.1 - x))

head(X)
```

```
##           y h1  h2
## 1 1.3709584  0 3.1
## 2 0.3353018  0 3.0
## 3 2.1631284  0 2.9
## 4 3.3328626  0 2.8
## 5 4.0042683  0 2.7
## 6 4.3938755  0 2.6
```

```r
lm(y ~ h1 + h2, data = X)
```

```
## 
## Call:
## lm(formula = y ~ h1 + h2, data = X)
## 
## Coefficients:
## (Intercept)           h1           h2  
##      26.902       -1.995       -8.404
```

---

### And which method knows how to scan X for the best partition?

&lt;img src="images/a_witch.jpg" style="width: 80%" /&gt;

---

### What's the No. 1 complaint about Trees?

&lt;img src="images/Bad-Trees-1.png" width="50%" /&gt;

---

### Why are CART also called "Recursive Partitioning"?

&lt;img src="images/mars.png" style="width: 100%" /&gt;

---

### From CART to MARS

Let `\(B_m(X)= \prod_{k=1}^{K_m}I(X_{v(km)} - t_{km} \ge 0)\)`

Following [Friedman (1991)](https://projecteuclid.org/euclid.aos/1176347963), let:

`$$H(z) =
  \begin{cases}
   1 &amp; \mbox{if } z \ge 0 \\
   0 &amp; \mbox{otherwise}
 \end{cases}$$`

So: `\(B_m(X)= \prod_{k=1}^{K_m}H[s_{km}(X_{v(km)} - t_{km})]\)` where `\(s_{km}=\pm 1\)`

And: `\(\hat{y} = g(X)\)`

And: `\(LOF(g)\)` is the Lack of Fit function for `\(g(X)\)`, typically the `\(SSE\)` critetion

And: Switch all our `\(\beta\)`s to `\(\alpha\)`s

---

Then the growing of trees algorithm is really...

&lt;img src="images/cart_algo1.png" style="width: 100%" /&gt;

---

### Why gotta be H?

&lt;img src="images/mars_algo1.png" style="width: 100%" /&gt;

---

### More than H changed on the way to MARS

* Most important: NOT removing the parent `\(B_{m*}(X)\)`, we can always go back to the parent and split it *again*, allowing for additive models such as `\(\hat{y} = 6 + 5[-(x_1-4)]_+ - 2[+(x_2-3)]_+\)`

.insight[
💡 Can you do this with a tree?
]

* The generalized form is actually `\([\pm(x-t)]^q_+\)` where `\(q\)` is usually 1 but *can be* 2 or 3

.insight[
💡 What if `\(q=0\)`?
]

---

* At each step of adding to `\(B_{m}(X)\)` only variables which have not yet participated in this path are added

.insight[
💡 How is it different from trees and why do you think this was added?
]

* For "growing" and "pruning" the `\(LOF(g)\)` used by MARS is `\(GCV\)` (Generalized Cross Validation) which is a penalized version of `\(SSE\)` which is out of scope

* Easily extensible to GLM via the proper link function, e.g. for Logistic Regression: `\(logit(p) = \sum_k \beta_k B_k\)`

* And more...

---

### "Pruning" MARS

Another advantage of MARS is that once the full model has been "grown", **each and every** term is candidate for pruning from the formula, not whole pairs:

&lt;img src="images/mars_algo2.png" style="width: 60%" /&gt;

The forward pass adds terms in pairs, but the pruning (backward pass) typically discards one side of the pair and so terms are often not seen in pairs in the final model.

---

### Predicting OKCupid Income




```r
mod_mars &lt;- earth(income ~ ., data = okcupid2_imp_mice_train, degree = 1)

summary(mod_mars, digits = 2)
```

```
## Call: earth(formula=income~., data=okcupid2_imp_mice_train, degree=1)
## 
##                           coefficients
## (Intercept)                     -0.011
## sexm                             0.062
## body_typeathletic                0.042
## body_typejacked                  0.157
## body_typeusedup                  0.251
## body_type_not_perfectTRUE       -0.058
## diet2kosher                      0.194
## diet2other                       0.065
## drinksnotatall                  -0.350
## drinksoften                     -0.235
## drinksrarely                    -0.330
## drinkssocially                  -0.272
## drinksveryoften                 -0.199
## drugssometimes                  -0.049
## religion2christian              -0.042
## education2degree2                0.078
## education2degree3                0.114
## education2high_school           -0.166
## education2student0              -0.213
## education2student1              -0.115
## education2student3              -0.135
## job3financial                    0.197
## job3hardware                     0.185
## job3health                       0.072
## job3legalservices                0.181
## job3management                   0.224
## job3marketing                    0.091
## job3retired                      0.196
## job3student                     -0.100
## job3tech                         0.150
## job3unemployed                  -0.189
## orientationstraight              0.056
## h(age-30)                        0.037
## h(32-age)                       -0.009
## h(age-32)                       -0.037
## h(height_cm-203.2)               0.023
## h(1.39029-essay4_len)           -0.317
## h(essay4_len-1.39029)           -0.015
## h(essay5_len-7.29167)            0.358
## h(1.10393-essay6_len)            0.498
## 
## Selected 40 of 44 terms, and 36 of 114 predictors
## Termination condition: RSq changed by less than 0.001 at 44 terms
## Importance: age, body_type_not_perfectTRUE, job3hardware, job3tech, ...
## Number of terms at each degree of interaction: 1 39 (additive model)
## GCV 0.12    RSS 948    GRSq 0.25    RSq 0.26
```

---

&lt;img src="images/earth_summary_cont.png" style="width: 90%" /&gt;


```r
pred_mars &lt;- predict(mod_mars, okcupid2_imp_mice_valid)
report_rmse_and_cor(okcupid2_imp_mice_valid$income, pred_mars)
```

```
## RMSE: 0.357
## CORR: 0.481
```

---

### Pros and Cons

Pros:

- Flexible, built-in interactions
- Highly interpretable
- Little pre-processing of predictors needed
- Handle all types of predictors (continuous, categorical)
- Feature selection built-in (but when predictors are correlated...)
- Good variance-bias tradeoff

Cons:
- Greedy
- The resulting fitted function may not be smooth (not differentiable along hinges)
- No built-in CI for variables params

---

class: section-slide

# CCA
#### (Canonical Correlation Analysis)

---

### What is Linear Regression, really?

&lt;img src="images/LR_geometric_interp.png" style="width: 60%" /&gt;

.font80percent[
([ESL II](https://web.stanford.edu/~hastie/Papers/ESLII.pdf))
]

`\(y = X\beta + \epsilon \rightarrow \hat{y} = X\hat\beta = X(X'X)^{-1}X'y = Hy\)`

"We project `\(y\)` to the space spanned by `\(X\)`..."

"We seek a linear combination of `\(X\)` that would be closest to `\(y\)`..."

---

### What if `\(y\)` is `\(Y\)`?

If `\(Y\)` is multi-dimensional (not a single vector, but a matrix of `\(nxq\)` dependent variables), we get the same result with a `\(\hat{B}_{pxq}\)` matrix:

`\(Y = XB + E \rightarrow \hat{Y} = X\hat{B} = X(X'X)^{-1}X'Y = HY\)`

And if the columns of `\(X\)` are linearly independent this is equivalent to `\(q\)` separate linear regressions:


```r
n &lt;- 1000
x1 &lt;- rnorm(n)
x2 &lt;- rnorm(n)
x3 &lt;- rnorm(n)

y1 &lt;- 0 + 1 * x1 + 2 * x2 + 3 * x3 + rnorm(n)
y2 &lt;- 1 + 2 * x1 + 3 * x2 + 4 * x3 + rnorm(n)
Y &lt;- cbind(y1, y2)

lm(Y ~ x1 + x2 + x3)
```

---


```
## 
## Call:
## lm(formula = Y ~ x1 + x2 + x3)
## 
## Coefficients:
##              y1        y2      
## (Intercept)  -0.05197   1.00172
## x1            1.02041   2.02414
## x2            2.07436   3.01918
## x3            2.98838   3.96748
```

Though harder to interpret geometrically, it isn't that different:

"We project each of `\(Y\)`'s columns to the space spanned by `\(X\)`..."

"We seek a few linear combinations of `\(X\)` that would be closest to each of `\(Y\)`'s columns..."

➡️ But in case of multi-dimensional `\(Y\)` could we not find linear combinations **of `\(Y\)`** that would be closest to linear combinations of `\(X\)`?

---

### What is Pearson correlation coefficient, really?

`$$r=Corr(x, y) = \frac{Cov(x, y)}{\sqrt{Var(x)}\sqrt{Var(y)}} =\frac{\sum(x_i - \bar x)(y_i - \bar y)}{\sqrt{\sum(x_i - \bar x)^2}\sqrt{\sum(y_i - \bar y)^2}}$$`

If we assume `\(x\)` and `\(y\)` are standardized (centered and scaled) to have mean 0 and variance `\(\frac{1}{n - 1}\)` (they have a L2 norm of 1), we could write:

`\(r = \sum x_i y_i \space s.t. \space \sum x_i = 0;\sum y_i = 0; \sum x^2_i=1; \sum y^2_i=1\)`

Or:

`\(r = x'y \space s.t. \space \sum x_i = 0;\sum y_i = 0; x'x=1; y'y=1\)`

---


```r
center_vec &lt;- function(x) x - mean(x)
norm_vec &lt;- function(x) sqrt(sum(x^2))
x_scaled &lt;- center_vec(x1) / norm_vec(center_vec(x1))
y_scaled &lt;- center_vec(y1) / norm_vec(center_vec(y1))

glue::glue("pearson: {format(cor(x1, y1), digits = 3)}
x'y: {format(t(x_scaled) %*% y_scaled, digits = 3)}")
```

```
## pearson: 0.246
## x'y: 0.246
```

So the Pearson's `\(r\)` can be viewed as the dot product between `\(x\)` and `\(y\)`, when scaled.

And the dot product between two vectors with L2 norm of 1 is actually the cosine of the angle between them. The higher the correlation, the larger the cosine, the smaller the angle between them, which gives another interpretation for correlation.

---

### What does correlation have to do with regression?

`\(y = a + bx + \epsilon \rightarrow y = X\beta + \epsilon\)`

And we're intereseted in `\(\hat{b}\)` which is `\(\hat \beta_2\)`.

`$$\hat \beta = (X'X)^{-1}X'y = \\
(\begin{bmatrix}
1 &amp; ... &amp; 1\\
x_1 &amp; ... &amp; x_n
\end{bmatrix} \begin{bmatrix}
1 &amp; x_1\\
\vdots &amp; \vdots \\
1 &amp; x_n
\end{bmatrix})^{-1} \begin{bmatrix}
1 &amp; ... &amp; 1\\
x_1 &amp; ... &amp; x_n
\end{bmatrix} \begin{bmatrix}
y_1\\
\vdots \\
y_n
\end{bmatrix}$$`

It's not that hard to get that:

`\(\hat{b} = \hat \beta_2 = \frac{\sum(x_i - \bar x)(y_i - \bar y)}{\sum(x_i - \bar x)^2}\)`

But that means...

---

`$$\hat{b} = \hat \beta_2 = \frac{\sum(x_i - \bar x)(y_i - \bar y)\sqrt{\sum(y_i - \bar y)^2}}{\sum(x_i - \bar x)^2\sqrt{\sum(y_i - \bar y)^2}} \\
=\frac{\sum(x_i - \bar x)(y_i - \bar y)\sqrt{\sum(y_i - \bar y)^2}}{\sqrt{\sum(x_i - \bar x)^2}\sqrt{\sum(y_i - \bar y)^2}\sqrt{\sum(x_i - \bar x)^2}} \\
=\frac{Cov(x, y)}{\sqrt{Var(x)}\sqrt{Var(y)}}\frac{s_y}{s_x} = r\frac{s_y}{s_x}$$`

So the Pearson's `\(r\)` is the regression coefficient when regressing `\(y\)` against `\(x\)`, and vice versa, times a different factor! .font80percent[(And the same if they are standardized to have the same variance)]




```r
lm_yx &lt;- lm(y1 ~ x1); lm_xy &lt;- lm(x1 ~ y1); s_x = sd(x1); s_y = sd(y1)

glue::glue("pearson: {format(cor(x1, y1), digits = 3)}
b_y * s_x / s_y: {format(lm_yx$coef[2] * s_x/s_y, digits = 3)}; b_x * s_y / s_x: {format(lm_xy$coef[2] * s_y/s_x, digits = 3)}")
```

```
## pearson: 0.246
## b_y * s_x / s_y: 0.246; b_x * s_y / s_x: 0.246
```

---

### And now, CCA

- Let `\(X_{nxp}\)` be a matrix of `\(n\)` observations with `\(p\)` random variables and `\(Y_{nxq}\)` matrix of the same `\(n\)` observations with `\(q\)` different random variables.

- For example, `\(X\)` represents `\(n\)` people answering `\(p\)` questions in questionnaire A measuring stress, and `\(Y\)` represents the same `\(n\)` people answering `\(q\)` in questionnaire B also measuring stress.

- Canonical Correlation Analysis seeks to find linear combinations of `\(Y\)` and of `\(X\)` that would be "closest" in the common space.

- Whether you understand "closest" via correlation (small angle) or via "regression" (small sum of squares) it is the same thing.

- A typical result would be `\(Corr(x_1, \frac{y_1+y_2+y_3}{3})=0.8\)`, meaning question 1 of questionnaire A is "very" correlated with the average of questions 1, 2 and 3 of questionnaire B.

---

We want a "viewpoint" `\(u\)` to look at high-dimensional `\(X\)`, and a "viewpoint" `\(v\)` to look at high-dimensional `\(Y\)`, such that:

`\((u, v) = \text{argmax}_{u,v}{Corr(Xu, Yv)} = \text{argmax}_{u,v}{\frac{Cov(Xu, Yv)}{\sqrt{Var(xu)}\sqrt{Var(Yv)}}}\)`

Writing `\(Corr(Xu, Yv)\)` as we did:

`\((u, v)= \text{argmax}_{u,v}{Corr(Xu, Yv)} = \text{argmax}_{u,v}u^tX^tYv\)`
`\(\text{ s.t. }\sum{Xu_i}=0;\sum{Yv_i}=0;u^tX^tXu=1;v^tY^tYv=1\)`

Applying the Lagrange multipliers technique to this optimization gives:

`\(L = u^tX^tYv - \frac{\theta}{2}(u^tX^tXu - 1) - \frac{\tau}{2}(v^tY^tYv - 1)\rightarrow max_{u,v}\)`

(where the `\(\sum{Xu_i}=0\)` terms are dropped since they won't have an effect on `\(u\)` and `\(v\)`)

---

Taking derivatives with respect to `\(u\)` and `\(v\)` and comparing to 0 we obtain the equations:
`$$(1) \space X'Yv - \theta X'Xu = 0 \\
(2) \space Y'Xu - \tau Y'Yv = 0 $$

Multiplying equation (1) by `\(u\)`, and equation (2) by `\(v\)`, and subtracting equation (2) from equation (1) we get: `\(\theta u'X'Xu = \tau v'Y'Yv\)`

However, `\(u'X'Xu = v'Y'Yv = 1\)`, so we get:

`\(\theta = \tau = u'X'Yv = Corr(Xu, Yv) = r\)`

Equations (1) and (2) now become:
`$$(3) \space X'Yv - rX'Xu = 0 \\
(4) \space Y'Xu - rY'Yv = 0 $$

I we assume `\(Y'Y\)` is invertible we can write from (4):

`\((5) \space v = \frac{1}{r}(Y'Y)^{-1}Y'Xu\)`

---

Substituting for `\(v\)` in equation (3):

`\((X'X)^{-1}X'Y(Y'Y)^{-1}Y'Xu = r^2u\)`

If we write `\(A = (X'X)^{-1}X'Y(Y'Y)^{-1}Y'X\)` we see that:

`\(Au = r^2u\)`

is a simple eigenproblem.

Here the first `\(k\)` canonical correlations `\(r_1, ..., r_k\)` are the square roots of the first ordered `\(k\)` eigenvalues of the matrix `\(A = (X'X)^{-1}X'Y(Y'Y)^{-1}Y'X\)` and the first `\(k\)` weight vectors `\(u_1, ..., u_k\)` are the associated eigenvectors.

Similarly, `\(v_1, ..., v_k\)` are the eigenvectors of the matrix `\(B = (Y'Y)^{-1}Y'X(X'X)^{-1}X'Y\)`, or we can put in (5) any `\(u_i\)` and `\(r_i\)`, to give `\(v_i\)`.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="../libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
