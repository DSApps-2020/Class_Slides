---
title: "Topics in Regression"
subtitle: "Applications of Data Science"
author: "Giora Simchoni"
institute: "Stat. and OR Department, TAU"
date: "`r Sys.Date()`"
output_dir: "images"
output:
  xaringan::moon_reader:
    css: "../slides.css"
    seal: false
    chakra: "../libs/remark-latest.min.js"
    includes:
      in_header: "../header.html"
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

class: logo-slide

---

class: title-slide

## Topics in Regression

### Applications of Data Science - Class 14

### Giora Simchoni

#### `gsimchoni@gmail.com and add #dsapps in subject`

### Stat. and OR Department, TAU
### `r Sys.Date()`

---
```{r child = "../setup.Rmd"}
```

```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
```

class: section-slide

# MARS
#### (Multivariate Adaptive Regression Splines)
---

### What's the No. 1 complaint about LR?

```{r Bad-LR, echo=FALSE, out.width="50%"}
x1 <- seq(0, 3, 0.1)
y1 <- 9 * x1 + rnorm(length(x1), sd = 3)
x2 <- seq(3, 8, 0.1)
y2 <- 33 - 2 * x2 + rnorm(length(x2), sd = 3)
x <- c(x1, x2)
y <- c(y1, y2)
lm1 <- lm(y ~ x)
plot(x, y)
abline(lm1$coef, col = "red")
```

$\hat{y} = 12 + 1.5x$

---

### It would have been much nicer if we could...

```{r LR-Good, echo=FALSE, out.width="50%"}
plot(x, y)
lines(x1, 9 * x1, col = "red")
lines(x2, 33 - 2 * x2, col = "red")
```

$$\hat{y} = [9x]I(x < 3) + [33 - 2x]I(x \gt 3) = \\
= 27 - 9\max(0, 3 - x) - 2\max(0, x - 3) = \\
27 - 9[-(x-3)]_+ - 2[+(x-3)]_+$$

---

### The Hinge Function

$h(x - c) = [(x - c)]_+ = max(0, x - c)$

Where $c$ would be called a knot.

```{r Hinge, out.width="40%", echo = FALSE}
x1 <- seq(0, 8, 0.1)
plot(x1, pmax(0, x1 - 3), type = "l", main = "h(x-3)", xlab = "x", ylab = "h(x-3)")
```

.insight[
`r emo::ji("bulb")` If you know about Deep Learning this would be called...
]

---

### MARS via `earth`

```{r, echo=FALSE}
set.seed(42)
x1 <- seq(0, 3, 0.1)
y1 <- 9 * x1 + rnorm(length(x1), sd = 1)
x2 <- seq(3, 8, 0.1)
y2 <- 33 - 2 * x2 + rnorm(length(x2), sd = 1)
x <- c(x1, x2)
y <- c(y1, y2)
```

```{r, warning=FALSE, message=FALSE}
library(earth)

mod_mars <- earth(y ~ x, degree = 1)

summary(mod_mars)
```

.insight[
`r emo::ji("bulb")` Why on earth is the package for MARS called... `r emo::ji("person_facepalming")`
]

---

### This is exactly like

```{r}
X <- data.frame(y, h1 = pmax(0, x - 3.1), h2 = pmax(0, 3.1 - x))

head(X)

lm(y ~ h1 + h2, data = X)
```

---

### And which method knows how to scan X for the best partition?

<img src="images/a_witch.jpg" style="width: 80%" />

---

### What's the No. 1 complaint about Trees?

```{r Bad-Trees, echo=FALSE, out.width="50%"}
plot(x, y)
lines(x1, rep(mean(y1), length(x1)), col = "red")
lines(x2, rep(mean(y2), length(x2)), col = "red")
```

---

### Why are CART also called "Recursive Partitioning"?

<img src="images/mars.png" style="width: 100%" />

---

### From CART to MARS

Let $B_m(X)= \prod_{k=1}^{K_m}I(X_{v(km)} - t_{km} \ge 0)$

Following [Friedman (1991)](https://projecteuclid.org/euclid.aos/1176347963), let:

$$H(z) =
  \begin{cases}
   1 & \mbox{if } z \ge 0 \\
   0 & \mbox{otherwise}
 \end{cases}$$

So: $B_m(X)= \prod_{k=1}^{K_m}H[s_{km}(X_{v(km)} - t_{km})]$ where $s_{km}=\pm 1$

And: $\hat{y} = g(X)$

And: $LOF(g)$ is the Lack of Fit function for $g(X)$, typically the $SSE$ critetion

And: Switch all our $\beta$s to $\alpha$s

---

Then the growing of trees algorithm is really...

<img src="images/cart_algo1.png" style="width: 100%" />

---

### Why gotta be H?

<img src="images/mars_algo1.png" style="width: 100%" />

---

### More than H changed on the way to MARS

* Most important: NOT removing the parent $B_{m*}(X)$, we can always go back to the parent and split it *again*, allowing for additive models such as $\hat{y} = 6 + 5[-(x_1-4)]_+ - 2[+(x_2-3)]_+$

.insight[
`r emo::ji("bulb")` Can you do this with a tree?
]

* The generalized form is actually $[\pm(x-t)]^q_+$ where $q$ is usually 1 but *can be* 2 or 3

.insight[
`r emo::ji("bulb")` What if $q=0$?
]

---

* At each step of adding to $B_{m}(X)$ only variables which have not yet participated in this path are added

.insight[
`r emo::ji("bulb")` How is it different from trees and why do you think this was added?
]

* For "growing" and "pruning" the $LOF(g)$ used by MARS is $GCV$ (Generalized Cross Validation) which is a penalized version of $SSE$ which is out of scope

* Easily extensible to GLM via the proper link function, e.g. for Logistic Regression: $logit(p) = \sum_k \beta_k B_k$

* And more...

---

### "Pruning" MARS

Another advantage of MARS is that once the full model has been "grown", **each and every** term is candidate for pruning from the formula, not whole pairs:

<img src="images/mars_algo2.png" style="width: 60%" />

The forward pass adds terms in pairs, but the pruning (backward pass) typically discards one side of the pair and so terms are often not seen in pairs in the final model.

---

### Predicting OKCupid Income

```{r, echo=FALSE}
okcupid2_imp_mice <- read_rds("../data/okcupid2_imp_mice.rds")
idx <- read_rds("../data/okcupid2_idx.rda")
train_idx <- idx$train_idx
valid_idx <- idx$valid_idx
test_idx <- idx$test_idx

okcupid2_imp_mice_train <- okcupid2_imp_mice[train_idx, ]
okcupid2_imp_mice_valid <- okcupid2_imp_mice[valid_idx, ]

rmse <- function(obs, pred) sqrt(mean((obs - pred)^2))

report_rmse_and_cor <- function(obs, pred) {
  RMSE <- rmse(obs, pred)
  CORR <- cor(obs, pred)
  glue::glue("RMSE: {format(RMSE, digits = 3)}
       CORR: {format(CORR, digits = 3)}")
}
```

```{r}
mod_mars <- earth(income ~ ., data = okcupid2_imp_mice_train, degree = 1)

summary(mod_mars, digits = 2)
```

---

<img src="images/earth_summary_cont.png" style="width: 90%" />

```{r}
pred_mars <- predict(mod_mars, okcupid2_imp_mice_valid)
report_rmse_and_cor(okcupid2_imp_mice_valid$income, pred_mars)
```

---

### Pros and Cons

Pros:

- Flexible, built-in interactions
- Highly interpretable
- Little pre-processing of predictors needed
- Handle all types of predictors (continuous, categorical)
- Feature selection built-in (but when predictors are correlated...)
- Good variance-bias tradeoff

Cons:
- Greedy
- The resulting fitted function may not be smooth (not differentiable along hinges)
- No built-in CI for variables params

---

class: section-slide

# CCA
#### (Canonical Correlation Analysis)

---

### What is Linear Regression, really?

<img src="images/LR_geometric_interp.png" style="width: 60%" />

.font80percent[
([ESL II](https://web.stanford.edu/~hastie/Papers/ESLII.pdf))
]

$y = X\beta + \epsilon \rightarrow \hat{y} = X\hat\beta = X(X'X)^{-1}X'y = Hy$

"We project $y$ to the space spanned by $X$..."

"We seek a linear combination of $X$ that would be closest to $y$..."

---

### What if $y$ is $Y$?

If $Y$ is multi-dimensional (not a single vector, but a matrix of $nxq$ dependent variables), we get the same result with a $\hat{B}_{pxq}$ matrix:

$Y = XB + E \rightarrow \hat{Y} = X\hat{B} = X(X'X)^{-1}X'Y = HY$

And if the columns of $X$ are linearly independent this is equivalent to $q$ separate linear regressions:

```{r, eval=FALSE}
n <- 1000
x1 <- rnorm(n)
x2 <- rnorm(n)
x3 <- rnorm(n)

y1 <- 0 + 1 * x1 + 2 * x2 + 3 * x3 + rnorm(n)
y2 <- 1 + 2 * x1 + 3 * x2 + 4 * x3 + rnorm(n)
Y <- cbind(y1, y2)

lm(Y ~ x1 + x2 + x3)
```

---

```{r, echo=FALSE}
n <- 1000
x1 <- rnorm(n)
x2 <- rnorm(n)
x3 <- rnorm(n)

y1 <- 0 + 1 * x1 + 2 * x2 + 3 * x3 + rnorm(n)
y2 <- 1 + 2 * x1 + 3 * x2 + 4 * x3 + rnorm(n)
Y <- cbind(y1, y2)

lm(Y ~ x1 + x2 + x3)
```

Though harder to interpret geometrically, it isn't that different:

"We project each of $Y$'s columns to the space spanned by $X$..."

"We seek a few linear combinations of $X$ that would be closest to each of $Y$'s columns..."

`r emo::ji("right_arrow")` But in case of multi-dimensional $Y$ could we not find linear combinations **of $Y$** that would be closest to linear combinations of $X$?

---

### What is Pearson correlation coefficient, really?

$$r=Corr(x, y) = \frac{Cov(x, y)}{\sqrt{Var(x)}\sqrt{Var(y)}} =\frac{\sum(x_i - \bar x)(y_i - \bar y)}{\sqrt{\sum(x_i - \bar x)^2}\sqrt{\sum(y_i - \bar y)^2}}$$

If we assume $x$ and $y$ are standardized (centered and scaled) to have mean 0 and variance $\frac{1}{n - 1}$ (they have a L2 norm of 1), we could write:

$r = \sum x_i y_i \space s.t. \space \sum x_i = 0;\sum y_i = 0; \sum x^2_i=1; \sum y^2_i=1$

Or:

$r = x'y \space s.t. \space \sum x_i = 0;\sum y_i = 0; x'x=1; y'y=1$

---

```{r}
center_vec <- function(x) x - mean(x)
norm_vec <- function(x) sqrt(sum(x^2))
x_scaled <- center_vec(x1) / norm_vec(center_vec(x1))
y_scaled <- center_vec(y1) / norm_vec(center_vec(y1))

glue::glue("pearson: {format(cor(x1, y1), digits = 3)}
x'y: {format(t(x_scaled) %*% y_scaled, digits = 3)}")
```

So the Pearson's $r$ can be viewed as the dot product between $x$ and $y$, when scaled.

And the dot product between two vectors with L2 norm of 1 is actually the cosine of the angle between them. The higher the correlation, the larger the cosine, the smaller the angle between them, which gives another interpretation for correlation.

---

### What does correlation have to do with regression?

$y = a + bx + \epsilon \rightarrow y = X\beta + \epsilon$

And we're intereseted in $\hat{b}$ which is $\hat \beta_2$.

$$\hat \beta = (X'X)^{-1}X'y = \\
(\begin{bmatrix}
1 & ... & 1\\
x_1 & ... & x_n
\end{bmatrix} \begin{bmatrix}
1 & x_1\\
\vdots & \vdots \\
1 & x_n
\end{bmatrix})^{-1} \begin{bmatrix}
1 & ... & 1\\
x_1 & ... & x_n
\end{bmatrix} \begin{bmatrix}
y_1\\
\vdots \\
y_n
\end{bmatrix}$$

It's not that hard to get that:

$\hat{b} = \hat \beta_2 = \frac{\sum(x_i - \bar x)(y_i - \bar y)}{\sum(x_i - \bar x)^2}$

But that means...

---

$$\hat{b} = \hat \beta_2 = \frac{\sum(x_i - \bar x)(y_i - \bar y)\sqrt{\sum(y_i - \bar y)^2}}{\sum(x_i - \bar x)^2\sqrt{\sum(y_i - \bar y)^2}} \\
=\frac{\sum(x_i - \bar x)(y_i - \bar y)\sqrt{\sum(y_i - \bar y)^2}}{\sqrt{\sum(x_i - \bar x)^2}\sqrt{\sum(y_i - \bar y)^2}\sqrt{\sum(x_i - \bar x)^2}} \\
=\frac{Cov(x, y)}{\sqrt{Var(x)}\sqrt{Var(y)}}\frac{s_y}{s_x} = r\frac{s_y}{s_x}$$

So the Pearson's $r$ is the regression coefficient when regressing $y$ against $x$, and vice versa, times a different factor! .font80percent[(And the same if they are standardized to have the same variance)]

```{r, eval=FALSE, echo=FALSE}
x1_scaled <- scale(x1); y1_scaled <- scale(y1)

glue::glue("pearson: {format(cor(x1_scaled, y1_scaled)[1, 1], digits = 3)}
b_y: {format(lm(x1_scaled ~ y1_scaled)$coef[2], digits = 3)}; b_x: {format(lm(y1_scaled ~ x1_scaled)$coef[2], digits = 3)}")
```

```{r}
lm_yx <- lm(y1 ~ x1); lm_xy <- lm(x1 ~ y1); s_x = sd(x1); s_y = sd(y1)

glue::glue("pearson: {format(cor(x1, y1), digits = 3)}
b_y * s_x / s_y: {format(lm_yx$coef[2] * s_x/s_y, digits = 3)}; b_x * s_y / s_x: {format(lm_xy$coef[2] * s_y/s_x, digits = 3)}")
```

---

### And now, CCA

- Let $X_{nxp}$ be a matrix of $n$ observations with $p$ random variables and $Y_{nxq}$ matrix of the same $n$ observations with $q$ different random variables.

- For example, $X$ represents $n$ people answering $p$ questions in questionnaire A measuring stress, and $Y$ represents the same $n$ people answering $q$ in questionnaire B also measuring stress.

- Canonical Correlation Analysis seeks to find linear combinations of $Y$ and of $X$ that would be "closest" in the common space.

- Whether you understand "closest" via correlation (small angle) or via "regression" (small sum of squares) it is the same thing.

- A typical result would be $Corr(x_1, \frac{y_1+y_2+y_3}{3})=0.8$, meaning question 1 of questionnaire A is "very" correlated with the average of questions 1, 2 and 3 of questionnaire B.

---

We want a "viewpoint" $u$ to look at high-dimensional $X$, and a "viewpoint" $v$ to look at high-dimensional $Y$, such that:

$(u, v) = \text{argmax}_{u,v}{Corr(Xu, Yv)} = \text{argmax}_{u,v}{\frac{Cov(Xu, Yv)}{\sqrt{Var(xu)}\sqrt{Var(Yv)}}}$

Writing $Corr(Xu, Yv)$ as we did:

$(u, v)= \text{argmax}_{u,v}{Corr(Xu, Yv)} = \text{argmax}_{u,v}u^tX^tYv$
$\text{ s.t. }\sum{Xu_i}=0;\sum{Yv_i}=0;u^tX^tXu=1;v^tY^tYv=1$

Applying the Lagrange multipliers technique to this optimization gives:

$L = u^tX^tYv - \frac{\theta}{2}(u^tX^tXu - 1) - \frac{\tau}{2}(v^tY^tYv - 1)\rightarrow max_{u,v}$

(where the $\sum{Xu_i}=0$ terms are dropped since they won't have an effect on $u$ and $v$)

---

Taking derivatives with respect to $u$ and $v$ and comparing to 0 we obtain the equations:
$$(1) \space X'Yv - \theta X'Xu = 0 \\
(2) \space Y'Xu - \tau Y'Yv = 0 $$

Multiplying equation (1) by $u$, and equation (2) by $v$, and subtracting equation (2) from equation (1) we get: $\theta u'X'Xu = \tau v'Y'Yv$

However, $u'X'Xu = v'Y'Yv = 1$, so we get:

$\theta = \tau = u'X'Yv = Corr(Xu, Yv) = r$

Equations (1) and (2) now become:
$$(3) \space X'Yv - rX'Xu = 0 \\
(4) \space Y'Xu - rY'Yv = 0 $$

I we assume $Y'Y$ is invertible we can write from (4):

$(5) \space v = \frac{1}{r}(Y'Y)^{-1}Y'Xu$

---

Substituting for $v$ in equation (3):

$(X'X)^{-1}X'Y(Y'Y)^{-1}Y'Xu = r^2u$

If we write $A = (X'X)^{-1}X'Y(Y'Y)^{-1}Y'X$ we see that:

$Au = r^2u$

is a simple eigenproblem.

Here the first $k$ canonical correlations $r_1, ..., r_k$ are the square roots of the first ordered $k$ eigenvalues of the matrix $A = (X'X)^{-1}X'Y(Y'Y)^{-1}Y'X$ and the first $k$ weight vectors $u_1, ..., u_k$ are the associated eigenvectors.

Similarly, $v_1, ..., v_k$ are the eigenvectors of the matrix $B = (Y'Y)^{-1}Y'X(X'X)^{-1}X'Y$, or we can put in (5) any $u_i$ and $r_i$, to give $v_i$.
