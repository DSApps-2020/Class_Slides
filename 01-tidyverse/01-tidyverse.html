<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Wrangling Data in the Tidyverse</title>
    <meta charset="utf-8" />
    <meta name="author" content="Giora Simchoni" />
    <meta name="date" content="2019-12-02" />
    <head>
      <link rel="icon" href="../DSApps_logo.jpg" type="image/jpg"> 
      <link rel="shortcut icon" href="../DSApps_logo.jpg" type="image/jpg">
    </head>
    <link rel="stylesheet" href="..\slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">


class: logo-slide

---

class: title-slide

## Wrangling Data in the Tidyverse

## Applications of Data Science - Class 1

### Giora Simchoni

#### `gsimchoni@gmail.com and add #dsapps in subject`

### Stat. and OR Department, TAU
### 2019-12-02

---



layout: true

&lt;div class="my-footer"&gt;
  &lt;span&gt;
    &lt;a href="https://dsapps-2020.github.io/Class_Slides/" target="_blank"&gt;Applications of Data Science
    &lt;/a&gt;
  &lt;/span&gt;
&lt;/div&gt;

---



class: section-slide

# I don't need to know about wrangling data, I get by.

---

# So, what's wrong with Excel?

(MS Excel is one amazing software. But it lacks:)

- Structure (or rather, structure is up to the user)
- Types to variables
- Automization (you could learn VBA Excel, but the horror)
- Reproducibility
- Open Source
- Extensibility
- Speed and Scale
- Modeling (there *is* a t-test, but the horror)

[MS Excel might be the most dangerous software on the planet](https://www.forbes.com/sites/timworstall/2013/02/13/microsofts-excel-might-be-the-most-dangerous-software-on-the-planet/#667084f5633d) (Tim Worstall, Forbes)

---

# So, what's wrong with base R?

(Base R is one amazing software. But it lacks:)

- Consistency:
  - Function names
  - Function arguments names
  - Function arguments order
  - Function return types (sometimes the same function!)
- Meaningful errors and warnings
- Good choices of default values to arguments
- Speed
- Good and easy visualizations
- One other thing

---

### (In) Consistency - Example 1: Strings

.font80percent[

```r
# split a string by pattern: strsplit(string, pattern)
strsplit("Who dis?", " ")
```

```
## [[1]]
## [1] "Who"  "dis?"
```


```r
# find if a pattern exists in a string: grepl(pattern, string)
grepl("di", "Who dis?")
```

```
## [1] TRUE
```


```r
# substitute a pattern in a string: sub(pattern, replace, string)
sub("di", "thi", "Who dis?")
```

```
## [1] "Who this?"
```


```r
# length of a string: nchar(string); length of object: length(obj)
nchar("Who dis?"); length("Who dis?")
```

```
## [1] 8
```

```
## [1] 1
```
]

---

### (In) Consistency - Example 2: Models


```r
n &lt;- 10000
x1 &lt;- runif(n)
x2 &lt;- runif(n)
t &lt;- 1 + 2 * x1 + 3 * x2
y &lt;- rbinom(n, 1, 1 / (1 + exp(-t)))
```


```r
glm(y ~ x1 + x2, family = "binomial")
```


```r
glmnet(as.matrix(cbind(x1, x2)), as.factor(y), family = "binomial")
```


```r
randomForest(as.factor(y) ~ x1 + x2)
```



```r
gbm(y ~ x1 + x2, data = data.frame(x1 = x1, x2 = x2, y = y))
```

ðŸ˜±

---

### (Un) Meaningful Errors - Example


```r
df &lt;- data.frame(Education = 1:5, Ethnicity = c(2, 4, 5, 2, 1))
table(df$Eduction, df$Ethnicity)
```

&lt;pre style="color: red;"&gt;&lt;code&gt;## Error in table(df$Eduction, df$Ethnicity): all arguments must have the same
length
&lt;/code&gt;&lt;/pre&gt;

---

### (Bad) Default Values - Example

![](images/bad_args_table.png)


```r
df &lt;- read.csv("data/bad_args_test.csv")
df$col3
```

```
## [1] a b c d
## Levels: a b c d
```


```r
df &lt;- read.csv("data/bad_args_test.csv", stringsAsFactors = FALSE)
df$col3
```

```
## [1] "a" "b" "c" "d"
```

---

### (No) Speed - Example


```r
file_path &lt;- "data/mediocre_file.csv"
df &lt;- read.csv(file_path)
dim(df)
```

```
## [1] 9180   14
```


```r
library(microbenchmark)

microbenchmark(
  read_base = read.csv(file_path),
  read_tidy = read_csv(file_path, col_types = cols()),
  read_dt = data.table::fread(file_path),
  times = 10)
```

```
## Unit: milliseconds
##       expr      min       lq      mean    median       uq      max neval
##  read_base 277.9616 283.1472 283.89872 285.25215 285.6633 286.7661    10
##  read_tidy  23.2688  23.6402  24.29083  23.82535  25.1923  26.0693    10
##    read_dt  11.6489  12.1630  12.48872  12.22925  12.7742  13.5657    10
```

---
class: section-slide

# Detour: The OKCupid Dataset

---

## The OKCupid Dataset

- ~60K active OKCupid users scraped on June 2012
- 35K Male, 25K Female (less awareness for non-binary back then)
- Answers to questions like:
  - Body Type
  - Diet
  - Substance Abuse
  - Education
  - Do you like pets?
  - Open questions, e.g. "On a typical Friday night I am..."
  - And the more boring demographic details like age, height, location, sign, religion etc.
- See [here](https://github.com/rudeboybert/JSE_OkCupid/blob/master/okcupid_codebook.txt) for the full codebook

---
class: section-slide

# End of Detour

---

### (Not) Good Vizualizations - Example

.font80percent[

```r
okcupid &lt;- read_csv("data/okcupid.csv.zip", col_types = cols())
okcupid$income[okcupid$income == -1] &lt;- NA
okcupid$height_cm &lt;- okcupid$height * 2.54
```


```r
plot(okcupid$height_cm, log10(okcupid$income + 1),
     col = c("red", "green")[as.factor(okcupid$sex)])
```

&lt;img src="images/Viz-Base-1.png" width="40%" /&gt;
]

---


```r
ggplot(okcupid, aes(height_cm, log10(income + 1), color = sex)) +
  geom_point()
```

&lt;pre style="color: red;"&gt;&lt;code&gt;## Warning: Removed 48442 rows containing missing values (geom_point).
&lt;/code&gt;&lt;/pre&gt;&lt;img src="images/Viz-Tidy-1.png" width="50%" /&gt;

---

## One other thing

Manager: "Give me the average income of women respondents above age 30 grouped by sexual orientation!"

You:


```r
mean_bi &lt;- mean(okcupid$income[okcupid$sex == "f" &amp; okcupid$age &gt; 30 &amp; okcupid$orientation == "bisexual"], na.rm = TRUE)
mean_gay &lt;- mean(okcupid$income[okcupid$sex == "f" &amp; okcupid$age &gt; 30 &amp; okcupid$orientation == "gay"], na.rm = TRUE)
mean_straight &lt;- mean(okcupid$income[okcupid$sex == "f" &amp; okcupid$age &gt; 30 &amp; okcupid$orientation == "straight"], na.rm = TRUE)

data.frame(orientation = c("bisexual", "gay", "straight"),
           income_mean = c(mean_bi, mean_gay, mean_straight))
```

```
##   orientation income_mean
## 1    bisexual   133421.05
## 2         gay    86489.36
## 3    straight    85219.74
```

---

Or the slightly better you:


```r
mean_income_function &lt;- function(orientation) {
  mean(okcupid$income[okcupid$sex == "f" &amp; okcupid$age &gt; 30 &amp; okcupid$orientation == orientation], na.rm = TRUE)
}

mean_bi &lt;- mean_income_function("bisexual")
mean_gay &lt;- mean_income_function("gay")
mean_straight &lt;- mean_income_function("straight")

data.frame(orientation = c("bisexual", "gay", "straight"),
           income_mean = c(mean_bi, mean_gay, mean_straight))
```

```
##   orientation income_mean
## 1    bisexual   133421.05
## 2         gay    86489.36
## 3    straight    85219.74
```

---

Or the even better you:


```r
orientations &lt;- c("bisexual", "gay", "straight")
income_means &lt;- numeric(3)

for (i in seq_along(orientations)) {
  income_means[i] &lt;- mean_income_function(orientations[i])
}

data.frame(orientation = orientations, income_mean = income_means)
```

```
##   orientation income_mean
## 1    bisexual   133421.05
## 2         gay    86489.36
## 3    straight    85219.74
```

---

Or the best you:


```r
okcupid_females_over30 &lt;- with(okcupid, okcupid[sex == "f" &amp; age &gt; 30, ])
aggregate(okcupid_females_over30$income,
          by = list(orientation = okcupid_females_over30$orientation),
          FUN = mean, na.rm = TRUE)
```

```
##   orientation         x
## 1    bisexual 133421.05
## 2         gay  86489.36
## 3    straight  85219.74
```

&lt;br&gt;

Manager: "What? Why would bisexual women have a higher income than straight or gay women? Could you add the median, trimmed mean, standard error and n?"

You: ðŸ˜±

---
class: section-slide

# The Tidyverse

---

## What *is* The Tidyverse?

&gt; The [tidyverse](https://www.tidyverse.org/) is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.

- `tibble`: the `data.frame` re-imagined
- `readr`: importing/exporting (mostly rectangular) data for humans
- `dplyr` + `tidyr`: a grammar of data manipulation
- `purrr`: functional programming in R
- `stringr`: string manipulation
- `ggplot2`: a grammar of graphics

---

The above can all be installed and loaded under the `tidyverse` package:


```r
library(tidyverse)
```

Many more:
- `lubridate`: manipulating dates
- `modelr`, `recipes`, `rsample`, `infer`: tidy modeling/statistics
- `rvest`: web scraping
- `tidytext`: tidy text analysis (life saver)
- `tidygraph` + `ggraph`: manipulating and plotting networks
- `glue`: print like a boss
- countless `gg` extensions (`ggmosaic`, `ggbeeswarm`, `gganimate`, `ggridges` etc.)

---

# What's so great about the Tidyverse?

- Tidy Data
- Consistentcy (in function names, args, return types, documentation)
- The Pipe
- `ggplot2`
- The Community

---

## Tidy Data

- Each variable must have its own column.
- Each observation must have its own row.
- Each value must have its own cell.

&lt;br&gt;

&lt;img src="images/tidy_data.png" style="width: 90%" /&gt;

---

### Which one of these datasets is tidy? (I)




```r
table1
```

```
## # A tibble: 315 x 4
##    religion      yob n_straight n_total
##    &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;
##  1 atheist      1950         26      29
##  2 buddhist     1950          6       6
##  3 christian    1950         28      32
##  4 hindu        1950          0       0
##  5 jewish       1950         21      24
##  6 muslim       1950          0       0
##  7 unspecified  1950         71      76
##  8 atheist      1951         31      33
##  9 buddhist     1951         11      11
## 10 christian    1951         23      24
## # ... with 305 more rows
```

---

### Which one of these datasets is tidy? (II)


```r
table2
```

```
## # A tibble: 630 x 4
##    religion    yob type         n
##    &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;
##  1 atheist    1950 straight    26
##  2 atheist    1950 total       29
##  3 buddhist   1950 straight     6
##  4 buddhist   1950 total        6
##  5 christian  1950 straight    28
##  6 christian  1950 total       32
##  7 hindu      1950 straight     0
##  8 hindu      1950 total        0
##  9 jewish     1950 straight    21
## 10 jewish     1950 total       24
## # ... with 620 more rows
```

---

### Which one of these datasets is tidy? (III)


```r
table3
```

```
## # A tibble: 315 x 3
##    religion      yob pct_straight
##    &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;       
##  1 atheist      1950 26/29       
##  2 buddhist     1950 6/6         
##  3 christian    1950 28/32       
##  4 hindu        1950 0/0         
##  5 jewish       1950 21/24       
##  6 muslim       1950 0/0         
##  7 unspecified  1950 71/76       
##  8 atheist      1951 31/33       
##  9 buddhist     1951 11/11       
## 10 christian    1951 23/24       
## # ... with 305 more rows
```

---

### Which one of these datasets is tidy? (IV)


```r
table4
```

```
## # A tibble: 7 x 91
##   religion n_total_1950 n_total_1951 n_total_1952 n_total_1953 n_total_1954
##   &lt;chr&gt;           &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;
## 1 atheist            29           33           34           37           40
## 2 buddhist            6           11           14           16           11
## 3 christi~           32           24           37           47           37
## 4 hindu               0            0            0            1            1
## 5 jewish             24           29           27           23           25
## 6 muslim              0            0            0            0            0
## 7 unspeci~           76           79           83           97           83
## # ... with 85 more variables: n_total_1955 &lt;dbl&gt;, n_total_1956 &lt;dbl&gt;,
## #   n_total_1957 &lt;dbl&gt;, n_total_1958 &lt;dbl&gt;, n_total_1959 &lt;dbl&gt;,
## #   n_total_1960 &lt;dbl&gt;, n_total_1961 &lt;dbl&gt;, n_total_1962 &lt;dbl&gt;,
## #   n_total_1963 &lt;dbl&gt;, n_total_1964 &lt;dbl&gt;, n_total_1965 &lt;dbl&gt;,
## #   n_total_1966 &lt;dbl&gt;, n_total_1967 &lt;dbl&gt;, n_total_1968 &lt;dbl&gt;,
## #   n_total_1969 &lt;dbl&gt;, n_total_1970 &lt;dbl&gt;, n_total_1971 &lt;dbl&gt;,
## #   n_total_1972 &lt;dbl&gt;, n_total_1973 &lt;dbl&gt;, n_total_1974 &lt;dbl&gt;,
## #   n_total_1975 &lt;dbl&gt;, n_total_1976 &lt;dbl&gt;, n_total_1977 &lt;dbl&gt;,
## #   n_total_1978 &lt;dbl&gt;, n_total_1979 &lt;dbl&gt;, n_total_1980 &lt;dbl&gt;,
## #   n_total_1981 &lt;dbl&gt;, n_total_1982 &lt;dbl&gt;, n_total_1983 &lt;dbl&gt;,
## #   n_total_1984 &lt;dbl&gt;, n_total_1985 &lt;dbl&gt;, n_total_1986 &lt;dbl&gt;,
## #   n_total_1987 &lt;dbl&gt;, n_total_1988 &lt;dbl&gt;, n_total_1989 &lt;dbl&gt;,
## #   n_total_1990 &lt;dbl&gt;, n_total_1991 &lt;dbl&gt;, n_total_1992 &lt;dbl&gt;,
## #   n_total_1993 &lt;dbl&gt;, n_total_1994 &lt;dbl&gt;, n_straight_1950 &lt;dbl&gt;,
## #   n_straight_1951 &lt;dbl&gt;, n_straight_1952 &lt;dbl&gt;, n_straight_1953 &lt;dbl&gt;,
## #   n_straight_1954 &lt;dbl&gt;, n_straight_1955 &lt;dbl&gt;, n_straight_1956 &lt;dbl&gt;,
## #   n_straight_1957 &lt;dbl&gt;, n_straight_1958 &lt;dbl&gt;, n_straight_1959 &lt;dbl&gt;,
## #   n_straight_1960 &lt;dbl&gt;, n_straight_1961 &lt;dbl&gt;, n_straight_1962 &lt;dbl&gt;,
## #   n_straight_1963 &lt;dbl&gt;, n_straight_1964 &lt;dbl&gt;, n_straight_1965 &lt;dbl&gt;,
## #   n_straight_1966 &lt;dbl&gt;, n_straight_1967 &lt;dbl&gt;, n_straight_1968 &lt;dbl&gt;,
## #   n_straight_1969 &lt;dbl&gt;, n_straight_1970 &lt;dbl&gt;, n_straight_1971 &lt;dbl&gt;,
## #   n_straight_1972 &lt;dbl&gt;, n_straight_1973 &lt;dbl&gt;, n_straight_1974 &lt;dbl&gt;,
## #   n_straight_1975 &lt;dbl&gt;, n_straight_1976 &lt;dbl&gt;, n_straight_1977 &lt;dbl&gt;,
## #   n_straight_1978 &lt;dbl&gt;, n_straight_1979 &lt;dbl&gt;, n_straight_1980 &lt;dbl&gt;,
## #   n_straight_1981 &lt;dbl&gt;, n_straight_1982 &lt;dbl&gt;, n_straight_1983 &lt;dbl&gt;,
## #   n_straight_1984 &lt;dbl&gt;, n_straight_1985 &lt;dbl&gt;, n_straight_1986 &lt;dbl&gt;,
## #   n_straight_1987 &lt;dbl&gt;, n_straight_1988 &lt;dbl&gt;, n_straight_1989 &lt;dbl&gt;,
## #   n_straight_1990 &lt;dbl&gt;, n_straight_1991 &lt;dbl&gt;, n_straight_1992 &lt;dbl&gt;,
## #   n_straight_1993 &lt;dbl&gt;, n_straight_1994 &lt;dbl&gt;
```

---

### Why Tidy?

&gt; Happy families are all alike; every unhappy family is unhappy in its own way. (Leo Tolstoy)

&lt;br&gt;

&gt; It allows Râ€™s vectorised nature to shine. (Hadley Wickham)

---

### A Tidy dataset will be much easier to transform


```r
table1$pct_straight = table1$n_straight / table1$n_total
table1
```

```
## # A tibble: 315 x 5
##    religion      yob n_straight n_total pct_straight
##    &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;
##  1 atheist      1950         26      29        0.897
##  2 buddhist     1950          6       6        1    
##  3 christian    1950         28      32        0.875
##  4 hindu        1950          0       0      NaN    
##  5 jewish       1950         21      24        0.875
##  6 muslim       1950          0       0      NaN    
##  7 unspecified  1950         71      76        0.934
##  8 atheist      1951         31      33        0.939
##  9 buddhist     1951         11      11        1    
## 10 christian    1951         23      24        0.958
## # ... with 305 more rows
```

---

### A Tidy dataset will be much easier to plot


```r
ggplot(table1, aes(x = yob, y = pct_straight, color = religion)) +
  geom_smooth(method = "loess", se = FALSE)
```

&lt;img src="images/Table1-Plot-1.png" width="50%" /&gt;

---
class: section-slide

# Detour: The `tibble`

---

### The `tibble`: the `data.frame` re-imagined

- Prints nicer:


```r
tib1 &lt;- tibble(day = lubridate::today() + runif(1e3) * 30,
               type = sample(letters, 1e3, replace = TRUE),
               quantity = sample(seq(0, 100, 10), 1e3, replace = TRUE))
tib1
```

```
## # A tibble: 1,000 x 3
##    day        type  quantity
##    &lt;date&gt;     &lt;chr&gt;    &lt;dbl&gt;
##  1 2019-12-21 c           20
##  2 2019-12-04 w           70
##  3 2019-12-05 a           50
##  4 2019-12-31 v            0
##  5 2019-12-12 j          100
##  6 2019-12-14 u           70
##  7 2019-12-06 g           90
##  8 2019-12-11 p           70
##  9 2019-12-30 l           50
## 10 2019-12-22 d            0
## # ... with 990 more rows
```

---


```r
df1 &lt;- data.frame(day = lubridate::today() + runif(1e3) * 30,
                  type = sample(letters, 1e3, replace = TRUE),
                  quantity = sample(seq(0, 100, 10), 1e3, replace = TRUE))
df1
```

```
##            day type quantity
## 1   2019-12-20    z       10
## 2   2019-12-30    k       70
## 3   2019-12-12    y       90
## 4   2019-12-13    l       10
## 5   2019-12-29    e       40
## 6   2019-12-18    e       20
## 7   2019-12-25    w       80
## 8   2019-12-31    f       40
## 9   2019-12-14    k       10
## 10  2019-12-26    k       60
## 11  2019-12-13    b       20
## 12  2019-12-11    p       70
## 13  2019-12-29    z       70
## 14  2019-12-18    a       90
## 15  2019-12-02    d      100
## 16  2019-12-23    h       30
## 17  2019-12-14    o       60
## 18  2019-12-25    d       20
## 19  2019-12-12    x      100
## 20  2019-12-25    l       10
## 21  2019-12-06    u       90
## 22  2019-12-25    v       60
## 23  2019-12-31    p       50
## 24  2019-12-04    o       70
## 25  2019-12-06    m       60
## 26  2019-12-14    f       50
## 27  2019-12-06    q       80
## 28  2019-12-26    d       40
## 29  2019-12-11    b       70
## 30  2019-12-11    t       90
## 31  2019-12-05    n       90
## 32  2019-12-06    o       40
## 33  2019-12-06    y       60
## 34  2019-12-02    o      100
## 35  2019-12-07    d       40
## 36  2019-12-24    h        0
## 37  2019-12-15    m       50
## 38  2019-12-25    k       50
## 39  2019-12-19    i       20
## 40  2019-12-22    g       70
## 41  2019-12-07    b        0
## 42  2019-12-26    y       10
## 43  2019-12-30    x       60
## 44  2019-12-03    t       40
## 45  2019-12-19    o       10
## 46  2019-12-29    f       60
## 47  2019-12-24    m       30
## 48  2019-12-20    i       90
## 49  2019-12-19    t       70
## 50  2019-12-28    h       40
## 51  2019-12-31    l       70
## 52  2019-12-14    v       90
## 53  2019-12-26    q       80
## 54  2019-12-11    m       30
## 55  2019-12-20    w       20
## 56  2019-12-13    b       30
## 57  2019-12-02    h       50
## 58  2019-12-26    m       60
## 59  2019-12-12    k       20
## 60  2019-12-12    c       60
## 61  2019-12-23    e       90
## 62  2019-12-17    n       50
## 63  2019-12-11    p       90
## 64  2019-12-26    v       30
## 65  2019-12-22    g        0
## 66  2019-12-18    o        0
## 67  2019-12-17    o      100
## 68  2019-12-14    o       30
## 69  2019-12-21    q        0
## 70  2019-12-28    a       90
## 71  2019-12-22    g       70
## 72  2019-12-20    p       20
## 73  2019-12-06    h       40
## 74  2019-12-06    i       20
## 75  2019-12-23    s       70
## 76  2019-12-10    h       60
## 77  2019-12-13    q       50
## 78  2019-12-24    t       30
## 79  2019-12-28    m       50
## 80  2019-12-05    h        0
## 81  2019-12-12    b        0
## 82  2019-12-17    a       30
## 83  2019-12-05    a       30
## 84  2019-12-17    i       90
## 85  2019-12-31    p       60
## 86  2019-12-10    t       30
## 87  2019-12-29    o       30
## 88  2019-12-09    z       50
## 89  2019-12-05    v       30
## 90  2019-12-13    d       10
## 91  2019-12-05    f       10
## 92  2019-12-18    d       70
## 93  2019-12-30    m       70
## 94  2019-12-09    x       10
## 95  2019-12-08    q       70
## 96  2019-12-25    r       90
## 97  2019-12-12    z       10
## 98  2019-12-09    w       80
## 99  2019-12-17    p       50
## 100 2019-12-03    k      100
## 101 2019-12-14    j       30
## 102 2019-12-15    m       40
## 103 2019-12-26    s       90
## 104 2019-12-28    e        0
## 105 2019-12-05    w      100
## 106 2019-12-20    i       60
## 107 2019-12-11    s       10
## 108 2019-12-28    m       90
## 109 2019-12-22    b       30
## 110 2019-12-05    m       70
## 111 2019-12-29    f       60
## 112 2019-12-02    d       40
## 113 2019-12-17    c       80
## 114 2019-12-23    v       30
## 115 2019-12-16    a       10
## 116 2019-12-24    m       50
## 117 2019-12-16    a       20
## 118 2019-12-17    d       40
## 119 2019-12-30    q       80
## 120 2019-12-18    d       10
## 121 2019-12-30    y       70
## 122 2019-12-22    i       30
## 123 2019-12-18    e       60
## 124 2019-12-17    k       70
## 125 2019-12-16    z       90
## 126 2019-12-16    b       50
## 127 2019-12-31    w       20
## 128 2019-12-22    t       90
## 129 2019-12-29    v      100
## 130 2019-12-23    r       10
## 131 2019-12-29    k       30
## 132 2019-12-12    w       90
## 133 2019-12-23    u       20
## 134 2019-12-18    g        0
## 135 2019-12-06    h       10
## 136 2019-12-28    t       10
## 137 2019-12-21    o       10
## 138 2019-12-04    c       80
## 139 2019-12-11    x       20
## 140 2019-12-03    t       30
## 141 2019-12-24    j        0
## 142 2019-12-13    m       50
## 143 2019-12-23    s      100
## 144 2019-12-02    k       60
## 145 2019-12-29    i       50
## 146 2019-12-14    d       40
## 147 2019-12-04    j       80
## 148 2019-12-16    z       50
## 149 2019-12-02    m       60
## 150 2019-12-16    z        0
## 151 2019-12-19    j       80
## 152 2019-12-21    e       80
## 153 2019-12-26    g       20
## 154 2019-12-15    p       30
## 155 2019-12-24    m       80
## 156 2019-12-26    f       30
## 157 2019-12-22    d      100
## 158 2019-12-24    h       70
## 159 2019-12-21    k      100
## 160 2019-12-19    y       70
## 161 2019-12-10    s       90
## 162 2019-12-03    b       50
## 163 2019-12-17    d       50
## 164 2019-12-09    e       10
## 165 2019-12-21    b        0
## 166 2019-12-27    f      100
## 167 2019-12-12    t       40
## 168 2019-12-06    v       10
## 169 2019-12-14    o      100
## 170 2019-12-24    y        0
## 171 2019-12-09    x       30
## 172 2019-12-06    t       80
## 173 2019-12-19    b      100
## 174 2019-12-04    m       30
## 175 2019-12-24    n       70
## 176 2019-12-10    t      100
## 177 2019-12-22    v       90
## 178 2019-12-23    h       40
## 179 2019-12-12    q      100
## 180 2019-12-16    e       70
## 181 2019-12-24    x        0
## 182 2019-12-14    f        0
## 183 2019-12-16    d       80
## 184 2019-12-18    l       20
## 185 2019-12-30    m       80
## 186 2019-12-11    e      100
## 187 2019-12-09    g       70
## 188 2019-12-13    r       30
## 189 2019-12-06    x        0
## 190 2019-12-29    s       50
## 191 2019-12-02    f       20
## 192 2019-12-02    l       90
## 193 2019-12-12    w       70
## 194 2019-12-23    y       50
## 195 2019-12-13    v        0
## 196 2019-12-20    q        0
## 197 2019-12-27    w       90
## 198 2019-12-20    f      100
## 199 2019-12-10    n        0
## 200 2019-12-03    i       90
## 201 2019-12-03    d      100
## 202 2019-12-28    f       80
## 203 2019-12-10    f      100
## 204 2019-12-08    d       90
## 205 2019-12-21    s        0
## 206 2019-12-20    o       20
## 207 2019-12-22    d        0
## 208 2019-12-25    j      100
## 209 2019-12-26    x       10
## 210 2019-12-20    v       60
## 211 2019-12-17    x       70
## 212 2019-12-29    n       30
## 213 2019-12-18    e       60
## 214 2019-12-14    i       10
## 215 2019-12-22    c       40
## 216 2019-12-10    o      100
## 217 2019-12-17    c       10
## 218 2019-12-15    b       50
## 219 2019-12-07    q       70
## 220 2019-12-14    w        0
## 221 2019-12-17    f       60
## 222 2019-12-06    a       90
## 223 2019-12-07    n       20
## 224 2019-12-23    i       60
## 225 2019-12-15    w      100
## 226 2019-12-13    d      100
## 227 2019-12-22    d       70
## 228 2019-12-29    h       60
## 229 2019-12-26    n       10
## 230 2019-12-03    k       20
## 231 2019-12-29    e       60
## 232 2019-12-07    f       60
## 233 2019-12-18    b       30
## 234 2019-12-10    i       10
## 235 2019-12-19    a       40
## 236 2019-12-16    s      100
## 237 2019-12-19    z       20
## 238 2019-12-04    t      100
## 239 2019-12-22    n        0
## 240 2019-12-23    l       50
## 241 2019-12-21    c       70
## 242 2019-12-27    s       60
## 243 2019-12-02    w       20
## 244 2019-12-17    k       80
## 245 2019-12-29    o       70
## 246 2019-12-27    s       40
## 247 2019-12-27    e       40
## 248 2019-12-18    b       10
## 249 2019-12-18    l      100
## 250 2019-12-29    a       80
## 251 2019-12-25    n       70
## 252 2019-12-09    g      100
## 253 2019-12-12    i        0
## 254 2019-12-22    z       60
## 255 2019-12-05    h       40
## 256 2019-12-25    m       60
## 257 2019-12-11    w       90
## 258 2019-12-31    x      100
## 259 2019-12-31    j        0
## 260 2019-12-30    a       80
## 261 2019-12-31    x      100
## 262 2019-12-21    k       70
## 263 2019-12-28    k       70
## 264 2019-12-22    h       50
## 265 2019-12-09    e       10
## 266 2019-12-20    f       50
## 267 2019-12-25    p      100
## 268 2019-12-03    p      100
## 269 2019-12-15    s       20
## 270 2019-12-23    v       10
## 271 2019-12-15    b       10
## 272 2019-12-20    n       60
## 273 2019-12-24    l       60
## 274 2019-12-12    e        0
## 275 2019-12-12    n       30
## 276 2019-12-14    d       40
## 277 2019-12-16    p       20
## 278 2019-12-23    u        0
## 279 2019-12-05    l       60
## 280 2019-12-20    o       70
## 281 2019-12-29    o      100
## 282 2019-12-09    i       80
## 283 2019-12-31    t      100
## 284 2019-12-18    w       30
## 285 2019-12-24    u       40
## 286 2019-12-05    o       30
## 287 2019-12-12    w       60
## 288 2019-12-24    v       30
## 289 2019-12-24    n       80
## 290 2019-12-18    n      100
## 291 2019-12-07    t       80
## 292 2019-12-29    g       30
## 293 2019-12-18    s       80
## 294 2019-12-09    f       30
## 295 2019-12-24    i       90
## 296 2019-12-23    d       50
## 297 2019-12-25    e        0
## 298 2019-12-13    t        0
## 299 2019-12-24    z       90
## 300 2019-12-23    r       90
## 301 2019-12-13    k        0
## 302 2019-12-14    n       10
## 303 2019-12-12    g       40
## 304 2019-12-19    c       70
## 305 2019-12-03    k        0
## 306 2019-12-13    h       30
## 307 2019-12-28    r      100
## 308 2019-12-22    h       20
## 309 2019-12-15    t       50
## 310 2019-12-30    j        0
## 311 2019-12-19    b       60
## 312 2019-12-26    z       20
## 313 2019-12-02    b       50
## 314 2019-12-14    o       10
## 315 2019-12-31    k       40
## 316 2019-12-12    j        0
## 317 2019-12-22    x      100
## 318 2019-12-02    d       50
## 319 2019-12-22    a        0
## 320 2019-12-16    r       60
## 321 2019-12-04    t       40
## 322 2019-12-25    k       30
## 323 2019-12-31    u       60
## 324 2019-12-09    b       10
## 325 2019-12-05    y      100
## 326 2019-12-31    t       80
## 327 2019-12-27    h       90
## 328 2019-12-20    e       30
## 329 2019-12-19    m       10
## 330 2019-12-20    v       30
## 331 2019-12-19    l       60
## 332 2019-12-04    r       90
## 333 2019-12-14    a       30
##  [ reached 'max' / getOption("max.print") -- omitted 667 rows ]
```

---

- Warns you when you make mistakes (!):


```r
tib1$quanitty
```

&lt;pre style="color: red;"&gt;&lt;code&gt;## Warning: Unknown or uninitialised column: 'quanitty'.
&lt;/code&gt;&lt;/pre&gt;

```
## NULL
```


```r
df1$quanitty
```

```
## NULL
```

---

- Can also create via `tribble()`:


```r
tribble(
  ~a, ~b, ~c,
  "a", 1, 2.2,
  "b", 2, 4.3,
  "c", 3, 3.4
)
```

```
## # A tibble: 3 x 3
##   a         b     c
##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 a         1   2.2
## 2 b         2   4.3
## 3 c         3   3.4
```

---

- Can build on top variables during creation:


```r
tibble(x = 1:5, y = x^2)
```

```
## # A tibble: 5 x 2
##       x     y
##   &lt;int&gt; &lt;dbl&gt;
## 1     1     1
## 2     2     4
## 3     3     9
## 4     4    16
## 5     5    25
```


```r
data.frame(x = 1:5, y = x^2)
```

&lt;pre style="color: red;"&gt;&lt;code&gt;## Error in data.frame(x = 1:5, y = x^2): object 'x' not found
&lt;/code&gt;&lt;/pre&gt;

---

- Will never turn your strings into factors, will never change your column names:


```r
tib1 &lt;- readr::read_csv("data/bad_args_test.csv", col_types = cols())
colnames(tib1)
```

```
## [1] "col1" "col2" "col3"
```

```r
tib1$col3
```

```
## [1] "a" "b" "c" "d"
```


```r
df1 &lt;- read.csv("data/bad_args_test.csv")
colnames(df1)
```

```
## [1] "Ã¯..col1" "col2"    "col3"
```

```r
df1$col3
```

```
## [1] a b c d
## Levels: a b c d
```

---

Though one ought to remember a `tibble` is still a `data.frame`:


```r
class(tib1)
```

```
## [1] "spec_tbl_df" "tbl_df"      "tbl"         "data.frame"
```


```r
class(df1)
```

```
## [1] "data.frame"
```

---
class: section-slide

# End of Detour

---

## Consistency - Example: `stringr`

&gt; a cohesive set of functions designed to make working with strings as easy as possible.


```r
strings_vec &lt;- c("I'm feeling fine", "I'm perfectly OK",
                 "Nothing is wrong!")
str_length(strings_vec)
```

```
## [1] 16 16 17
```

```r
str_c(strings_vec, collapse = ", ")
```

```
## [1] "I'm feeling fine, I'm perfectly OK, Nothing is wrong!"
```

```r
str_sub(strings_vec, 1, 3)
```

```
## [1] "I'm" "I'm" "Not"
```

---


```r
str_detect(strings_vec, "I'm")
```

```
## [1]  TRUE  TRUE FALSE
```

```r
str_replace(strings_vec, "I'm", "You're")
```

```
## [1] "You're feeling fine" "You're perfectly OK" "Nothing is wrong!"
```

```r
str_split("Do you know regex?", " ")
```

```
## [[1]]
## [1] "Do"     "you"    "know"   "regex?"
```

```r
str_extract(strings_vec, "[aeiou]")
```

```
## [1] "e" "e" "o"
```

```r
str_count(strings_vec, "[A-Z]")
```

```
## [1] 1 3 1
```

---

## The Pipe

Remember you?


```r
mean_bi &lt;- mean(okcupid$income[okcupid$sex == "f" &amp; okcupid$age &gt; 30 &amp; okcupid$orientation == "bisexual"], na.rm = TRUE)
mean_gay &lt;- mean(okcupid$income[okcupid$sex == "f" &amp; okcupid$age &gt; 30 &amp; okcupid$orientation == "gay"], na.rm = TRUE)
mean_straight &lt;- mean(okcupid$income[okcupid$sex == "f" &amp; okcupid$age &gt; 30 &amp; okcupid$orientation == "straight"], na.rm = TRUE)

data.frame(orientation = c("bisexual", "gay", "straight"),
           income_mean = c(mean_bi, mean_gay, mean_straight))
```

```
##   orientation income_mean
## 1    bisexual   133421.05
## 2         gay    86489.36
## 3    straight    85219.74
```

---

Doesn't this make much more sense?


```r
okcupid %&gt;%
  filter(sex == "f", age &gt; 30) %&gt;%
  group_by(orientation) %&gt;%
  summarize(income_mean = mean(income, na.rm = TRUE))
```

```
## # A tibble: 3 x 2
##   orientation income_mean
##   &lt;chr&gt;             &lt;dbl&gt;
## 1 bisexual        133421.
## 2 gay              86489.
## 3 straight         85220.
```

- Read as:
  - Take the OKCupid data,
  - Filter only women above the age of 30,
  - And for each group of sexual orientation,
  - Give me the average income

---

- Make verbs, not nouns
- Can always access the dataset last stage with "`.`":

```r
okcupid %&gt;%
  filter(str_count(essay0) &gt; median(str_count(.$essay0), na.rm = T))
```
- Operates not just on data frames or tibbles:

```r
strings_vec %&gt;% str_to_title()
```

```
## [1] "I'm Feeling Fine"  "I'm Perfectly Ok"  "Nothing Is Wrong!"
```
- No intermediate objects
- Don't strive to make the longest possible pipe (though it is a fun experiment)
- Tools exist for debugging

---

And, if you want to throw in the n, the median:


```r
okcupid %&gt;%
  filter(sex == "f", age &gt; 30) %&gt;%
  group_by(orientation) %&gt;%
  summarize(income_mean = mean(income, na.rm = TRUE),
            income_median = median(income, na.rm = TRUE),
            n = n())
```

```
## # A tibble: 3 x 4
##   orientation income_mean income_median     n
##   &lt;chr&gt;             &lt;dbl&gt;         &lt;dbl&gt; &lt;int&gt;
## 1 bisexual        133421.         50000   652
## 2 gay              86489.         40000   664
## 3 straight         85220.         60000 10436
```

---

And if you want this for the age as well:


```r
okcupid %&gt;%
  filter(sex == "f", age &gt; 30) %&gt;%
  group_by(orientation) %&gt;%
  summarize_at(vars(income, age),
               list(mean = mean, median = median), na.rm = TRUE)
```

```
## # A tibble: 3 x 5
##   orientation income_mean age_mean income_median age_median
##   &lt;chr&gt;             &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;
## 1 bisexual        133421.     37.8         50000         36
## 2 gay              86489.     40.4         40000         38
## 3 straight         85220.     40.7         60000         38
```

Now *this* is a language for Data Science.

But we're getting ahead of ourselves.

---

## `ggplot2`

&lt;img src="images/ave_mariah_ggridges.png" style="width: 80%" /&gt;

.font80percent[
[Ave Mariah / Giora Simchoni](http://giorasimchoni.com/2017/12/10/2017-12-10-ave-mariah/)
]
---

## `ggplot2`

&lt;img src="images/soviet_space_dogs.png" style="width: 50%" /&gt;

.font80percent[
[Soviet Space Dogs / David Smale](https://davidsmale.netlify.com/portfolio/soviet-space-dogs-part-2/)
]
---

## `ggplot2`

&lt;img src="images/tennisBig4.gif" style="width: 70%" /&gt;

.font80percent[
[Federer, Nadal, Djokovic and Murray, Love. / Giora Simchoni](http://giorasimchoni.com/2017/05/01/2017-05-01-federer-nadal-djokovic-and-murray-love/)
]
---

## `ggplot2`

&lt;img src="images/washington_heat.png" style="width: 40%" /&gt;

.font80percent[
[NYT-style urban heat island maps / Katie Jolly](https://www.katiejolly.io/blog/2019-08-28/nyt-urban-heat)
]
---

## `ggplot2`

&lt;img src="images/marriage_by_state.png" style="width: 90%" /&gt;

.font80percent[
[A map of marriage rates, state by state / Unkown](https://www.r-graph-gallery.com/328-hexbin-map-of-the-usa.html)
]
---

## `ggplot2`

&lt;img src="images/calendar_graph.png" style="width: 70%" /&gt;

.font80percent[
[Calendar-based graphics for visualizing peopleâ€™s daily schedules / Earo Wang](https://pdf.earo.me/calendar-vis.pdf)
]
---

## The Community

.pull-left[
- 100% Open Source on Github
- Cheatsheet for everything
- Documentation for humans, Packages websites, Webinars
- [Rstudio Community forum](https://community.rstudio.com/)
- [RLadies](https://rladies.org/) worldwide branches .font80percent[(who will pick up the ðŸ¥Š and create RLadies TLV?)]
- Very strong on Twitter #rstats
]

.pull-right[
&lt;a href="https://rstudio.com/resources/cheatsheets/"&gt;&lt;img src="images/stringr_cheatsheet.png" style="width: 100%" /&gt;&lt;/a&gt;
]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="../libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
