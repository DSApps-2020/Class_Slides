<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Topics in Classification</title>
    <meta charset="utf-8" />
    <meta name="author" content="Giora Simchoni" />
    <meta name="date" content="2020-04-03" />
    <head>
      <link rel="icon" href="../DSApps_logo.jpg" type="image/jpg"> 
      <link rel="shortcut icon" href="../DSApps_logo.jpg" type="image/jpg">
    </head>
    <link rel="stylesheet" href="..\slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">


class: logo-slide

---

class: title-slide

## Topics in Classification

### Applications of Data Science - Class 13

### Giora Simchoni

#### `gsimchoni@gmail.com and add #dsapps in subject`

### Stat. and OR Department, TAU
### 2020-04-03

---



layout: true

&lt;div class="my-footer"&gt;
  &lt;span&gt;
    &lt;a href="https://dsapps-2020.github.io/Class_Slides/" target="_blank"&gt;Applications of Data Science
    &lt;/a&gt;
  &lt;/span&gt;
&lt;/div&gt;

---



class: section-slide

# Topics in Classification

---

# Life isn't perfect

Let's tackle just a few issues:

- Imbalanced Classes
- Multiple Classes
- Not enough labelled data and data labeling is expensive

---

class: section-slide

# Imbalanced Classes

---

### Typical examples of Imbalanced Classes scenarios

- Rare diseases: [this](https://www.kaggle.com/c/hivprogression) dataset contains genetic data for 1,000 HIV patients, 206 out of 1,000 patients improved after 16 weeks of therapy
- Conversion/Sell/CTR rates: [this]() dataset contains 10 days of Click-Through-Rate data for Avazu mobile ads, ~6.8M clicked out of ~40.4M
- Fraud detection: [this](https://www.kaggle.com/mlg-ulb/creditcardfraud) dataset contains credit card transactions for a major European CC, 492 frauds out of 284,807 transactions

---

class: section-slide

# Active Learning

---

### Got Data?




```r
n &lt;- 20
x1 &lt;- rnorm(n, 0, 1); x2 &lt;- rnorm(n, 0, 1)
t &lt;- 2 - 4 * x1 + 3 * x2
y &lt;- rbinom(n, 1, 1 / (1 + exp(-t)))
glm_mod &lt;- glm(y ~ x1 + x2, family = "binomial")
```

&lt;img src="images/AL-LR-Example-1.png" width="50%" /&gt;

---

### Want more?

&gt; The key idea behind *active learning* is that a machine learning algorithm can
achieve greater accuracy with fewer training labels if it is allowed to choose the
data from which it learns. An active learner may pose *queries*, usually in the form
of unlabeled data instances to be labeled by an *oracle* (e.g., a human annotator).
Active learning is well-motivated in many modern machine learning problems,
where unlabeled data may be abundant or easily obtained, but labels are difficult,
time-consuming, or expensive to obtain.

([Settles, 2010](http://burrsettles.com/pub/settles.activelearning.pdf))

&gt; You want data? Well data costs!

(No one, ever)

---

### Where this is going

&lt;img src="images/active_learning_plan.png" style="width: 90%" /&gt;

---

### Active Learning Scenarios

1. **Membership Query Synthesis**: You get to choose which (maybe theoretical) points you'd want `\(y\)` labelled for.
2. **Stream-Based Selective Sampling**: You get a 1 point at a time and decide which ones you'd like to query and which to discard.
3. **Pool-Based Sampling**: You have a large collecetion of unlabelled points at your disposal, you need to send the "best ones" for labelling

&lt;img src="images/active_learning_scenarios.png" style="width: 70%" /&gt;

---

### Uncertainty Sampling

.insight[
ðŸ’¡ For a 2-class dataset, the observations your model is most uncertain of are...
]

&lt;img src="images/AL-LR-Example2-1.png" width="50%" /&gt;

---

### Uncertainty Sampling Measures

Let `\(\hat{y}_i\)` be the predicted classes with `\(i\)`th highest score (probability), for observations `\(x\)` under some model `\(\theta\)`.

So `\(\hat{y}_1 = \arg\max{P_{\theta}(y|x)}\)` are the actual predicted classes, `\(\hat{y}_2\)` is the second choices, etc.

* Least Confidence: Choose those observations for which `\(P_{\theta}(\hat{y}_1|x)\)` is smallest:

`\(x^*_{LC} = \arg\min{P_{\theta}(\hat{y}_1|x)}\)`

.insight[
ðŸ’¡ For a 2-class balanced dataset, this means...
]

---

* Margin Sampling: Choose those observations for which the margin between the two highest scores is smallest:

`\(x^*_M = \arg\min{P_{\theta}(\hat{y}_1|x) - P_{\theta}(\hat{y}_2|x)}\)`

.insight[
ðŸ’¡ For a 2-class balanced dataset, this means...
]

* Entropy: Choose the observations for which entropy is highest:

`\(x^*_H = \arg\max-{\sum_i P_{\theta}(\hat{y}_i|x) \log[P_{\theta}(\hat{y}_i|x)]}\)`

We will talk more about entropy in Neural Networks, let's minimize negative entropy.

.insight[
ðŸ’¡ For a 2-class balanced dataset, this means...
]

---

### Example: The `spotify_songs` data from HW3


```r
spotify_songs &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv')

spotify_songs %&gt;% count(playlist_genre)
```

```
## # A tibble: 6 x 2
##   playlist_genre     n
##   &lt;chr&gt;          &lt;int&gt;
## 1 edm             6043
## 2 latin           5155
## 3 pop             5507
## 4 r&amp;b             5431
## 5 rap             5746
## 6 rock            4951
```

Let's try to classify the genre of a song!

---

We'll take only the 12 audio features as predictors, and choose each `track_id` once (remember each song appears a few times?):


```r
library(tidymodels)

predictors &lt;- 12:23

spotify_songs &lt;- spotify_songs %&gt;%
  group_by(track_id) %&gt;%
  sample_n(1) %&gt;%
  ungroup() %&gt;%
  distinct(track_name, .keep_all = TRUE) %&gt;%
  select(track_id, track_name, track_artist, playlist_genre, predictors) %&gt;%
  mutate(playlist_genre = recode(playlist_genre, "r&amp;b" = "rnb"))

set.seed(42)
sptfy_split_obj &lt;- spotify_songs %&gt;%
  initial_split(prop = 0.8)
sptfy_tr &lt;- training(sptfy_split_obj)
sptfy_te &lt;- testing(sptfy_split_obj)
```

---

Plot twist! We only have 100 songs from each genre!


```r
sptfy_tr_small &lt;- sptfy_tr %&gt;%
  group_by(playlist_genre) %&gt;%
  sample_n(100) %&gt;%
  ungroup()

sptfy_tr_small %&gt;% count(playlist_genre)
```

```
## # A tibble: 6 x 2
##   playlist_genre     n
##   &lt;chr&gt;          &lt;int&gt;
## 1 edm              100
## 2 latin            100
## 3 pop              100
## 4 rap              100
## 5 rnb              100
## 6 rock             100
```

Muhaha!

---

We'll also have a pool of songs to query, `sptfy_tr_large`:


```r
sptfy_tr_large &lt;- sptfy_tr %&gt;%
  anti_join(sptfy_tr_small, by = "track_id")
```

We `bake()` the 3 datasets with the small sample params recipe:


```r
sptfy_rec &lt;- recipe(playlist_genre ~ ., data = sptfy_tr_small) %&gt;%
  update_role(track_id, track_name, track_artist,
              new_role = "id") %&gt;%
  step_normalize(all_numeric(), -has_role("id")) %&gt;%
  step_string2factor(playlist_genre) %&gt;%
  prep(sptfy_tr_small, strings_as_factors = FALSE)

sptfy_tr_small &lt;- juice(sptfy_rec)
sptfy_tr_large &lt;- bake(sptfy_rec, new_data = sptfy_tr_large)
sptfy_te &lt;- bake(sptfy_rec, new_data = sptfy_te)
```

---

Let's build a simple kNN model:


```r
mod_spec &lt;- nearest_neighbor(mode = "classification", neighbors = 10) %&gt;%
  set_engine("kknn")

mod_fit &lt;- mod_spec %&gt;%
  fit(playlist_genre ~ ., data = sptfy_tr_small %&gt;%
        select(-track_id, -track_name, -track_artist))

mod_pred &lt;- mod_fit %&gt;%
  predict(new_data = sptfy_tr_large, type = "prob")

mod_pred
```

```
## # A tibble: 18,161 x 6
##    .pred_edm .pred_latin .pred_pop .pred_rap .pred_rnb .pred_rock
##        &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;
##  1   0            0.0546     0.276   0          0.120      0.549 
##  2   0.0188       0          0.240   0          0          0.742 
##  3   0.0486       0.291      0.106   0.00600    0.224      0.324 
##  4   0.113        0.0853     0.369   0.291      0.0546     0.0874
##  5   0.297        0.345      0.178   0.113      0.0673     0     
##  6   0.145        0.200      0       0.603      0.0516     0     
##  7   0.00600      0          0.210   0          0.623      0.161 
##  8   0            0.113      0.324   0.476      0.0874     0     
##  9   0.779        0          0.115   0          0          0.106 
## 10   0.113        0.369      0.451   0.0673     0          0     
## # ... with 18,151 more rows
```

---

Test accuracy?


```r
mod_te_pred_class &lt;- mod_fit %&gt;%
      predict(new_data = sptfy_te) %&gt;%
      bind_cols(sptfy_te)

mod_te_pred_class %&gt;%
  accuracy(truth = playlist_genre, estimate = .pred_class)
```

```
## # A tibble: 1 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy multiclass     0.383
```




Remember this model was built on 600 of almost 19K available unique songs!

---

Test Recall and Precision:


```r
mod_te_pred_class %&gt;%
  group_by(playlist_genre) %&gt;%
  accuracy(truth = playlist_genre, estimate = .pred_class) %&gt;%
  select(playlist_genre, recall = .estimate) %&gt;%
  bind_cols(
    mod_te_pred_class %&gt;%
      group_by(.pred_class) %&gt;%
      accuracy(truth = playlist_genre, estimate = .pred_class) %&gt;%
      select(precision = .estimate)
  )
```

```
## # A tibble: 6 x 3
##   playlist_genre recall precision
##   &lt;fct&gt;           &lt;dbl&gt;     &lt;dbl&gt;
## 1 edm             0.522     0.503
## 2 latin           0.324     0.287
## 3 pop             0.303     0.247
## 4 rap             0.432     0.525
## 5 rnb             0.279     0.323
## 6 rock            0.4       0.409
```

---

Build a function which will take each row of predicted probs and return a list of 3 uncertainty metrics:


```r
uncertainty_lc &lt;- function(probs) {
  max(probs)
}

uncertainty_m &lt;- function(probs) {
  o &lt;- order(probs, decreasing = TRUE)
  probs[o[1]] - probs[o[2]]
}

uncertainty_h &lt;- function(probs) {
  sum(probs * log(probs + 0.000001))
}

uncertainty &lt;- function(...) {
  probs &lt;- c(...)
  list(
    lc = uncertainty_lc(probs),
    margin = uncertainty_m(probs),
    entropy = uncertainty_h(probs)
  )
}
```

---


```r
mod_unc &lt;- mod_pred %&gt;% pmap_dfr(uncertainty)

mod_unc
```

```
## # A tibble: 18,161 x 3
##       lc margin entropy
##    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;
##  1 0.549 0.272   -1.10 
##  2 0.742 0.502   -0.639
##  3 0.324 0.0329  -1.48 
##  4 0.369 0.0777  -1.55 
##  5 0.345 0.0478  -1.46 
##  6 0.603 0.403   -1.06 
##  7 0.623 0.413   -0.947
##  8 0.476 0.152   -1.18 
##  9 0.779 0.664   -0.682
## 10 0.451 0.0823  -1.15 
## # ... with 18,151 more rows
```

Obviously these are correlated:

---

.pull-left[


```r
mod_unc %&gt;% sample_n(1000) %&gt;%
  ggplot(aes(lc, margin)) +
  geom_point() +
  theme_light() +
  theme(text =
          element_text(size = 14))
```

&lt;img src="images/AL-MR-Lc-Margin-1.png" width="100%" /&gt;

]

.pull-right[


```r
mod_unc %&gt;% sample_n(1000) %&gt;%
  ggplot(aes(lc, entropy)) +
  geom_point() +
  theme_light() +
  theme(text =
          element_text(size = 14))
```

&lt;img src="images/AL-MR-Lc-Entropy-1.png" width="100%" /&gt;

]

---

Which are the top 10 songs in terms of each metric the model is most curious about?


```r
sptfy_tr_large_with_unc &lt;- sptfy_tr_large %&gt;%
  bind_cols(mod_unc) %&gt;%
  select(track_name, track_artist, playlist_genre, lc, margin, entropy)

sptfy_tr_large_with_unc %&gt;%
  top_n(-10, lc) %&gt;%
  arrange(lc, track_name)
```

```
## # A tibble: 152 x 6
##    track_name           track_artist   playlist_genre    lc  margin entropy
##    &lt;chr&gt;                &lt;chr&gt;          &lt;fct&gt;          &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
##  1 &lt;U+0412&gt;&lt;U+043E&gt;&lt;U+0443&gt;-&lt;U+0432&gt;&lt;U+043E&gt;&lt;U+0443&gt; &lt;U+043F&gt;&lt;U+0430&gt;&lt;U+043B&gt;&lt;U+0435&gt;&lt;U+0445&gt;&lt;U+0447&gt;&lt;U+044D&gt;      Jah Khalib     rap            0.291 5.89e-2   -1.56
##  2 &lt;U+0423&gt;&lt;U+0439&gt;&lt;U+0434&gt;&lt;U+0443&gt; &lt;U+0441&gt; &lt;U+043C&gt;&lt;U+044B&gt;&lt;U+0441&gt;&lt;U+043B&gt;&lt;U+044F&gt;&lt;U+043C&gt;&lt;U+0438&gt;       Nurminsky      rap            0.291 4.25e-2   -1.50
##  3 3 Days               Rhye           pop            0.291 5.15e-2   -1.55
##  4 911                  BONES          rap            0.291 6.35e-2   -1.59
##  5 A Million Lights - ~ Grant Smillie  edm            0.291 5.15e-2   -1.62
##  6 A Moment In A Milli~ Scorpions      rock           0.291 1.27e-2   -1.51
##  7 A Partir De Hoy      David Bisbal   latin          0.291 9.77e-2   -1.69
##  8 Alcoholic Shrink     Upchurch       rap            0.291 7.72e-4   -1.64
##  9 All About Us         He Is We       pop            0.291 4.01e-2   -1.63
## 10 All For Love         Tungevaag &amp; R~ pop            0.291 1.49e-2   -1.49
## # ... with 142 more rows
```

---


```r
sptfy_tr_large_with_unc %&gt;%
  top_n(-10, margin) %&gt;%
  arrange(margin, track_name)
```

```
## # A tibble: 14 x 6
##    track_name          track_artist    playlist_genre    lc  margin entropy
##    &lt;chr&gt;               &lt;chr&gt;           &lt;fct&gt;          &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
##  1 NiÃ±a Bonita         Chino &amp; Nacho   latin          0.364 1.00e-6   -1.09
##  2 Gimme Gimme Gimme ~ Narada Michael~ pop            0.385 9.01e-6   -1.23
##  3 King of Everything  Dominic Fike    pop            0.324 1.37e-5   -1.38
##  4 RedRover            BONES           rap            0.324 1.37e-5   -1.30
##  5 Bang My Head (feat~ David Guetta    latin          0.358 2.56e-5   -1.27
##  6 Bring Me Back       Pure Pleasure   latin          0.358 2.56e-5   -1.12
##  7 Temple              SwuM            rap            0.358 2.56e-5   -1.27
##  8 Fill the Crown      Poppy           rock           0.330 2.68e-5   -1.24
##  9 Insecure            Billy Bueffer   rap            0.379 4.03e-5   -1.24
## 10 Chillax (feat. Ky-~ Farruko         latin          0.297 4.05e-5   -1.61
## 11 Coloring Outside T~ MisterWives     pop            0.297 4.05e-5   -1.52
## 12 Crash n Burn        Ilvs            edm            0.297 4.05e-5   -1.48
## 13 Pyar Bolda          Jassa Dhillon   rap            0.297 4.05e-5   -1.48
## 14 Venom               Ghostemane      rap            0.297 4.05e-5   -1.45
```

---


```r
sptfy_tr_large_with_unc %&gt;%
  top_n(-10, entropy) %&gt;%
  arrange(entropy, track_name)
```

```
## # A tibble: 10 x 6
##    track_name             track_artist  playlist_genre    lc margin entropy
##    &lt;chr&gt;                  &lt;chr&gt;         &lt;fct&gt;          &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;
##  1 Contagious             Sean Tizzle   latin          0.291 0.100    -1.73
##  2 Together Forever       Lisette Mele~ latin          0.291 0.100    -1.73
##  3 Black Cat              Janet Jackson rnb            0.291 0.100    -1.73
##  4 I Feel Fly             Simon Green   edm            0.297 0.106    -1.73
##  5 Tbt                    Madiel Lara   latin          0.291 0.100    -1.73
##  6 Dancehall Queen (feat~ Beenie Man    rnb            0.291 0.0941   -1.72
##  7 Stay Away from My Baby Banes World   rock           0.291 0.100    -1.72
##  8 Ya No                  Lautaro Lopez rap            0.291 0.0941   -1.72
##  9 Chulin Culin Chunfly ~ Voltio        latin          0.291 0.100    -1.72
## 10 Saoco                  Wisin         latin          0.291 0.100    -1.72
```

---

So far it's only interesting. Will sending the observations our model is most curious about to the "oracle" prove to increase accuracy better than random observations? See full code in slides Rmd files.





&lt;img src="images/Simul-Unc-1.png" width="60%" /&gt;

---

### Query by Commity (QBC)

Similar to ensemble models, we have a committee of models:

`\(C = \{\theta_1, ..., \theta_C\}\)`

Which observations the commitee is most uncertain of? E.g.

`\(x^*_{VE} = \arg\max-{\sum_i \frac{V(\hat{y}_i|x)}{|C|}\log{\frac{V(\hat{y}_i|x)}{|C|}}}\)`

Where `\(V(\hat{y}_i|x)\)` is the number of votes for `\(\hat{y}_i\)`.

How do you get a committee?
- Different models
- Bagging
- Same model, different subsets of features
- Same model, different params

---

For example let's try multinomial regression on the small dataset:


```r
mod_spec &lt;- multinom_reg() %&gt;%
  set_engine("glmnet")

mod_fit &lt;- mod_spec %&gt;%
  fit(playlist_genre ~ ., data = sptfy_tr_small %&gt;%
        select(-track_id, -track_name, -track_artist))

mod_te_pred_class &lt;- mod_fit %&gt;%
      predict(new_data = sptfy_te, penalty = 0.0001) %&gt;%
      bind_cols(sptfy_te)

mod_te_pred_class %&gt;%
  accuracy(truth = playlist_genre, estimate = .pred_class)
```

```
## # A tibble: 1 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy multiclass     0.445
```



---

Let's do 6 multinomial regression models, each receiving 2 different features:


```r
fit_sub_model &lt;- function(i, tr, te) {
  mod_fit &lt;- mod_spec %&gt;%
    fit(playlist_genre ~ ., data = tr %&gt;%
          select(playlist_genre, (2 + i * 2):(3 + i * 2)))

  mod_fit %&gt;%
    predict(new_data = te, penalty = 0.0001)
}
mod_pred &lt;- map_dfc(1:6, fit_sub_model,
                    tr = sptfy_tr_small, te = sptfy_tr_large)
mod_pred
```

```
## # A tibble: 18,161 x 6
##    .pred_class .pred_class1 .pred_class2 .pred_class3 .pred_class4
##    &lt;fct&gt;       &lt;fct&gt;        &lt;fct&gt;        &lt;fct&gt;        &lt;fct&gt;       
##  1 rnb         rnb          rock         rnb          rnb         
##  2 rock        edm          rock         edm          edm         
##  3 edm         rap          pop          edm          latin       
##  4 edm         edm          rock         rock         rnb         
##  5 latin       edm          pop          rock         rock        
##  6 rap         rock         rap          edm          rock        
##  7 rnb         edm          pop          rnb          pop         
##  8 rnb         rnb          rap          rock         edm         
##  9 edm         edm          rock         rock         edm         
## 10 latin       edm          pop          latin        latin       
## # ... with 18,151 more rows, and 1 more variable: .pred_class5 &lt;fct&gt;
```

---


```r
mod_qbc &lt;- mod_pred %&gt;%
  mutate(probs = pmap(
    select(., starts_with(".pred")),
    function(...) table(c(...)) / 6),
    vote_entropy = map_dbl(probs, uncertainty_h),
    vote_margin = map_dbl(probs, uncertainty_m))

sptfy_tr_large_with_qbc &lt;- sptfy_tr_large %&gt;%
  bind_cols(mod_qbc) %&gt;%
  select(track_name, track_artist, playlist_genre,
         starts_with(".pred"), vote_entropy)

sptfy_tr_large_with_qbc %&gt;%
  top_n(-10, vote_entropy) %&gt;%
  arrange(vote_entropy)
```

```
## # A tibble: 161 x 10
##    track_name track_artist playlist_genre .pred_class .pred_class1
##    &lt;chr&gt;      &lt;chr&gt;        &lt;fct&gt;          &lt;fct&gt;       &lt;fct&gt;       
##  1 Dedicated~ Katalyst     rnb            rock        edm         
##  2 About Las~ Miguel Reyes latin          rap         rnb         
##  3 You're Ma~ Toni Braxton rnb            rap         rock        
##  4 Bandito    Enzo Dong    rap            latin       edm         
##  5 Teenage F~ Jorja Smith  pop            pop         rnb         
##  6 Awake      Electric Gu~ pop            pop         rap         
##  7 Seu olhar~ 2STRANGE     rnb            rock        edm         
##  8 Brujeria   El Gran Com~ latin          rap         edm         
##  9 Born To B~ Kygo         latin          latin       edm         
## 10 Es un Sec~ Plan B       latin          latin       edm         
## # ... with 151 more rows, and 5 more variables: .pred_class2 &lt;fct&gt;,
## #   .pred_class3 &lt;fct&gt;, .pred_class4 &lt;fct&gt;, .pred_class5 &lt;fct&gt;,
## #   vote_entropy &lt;dbl&gt;
```

---

Will sending the observations our commitee is in most disagreement about to the "oracle" prove to increase accuracy better than random observations? See full code in slides Rmd files.





&lt;img src="images/Simul-QBC-1.png" width="60%" /&gt;

---

### Other Active Learning Metrics

- Expected Model Change
- Expected Error Reduction
- Variance Reduction
- And more...
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="../libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
