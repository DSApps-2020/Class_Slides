---
title: "Topics in Classification"
subtitle: "Applications of Data Science"
author: "Giora Simchoni"
institute: "Stat. and OR Department, TAU"
date: "`r Sys.Date()`"
output_dir: "images"
output:
  xaringan::moon_reader:
    css: "../slides.css"
    seal: false
    chakra: "../libs/remark-latest.min.js"
    includes:
      in_header: "../header.html"
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

class: logo-slide

---

class: title-slide

## Topics in Classification

### Applications of Data Science - Class 13

### Giora Simchoni

#### `gsimchoni@gmail.com and add #dsapps in subject`

### Stat. and OR Department, TAU
### `r Sys.Date()`

---
```{r child = "../setup.Rmd"}
```

```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
```

class: section-slide

# Topics in Classification

---

# Life isn't perfect

Let's tackle just a few issues:

- Imbalanced Classes
- Multiple Classes
- Not enough labelled data and data labeling is expensive

---

class: section-slide

# Imbalanced Classes

---

### Typical examples of Imbalanced Classes scenarios

- Rare diseases: [this](https://www.kaggle.com/c/hivprogression) dataset contains genetic data for 1,000 HIV patients, 206 out of 1,000 patients improved after 16 weeks of therapy
- Conversion/Sell/CTR rates: [this]() dataset contains 10 days of Click-Through-Rate data for Avazu mobile ads, ~6.8M clicked out of ~40.4M
- Fraud detection: [this](https://www.kaggle.com/mlg-ulb/creditcardfraud) dataset contains credit card transactions for a major European CC, 492 frauds out of 284,807 transactions

---

class: section-slide

# Active Learning

---

### Got Data?

```{r, echo=FALSE}
set.seed(42)
```

```{r, warning=FALSE}
n <- 20
x1 <- rnorm(n, 0, 1); x2 <- rnorm(n, 0, 1)
t <- 2 - 4 * x1 + 3 * x2
y <- rbinom(n, 1, 1 / (1 + exp(-t)))
glm_mod <- glm(y ~ x1 + x2, family = "binomial")
```

```{r AL-LR-Example, warning=FALSE, out.width="50%", echo=FALSE}
a <- -glm_mod$coef[1]/glm_mod$coef[3]
b <- -glm_mod$coef[2]/glm_mod$coef[3]
formula1 <- glue::glue("{format(glm_mod$coef[1], digits = 2)} {format(glm_mod$coef[2], digits = 2)} * x1 + {format(glm_mod$coef[3], digits = 2)} * x2 = 0")

tibble(x1 = x1, x2 = x2, y = factor(y)) %>%
  ggplot(aes(x1, x2, color = y)) +
  geom_point() +
  geom_abline(intercept =  a, slope = b) +
  xlim(c(-2.5, 2.5)) +
  ylim(c(-2.5, 2.5)) +
  guides(color = FALSE) +
  geom_text(x = 0, y = 0, label = formula1,
            parse = FALSE, color = "black", size = 5) +
  theme_classic() +
  theme(text = element_text(size = 20))
```

---

### Want more?

> The key idea behind *active learning* is that a machine learning algorithm can
achieve greater accuracy with fewer training labels if it is allowed to choose the
data from which it learns. An active learner may pose *queries*, usually in the form
of unlabeled data instances to be labeled by an *oracle* (e.g., a human annotator).
Active learning is well-motivated in many modern machine learning problems,
where unlabeled data may be abundant or easily obtained, but labels are difficult,
time-consuming, or expensive to obtain.

([Settles, 2010](http://burrsettles.com/pub/settles.activelearning.pdf))

> You want data? Well data costs!

(No one, ever)

---

### Where this is going

<img src="images/active_learning_plan.png" style="width: 90%" />

---

### Active Learning Scenarios

1. **Membership Query Synthesis**: You get to choose which (maybe theoretical) points you'd want $y$ labelled for.
2. **Stream-Based Selective Sampling**: You get a 1 point at a time and decide which ones you'd like to query and which to discard.
3. **Pool-Based Sampling**: You have a large collecetion of unlabelled points at your disposal, you need to send the "best ones" for labelling

<img src="images/active_learning_scenarios.png" style="width: 70%" />

---

### Uncertainty Sampling

.insight[
`r emo::ji("bulb")` For a 2-class dataset, the observations your model is most uncertain of are...
]

```{r AL-LR-Example2, ref.label = "AL-LR-Example", echo = FALSE, out.width = "50%", warning=FALSE}

```

---

### Uncertainty Sampling Measures

Let $\hat{y}_i$ be the predicted classes with $i$th highest score (probability), for observations $x$ under some model $\theta$.

So $\hat{y}_1 = \arg\max{P_{\theta}(y|x)}$ are the actual predicted classes, $\hat{y}_2$ is the second choices, etc.

* Least Confidence: Choose those observations for which $P_{\theta}(\hat{y}_1|x)$ is smallest:

$x^*_{LC} = \arg\min{P_{\theta}(\hat{y}_1|x)}$

.insight[
`r emo::ji("bulb")` For a 2-class balanced dataset, this means...
]

---

* Margin Sampling: Choose those observations for which the margin between the two highest scores is smallest:

$x^*_M = \arg\min{P_{\theta}(\hat{y}_1|x) - P_{\theta}(\hat{y}_2|x)}$

.insight[
`r emo::ji("bulb")` For a 2-class balanced dataset, this means...
]

* Entropy: Choose the observations for which entropy is highest:

$x^*_H = \arg\max-{\sum_i P_{\theta}(\hat{y}_i|x) \log[P_{\theta}(\hat{y}_i|x)}]$

We will talk more about entropy in Neural Networks.

.insight[
`r emo::ji("bulb")` For a 2-class balanced dataset, this means...
]

---

### Example: The `spotify_songs` data from HW3

```{r, message=FALSE}
spotify_songs <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv')

spotify_songs %>% count(playlist_genre)
```

Let's try to classify the genre of a song!

---

We'll take only the 12 audio features as predictors, and choose each `track_id` once (remember each song appears a few times?):

```{r, message=FALSE, warning=FALSE}
library(tidymodels)

predictors <- 12:23

spotify_songs <- spotify_songs %>%
  group_by(track_id) %>%
  sample_n(1) %>%
  ungroup() %>%
  distinct(track_name, .keep_all = TRUE) %>%
  select(track_id, track_name, track_artist, playlist_genre, predictors) %>%
  mutate(playlist_genre = recode(playlist_genre, "r&b" = "rnb"))

set.seed(42)
sptfy_split_obj <- spotify_songs %>%
  initial_split(prop = 0.8)
sptfy_tr <- training(sptfy_split_obj)
sptfy_te <- testing(sptfy_split_obj)
```

---

Plot twist! We only have 100 songs from each genre!

```{r}
sptfy_tr_small <- sptfy_tr %>%
  group_by(playlist_genre) %>%
  sample_n(100) %>%
  ungroup()

sptfy_tr_small %>% count(playlist_genre)
```

Muhaha!

---

We'll also have a pool of songs to query, `sptfy_tr_large`:

```{r}
sptfy_tr_large <- sptfy_tr %>%
  anti_join(sptfy_tr_small, by = "track_id")
```

We `bake()` the 3 datasets with the small sample params recipe:

```{r}
sptfy_rec <- recipe(playlist_genre ~ ., data = sptfy_tr_small) %>%
  update_role(track_id, track_name, track_artist,
              new_role = "id") %>%
  step_normalize(all_numeric(), -has_role("id")) %>%
  step_string2factor(playlist_genre) %>%
  prep(sptfy_tr_small, strings_as_factors = FALSE)

sptfy_tr_small <- juice(sptfy_rec)
sptfy_tr_large <- bake(sptfy_rec, new_data = sptfy_tr_large)
sptfy_te <- bake(sptfy_rec, new_data = sptfy_te)
```

---

Let's build a simple multinomial model:

```{r}
mod_mn_spec <- multinom_reg() %>%
  set_engine("glmnet")

mod_mn_fit <- mod_mn_spec %>%
  fit(playlist_genre ~ ., data = sptfy_tr_small %>%
        select(-track_id, -track_name, -track_artist))

mod_mn_pred <- mod_mn_fit %>%
  multi_predict(new_data = sptfy_tr_large,
                type = "prob", penalty = 0.0001)

mod_mn_pred <- mod_mn_pred %>% unnest(.pred) %>% select(-penalty)

mod_mn_pred
```

---

Build a function which will take each row of predicted probs and return a list of 3 uncertainty metrics:

```{r}
uncertainty_lc <- function(probs) {
  max(probs)
}

uncertainty_m <- function(probs) {
  o <- order(probs, decreasing = TRUE)
  probs[o[1]] - probs[o[2]]
}

uncertainty_h <- function(probs) {
  -sum(probs * log(probs + 0.000001))
}

uncertainty <- function(...) {
  probs <- c(...)
  list(
    lc = uncertainty_lc(probs),
    margin = uncertainty_m(probs),
    entropy = uncertainty_h(probs)
  )
}
```

---

```{r}
mod_mn_unc <- mod_mn_pred %>% pmap_dfr(uncertainty)

mod_mn_unc
```

Obviously these are correlated:

---

.pull-left[

```{r AL-MR-Lc-Margin, out.width="100%"}
mod_mn_unc %>% sample_n(1000) %>%
  ggplot(aes(lc, margin)) +
  geom_point() +
  theme_classic() +
  theme(text =
          element_text(size = 14))
```

]

.pull-right[

```{r AL-MR-Lc-Entropy, out.width="100%"}
mod_mn_unc %>% sample_n(1000) %>%
  ggplot(aes(lc, entropy)) +
  geom_point() +
  theme_light() +
  theme(text =
          element_text(size = 14))
```

]

---

Which are the top 10 songs in terms of each metric the model is most curious about?

```{r}
sptfy_tr_large %>%
  bind_cols(mod_mn_unc) %>%
  top_n(-10, lc) %>%
  select(track_name, track_artist, playlist_genre, lc) %>%
  arrange(lc)
```

---

```{r}
sptfy_tr_large %>%
  bind_cols(mod_mn_unc) %>%
  top_n(-10, margin) %>%
  select(track_name, track_artist, playlist_genre, margin) %>%
  arrange(margin)
```

---

```{r}
sptfy_tr_large %>%
  bind_cols(mod_mn_unc) %>%
  top_n(10, entropy) %>%
  select(track_name, track_artist, playlist_genre, entropy) %>%
  arrange(-entropy)
```