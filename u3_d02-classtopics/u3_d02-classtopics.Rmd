---
title: "Topics in Classification"
subtitle: "Applications of Data Science"
author: "Giora Simchoni"
institute: "Stat. and OR Department, TAU"
date: "`r Sys.Date()`"
output_dir: "images"
output:
  xaringan::moon_reader:
    css: "../slides.css"
    seal: false
    chakra: "../libs/remark-latest.min.js"
    includes:
      in_header: "../header.html"
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

class: logo-slide

---

class: title-slide

## Topics in Classification

### Applications of Data Science - Class 13

### Giora Simchoni

#### `gsimchoni@gmail.com and add #dsapps in subject`

### Stat. and OR Department, TAU
### `r Sys.Date()`

---
```{r child = "../setup.Rmd"}
```

```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
```

class: section-slide

# Topics in Classification

---

# Life isn't perfect

Let's tackle just a few issues:

- Imbalanced Classes
- Multiple Classes
- Not enough labelled data and data labeling is expensive

---

class: section-slide

# Imbalanced Classes

---

### Typical examples of Imbalanced Classes scenarios

- Rare diseases: [this](https://www.kaggle.com/c/hivprogression) dataset contains genetic data for 1,000 HIV patients, 206 out of 1,000 patients improved after 16 weeks of therapy
- Conversion/Sell/CTR rates: [this]() dataset contains 10 days of Click-Through-Rate data for Avazu mobile ads, ~6.8M clicked out of ~40.4M
- Fraud detection: [this](https://www.kaggle.com/mlg-ulb/creditcardfraud) dataset contains credit card transactions for a major European CC, 492 frauds out of 284,807 transactions

---

class: section-slide

# Active Learning

---

### Got Data?

```{r, echo=FALSE}
set.seed(42)
```

```{r, warning=FALSE}
n <- 20
x1 <- rnorm(n, 0, 1); x2 <- rnorm(n, 0, 1)
t <- 2 - 4 * x1 + 3 * x2
y <- rbinom(n, 1, 1 / (1 + exp(-t)))
glm_mod <- glm(y ~ x1 + x2, family = "binomial")
```

```{r AL-LR-Example, warning=FALSE, out.width="50%", echo=FALSE}
a <- -glm_mod$coef[1]/glm_mod$coef[3]
b <- -glm_mod$coef[2]/glm_mod$coef[3]
formula1 <- glue::glue("{format(glm_mod$coef[1], digits = 2)} {format(glm_mod$coef[2], digits = 2)} * x1 + {format(glm_mod$coef[3], digits = 2)} * x2 = 0")

tibble(x1 = x1, x2 = x2, y = factor(y)) %>%
  ggplot(aes(x1, x2, color = y)) +
  geom_point() +
  geom_abline(intercept =  a, slope = b) +
  xlim(c(-2.5, 2.5)) +
  ylim(c(-2.5, 2.5)) +
  guides(color = FALSE) +
  geom_text(x = 0, y = 0, label = formula1,
            parse = FALSE, color = "black", size = 5) +
  theme_light() +
  theme(text = element_text(size = 20))
```

---

### Want more?

> The key idea behind *active learning* is that a machine learning algorithm can
achieve greater accuracy with fewer training labels if it is allowed to choose the
data from which it learns. An active learner may pose *queries*, usually in the form
of unlabeled data instances to be labeled by an *oracle* (e.g., a human annotator).
Active learning is well-motivated in many modern machine learning problems,
where unlabeled data may be abundant or easily obtained, but labels are difficult,
time-consuming, or expensive to obtain.

([Settles, 2010](http://burrsettles.com/pub/settles.activelearning.pdf))

> You want data? Well data costs!

(No one, ever)

---

### Where this is going

<img src="images/active_learning_plan.png" style="width: 90%" />

---

### Active Learning Scenarios

1. **Membership Query Synthesis**: You get to choose which (maybe theoretical) points you'd want $y$ labelled for.
2. **Stream-Based Selective Sampling**: You get a 1 point at a time and decide which ones you'd like to query and which to discard.
3. **Pool-Based Sampling**: You have a large collecetion of unlabelled points at your disposal, you need to send the "best ones" for labelling

<img src="images/active_learning_scenarios.png" style="width: 70%" />

---

### Uncertainty Sampling

.insight[
`r emo::ji("bulb")` For a 2-class dataset, the observations your model is most uncertain of are...
]

```{r AL-LR-Example2, ref.label = "AL-LR-Example", echo = FALSE, out.width = "50%", warning=FALSE}

```

---

### Uncertainty Sampling Measures

Let $\hat{y}_i$ be the predicted classes with $i$th highest score (probability), for observations $x$ under some model $\theta$.

So $\hat{y}_1 = \arg\max{P_{\theta}(y|x)}$ are the actual predicted classes, $\hat{y}_2$ is the second choices, etc.

* Least Confidence: Choose those observations for which $P_{\theta}(\hat{y}_1|x)$ is smallest:

$x^*_{LC} = \arg\min{P_{\theta}(\hat{y}_1|x)}$

.insight[
`r emo::ji("bulb")` For a 2-class balanced dataset, this means...
]

---

* Margin Sampling: Choose those observations for which the margin between the two highest scores is smallest:

$x^*_M = \arg\min{P_{\theta}(\hat{y}_1|x) - P_{\theta}(\hat{y}_2|x)}$

.insight[
`r emo::ji("bulb")` For a 2-class balanced dataset, this means...
]

* Entropy: Choose the observations for which entropy is highest:

$x^*_H = \arg\max-{\sum_i P_{\theta}(\hat{y}_i|x) \log[P_{\theta}(\hat{y}_i|x)]}$

We will talk more about entropy in Neural Networks, let's minimize negative entropy.

.insight[
`r emo::ji("bulb")` For a 2-class balanced dataset, this means...
]

---

### Example: The `spotify_songs` data from HW3

```{r, message=FALSE}
spotify_songs <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv')

spotify_songs %>% count(playlist_genre)
```

Let's try to classify the genre of a song!

---

We'll take only the 12 audio features as predictors, and choose each `track_id` once (remember each song appears a few times?):

```{r, message=FALSE, warning=FALSE}
library(tidymodels)

predictors <- 12:23

spotify_songs <- spotify_songs %>%
  group_by(track_id) %>%
  sample_n(1) %>%
  ungroup() %>%
  distinct(track_name, .keep_all = TRUE) %>%
  select(track_id, track_name, track_artist, playlist_genre, predictors) %>%
  mutate(playlist_genre = recode(playlist_genre, "r&b" = "rnb"))

set.seed(42)
sptfy_split_obj <- spotify_songs %>%
  initial_split(prop = 0.8)
sptfy_tr <- training(sptfy_split_obj)
sptfy_te <- testing(sptfy_split_obj)
```

---

Plot twist! We only have 100 songs from each genre!

```{r}
sptfy_tr_small <- sptfy_tr %>%
  group_by(playlist_genre) %>%
  sample_n(100) %>%
  ungroup()

sptfy_tr_small %>% count(playlist_genre)
```

Muhaha!

---

We'll also have a pool of songs to query, `sptfy_tr_large`:

```{r}
sptfy_tr_large <- sptfy_tr %>%
  anti_join(sptfy_tr_small, by = "track_id")
```

We `bake()` the 3 datasets with the small sample params recipe:

```{r}
sptfy_rec <- recipe(playlist_genre ~ ., data = sptfy_tr_small) %>%
  update_role(track_id, track_name, track_artist,
              new_role = "id") %>%
  step_normalize(all_numeric(), -has_role("id")) %>%
  step_string2factor(playlist_genre) %>%
  prep(sptfy_tr_small, strings_as_factors = FALSE)

sptfy_tr_small <- juice(sptfy_rec)
sptfy_tr_large <- bake(sptfy_rec, new_data = sptfy_tr_large)
sptfy_te <- bake(sptfy_rec, new_data = sptfy_te)
```

---

Let's build a simple kNN model:

```{r}
mod_spec <- nearest_neighbor(mode = "classification", neighbors = 10) %>%
  set_engine("kknn")

mod_fit <- mod_spec %>%
  fit(playlist_genre ~ ., data = sptfy_tr_small %>%
        select(-track_id, -track_name, -track_artist))

mod_pred <- mod_fit %>%
  predict(new_data = sptfy_tr_large, type = "prob")

mod_pred
```

---

Test accuracy?

```{r}
mod_te_pred_class <- mod_fit %>%
      predict(new_data = sptfy_te) %>%
      bind_cols(sptfy_te)

mod_te_pred_class %>%
  accuracy(truth = playlist_genre, estimate = .pred_class)
```

```{r, echo=FALSE}
baseline_acc <- mod_te_pred_class %>%
  accuracy(truth = playlist_genre, estimate = .pred_class) %>%
  pull(.estimate)
```


Remember this model was built on 600 of almost 19K available unique songs!

---

Test Recall and Precision:

```{r}
mod_te_pred_class %>%
  group_by(playlist_genre) %>%
  accuracy(truth = playlist_genre, estimate = .pred_class) %>%
  select(playlist_genre, recall = .estimate) %>%
  bind_cols(
    mod_te_pred_class %>%
      group_by(.pred_class) %>%
      accuracy(truth = playlist_genre, estimate = .pred_class) %>%
      select(precision = .estimate)
  )
```

---

Build a function which will take each row of predicted probs and return a list of 3 uncertainty metrics:

```{r}
uncertainty_lc <- function(probs) {
  max(probs)
}

uncertainty_m <- function(probs) {
  o <- order(probs, decreasing = TRUE)
  probs[o[1]] - probs[o[2]]
}

uncertainty_h <- function(probs) {
  sum(probs * log(probs + 0.000001))
}

uncertainty <- function(...) {
  probs <- c(...)
  list(
    lc = uncertainty_lc(probs),
    margin = uncertainty_m(probs),
    entropy = uncertainty_h(probs)
  )
}
```

---

```{r}
mod_unc <- mod_pred %>% pmap_dfr(uncertainty)

mod_unc
```

Obviously these are correlated:

---

.pull-left[

```{r AL-MR-Lc-Margin, out.width="100%"}
mod_unc %>% sample_n(1000) %>%
  ggplot(aes(lc, margin)) +
  geom_point() +
  theme_light() +
  theme(text =
          element_text(size = 14))
```

]

.pull-right[

```{r AL-MR-Lc-Entropy, out.width="100%"}
mod_unc %>% sample_n(1000) %>%
  ggplot(aes(lc, entropy)) +
  geom_point() +
  theme_light() +
  theme(text =
          element_text(size = 14))
```

]

---

Which are the top 10 songs in terms of each metric the model is most curious about?

```{r}
sptfy_tr_large_with_unc <- sptfy_tr_large %>%
  bind_cols(mod_unc) %>%
  select(track_name, track_artist, playlist_genre, lc, margin, entropy)

sptfy_tr_large_with_unc %>%
  top_n(-10, lc) %>%
  arrange(lc, track_name)
```

---

```{r}
sptfy_tr_large_with_unc %>%
  top_n(-10, margin) %>%
  arrange(margin, track_name)
```

---

```{r}
sptfy_tr_large_with_unc %>%
  top_n(-10, entropy) %>%
  arrange(entropy, track_name)
```

---

So far it's only interesting. Will sending the observations our model is most curious about to the "oracle" prove to increase accuracy better than random observations? See full code in slides Rmd files.

```{r, echo=FALSE}
active_learning_uncertainty_iteration <- function(i, unc_metric, n, debug = FALSE) {
  if (debug) {
    print(str_c(i, n, nrow(mod_unc), nrow(sptfy_tr_small),
              nrow(sptfy_tr_large), as.character(rlang::enquo(unc_metric))[2],
              sep = " "))
  }
  
  mod_unc <<- mod_unc %>%
    mutate(random = sample(1:n(), n()))
  
  sptfy_added <- sptfy_tr_large %>%
    bind_cols(mod_unc) %>%
    arrange({{unc_metric}}, track_id) %>%
    slice(1:n) %>%
    select(-lc,-margin, -entropy, -random)

  sptfy_tr_large <<- sptfy_tr_large %>%
    anti_join(sptfy_added, by = "track_id")
  
  sptfy_tr_small <<- sptfy_tr_small %>%
    bind_rows(sptfy_added)
  
  mod_fit <- mod_spec %>%
    fit(playlist_genre ~ ., data = sptfy_tr_small %>%
          select(-track_id, -track_name, -track_artist))
  
  mod_unc <<- mod_fit %>%
    predict(new_data = sptfy_tr_large,
                  type = "prob") %>%
    pmap_dfr(uncertainty)
  
  acc <- mod_fit %>%
    predict(new_data = sptfy_te) %>%
    bind_cols(sptfy_te) %>%
    accuracy(truth = playlist_genre, estimate = .pred_class) %>%
    pull(.estimate)
  
  return(acc)
}

active_learning_uncertainty <- function(unc_metric, n = 100, iter = 10) {
  sptfy_tr_small_copy <<- sptfy_tr_small
  sptfy_tr_large_copy <<- sptfy_tr_large
  mod_unc_copy <<- mod_unc
  accuracies <- map_dbl(1:iter,
                     active_learning_uncertainty_iteration,
                     unc_metric = {{unc_metric}},
                     n = n)
  sptfy_tr_small <<- sptfy_tr_small_copy
  sptfy_tr_large <<- sptfy_tr_large_copy
  mod_unc <<- mod_unc_copy
  return(c(baseline_acc, accuracies))
}
```

```{r, echo=FALSE}
n_seq <- seq(0, 1000, 100)
simul_uncertainty <- bind_rows(
  tibble(unc_metric = "lc", n = n_seq, acc = active_learning_uncertainty(lc)),
  tibble(unc_metric = "margin", n = n_seq,acc = active_learning_uncertainty(margin)),
  tibble(unc_metric = "entropy", n = n_seq,acc = active_learning_uncertainty(entropy)),
  tibble(unc_metric = "random", n = n_seq, acc = active_learning_uncertainty(random))
)
```

```{r Simul-Unc, echo=FALSE, out.width="60%"}
simul_uncertainty %>%
  ggplot(aes(n, acc, color = unc_metric)) +
  geom_point() +
  geom_line() +
  labs(x = "No. of added observations", y = "Accuracy", color = "uncertainty") +
  scale_x_continuous(breaks = n_seq) +
  scale_y_continuous(breaks = seq(0.32, 0.40, 0.01),
                     labels = scales::percent_format(accuracy = 1)) +
  theme_light()
```

---

### Query by Commity (QBC)

Similar to ensemble models, we have a committee of models:

$C = \{\theta_1, ..., \theta_C\}$

Which observations the commitee is most uncertain of? E.g.

$x^*_{VE} = \arg\max-{\sum_i \frac{V(\hat{y}_i|x)}{|C|}\log{\frac{V(\hat{y}_i|x)}{|C|}}}$

Where $V(\hat{y}_i|x)$ is the number of votes for $\hat{y}_i$.

How do you get a committee?
- Different models
- Bagging
- Same model, different subsets of features
- Same model, different params

---

For example let's try multinomial regression on the small dataset:

```{r}
mod_spec <- multinom_reg() %>%
  set_engine("glmnet")

mod_fit <- mod_spec %>%
  fit(playlist_genre ~ ., data = sptfy_tr_small %>%
        select(-track_id, -track_name, -track_artist))

mod_te_pred_class <- mod_fit %>%
      predict(new_data = sptfy_te, penalty = 0.0001) %>%
      bind_cols(sptfy_te)

mod_te_pred_class %>%
  accuracy(truth = playlist_genre, estimate = .pred_class)
```

```{r, echo=FALSE}
baseline_acc <- mod_te_pred_class %>%
  accuracy(truth = playlist_genre, estimate = .pred_class) %>%
  pull(.estimate)
```

---

Let's do 6 multinomial regression models, each receiving 2 different features:

```{r}
fit_sub_model <- function(i, tr, te) {
  mod_fit <- mod_spec %>%
    fit(playlist_genre ~ ., data = tr %>%
          select(playlist_genre, (2 + i * 2):(3 + i * 2)))

  mod_fit %>%
    predict(new_data = te, penalty = 0.0001)
}
mod_pred <- map_dfc(1:6, fit_sub_model,
                    tr = sptfy_tr_small, te = sptfy_tr_large)
mod_pred
```

---

```{r}
mod_qbc <- mod_pred %>%
  mutate(probs = pmap(
    select(., starts_with(".pred")),
    function(...) table(c(...)) / 6),
    vote_entropy = map_dbl(probs, uncertainty_h),
    vote_margin = map_dbl(probs, uncertainty_m))

sptfy_tr_large_with_qbc <- sptfy_tr_large %>%
  bind_cols(mod_qbc) %>%
  select(track_name, track_artist, playlist_genre,
         starts_with(".pred"), vote_entropy)

sptfy_tr_large_with_qbc %>%
  top_n(-10, vote_entropy) %>%
  arrange(vote_entropy)
```

---

Will sending the observations our commitee is in most disagreement about to the "oracle" prove to increase accuracy better than random observations? See full code in slides Rmd files.

```{r, echo=FALSE}
active_learning_qbc_iteration <- function(i, qbc_metric, n, debug = FALSE) {
  if (debug) {
    print(str_c(i, n, nrow(mod_qbc), nrow(sptfy_tr_small),
              nrow(sptfy_tr_large), as.character(rlang::enquo(qbc_metric))[2],
              sep = " "))
  }
  
  mod_qbc <<- mod_qbc %>%
    select(-starts_with(".pred"), -probs) %>%
    mutate(random = sample(1:n(), n()))
  
  sptfy_added <- sptfy_tr_large %>%
    bind_cols(mod_qbc) %>%
    arrange({{qbc_metric}}, track_id) %>%
    slice(1:n) %>%
    select(-vote_entropy, -vote_margin, -random)

  sptfy_tr_large <<- sptfy_tr_large %>%
    anti_join(sptfy_added, by = "track_id")
  
  sptfy_tr_small <<- sptfy_tr_small %>%
    bind_rows(sptfy_added)
  
  mod_fit <- mod_spec %>%
    fit(playlist_genre ~ ., data = sptfy_tr_small %>%
          select(-track_id, -track_name, -track_artist))
  
  mod_qbc <<- map_dfc(1:6, fit_sub_model,
                      tr = sptfy_tr_small, te = sptfy_tr_large) %>%
    mutate(probs = pmap(
    select(., starts_with(".pred")),
    function(...) table(c(...)) / 6),
    vote_entropy = map_dbl(probs, uncertainty_h),
    vote_margin = map_dbl(probs, uncertainty_m))
  
  acc <- mod_fit %>%
    predict(new_data = sptfy_te, penalty = 0.0001) %>%
    bind_cols(sptfy_te) %>%
    accuracy(truth = playlist_genre, estimate = .pred_class) %>%
    pull(.estimate)
  
  return(acc)
}

active_learning_qbc <- function(qbc_metric, n = 100, iter = 10) {
  sptfy_tr_small_copy <<- sptfy_tr_small
  sptfy_tr_large_copy <<- sptfy_tr_large
  mod_qbc_copy <<- mod_qbc
  accuracies <- map_dbl(1:iter,
                     active_learning_qbc_iteration,
                     qbc_metric = {{qbc_metric}},
                     n = n)
  sptfy_tr_small <<- sptfy_tr_small_copy
  sptfy_tr_large <<- sptfy_tr_large_copy
  mod_qbc <<- mod_qbc_copy
  return(c(baseline_acc, accuracies))
}
```

```{r, echo=FALSE}
n_seq <- seq(0, 1000, 100)
simul_qbc <- bind_rows(
  tibble(qbc_metric = "vote entropy", n = n_seq,
         acc = active_learning_qbc(vote_entropy)),
  tibble(qbc_metric = "vote margin", n = n_seq,
         acc = active_learning_qbc(vote_margin)),
  tibble(qbc_metric = "random", n = n_seq,
         acc = active_learning_qbc(random))
)
```

```{r Simul-QBC, echo=FALSE, out.width="60%"}
simul_qbc %>%
  ggplot(aes(n, acc, color = qbc_metric)) +
  geom_point() +
  geom_line() +
  labs(x = "No. of added observations", y = "Accuracy", color = "QBC") +
  scale_y_continuous(breaks = seq(0.44, 0.50, 0.005),
                     labels = percent_format(accuracy = 0.1)) +
  theme_light()
```

---

### Other Active Learning Metrics

- Expected Model Change
- Expected Error Reduction
- Variance Reduction
- And more...